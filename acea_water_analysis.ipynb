{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACEA WATER ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# %matplotlib notebook\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from pathlib import Path\n",
    "from statsmodels.tsa.stattools import ccf\n",
    "from scipy import stats\n",
    "\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import InputLayer\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "plt.style.use('seaborn-muted')\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing functions\n",
    "\n",
    "def preprocess(df, col_ind, start_ind=0):\n",
    "    \"\"\"\n",
    "    Some basic preprocessing of data from selected column of dataframe.\n",
    "    This will probably need to be thought about more to deal with NaNs\n",
    "    more effectively\n",
    "    \n",
    "    \"\"\"\n",
    "    pd_series = df.iloc[start_ind:, col_ind]\n",
    "    pd_values = pd_series.to_numpy()\n",
    "    # max_value = np.max(np.abs(pd_values))\n",
    "    # pd_values = pd_values / max_value\n",
    "    name = df.columns[col_ind]\n",
    "    \n",
    "    return pd_values, name\n",
    "\n",
    "\n",
    "def preprocess_int(df, col_ind, start_ind=0, stop_ind=-1, fill_zeros=True):\n",
    "    \"\"\"\n",
    "    Some basic preprocessing of data from selected column of dataframe.\n",
    "    This will probably need to be thought about more to deal with NaNs\n",
    "    more effectively\n",
    "    \n",
    "    \"\"\"\n",
    "    pd_series = df.iloc[start_ind:stop_ind, col_ind]\n",
    "    pd_series = pd_series.astype('float32')\n",
    "    if fill_zeros:\n",
    "#         print(np.sum(np.isnan(pd_series.to_numpy())))\n",
    "#         print('change_zero')\n",
    "#         pd_series.replace(to_replace=0, value=np.nan)\n",
    "        pd_series.mask(pd_series==0)\n",
    "#         print(np.sum(np.isnan(pd_series.to_numpy())))\n",
    "    pd_series = pd_series.interpolate(method='linear')\n",
    "    pd_series = pd_series.fillna(method='bfill')\n",
    "    pd_series = pd_series.fillna(method='ffill')\n",
    "    pd_values = pd_series.to_numpy()\n",
    "    # max_value = np.max(np.abs(pd_values))\n",
    "    # pd_values = pd_values / max_value\n",
    "    name = df.columns[col_ind]\n",
    "    \n",
    "    return pd_values, name\n",
    "\n",
    "\n",
    "def get_time_series(col_inds, start_ind=0, stop_ind=-1, fill_zeros=True):\n",
    "    \"\"\"\n",
    "    Get all of the time series from the specified column indices and their names\n",
    "    and populate lists for each\n",
    "    \n",
    "    \"\"\"\n",
    "    ts = []\n",
    "    name = []\n",
    "    \n",
    "    for n in range(len(col_inds)):\n",
    "        ts_, name_ = preprocess_int(df, col_inds[n], start_ind=start_ind, stop_ind=stop_ind, fill_zeros=fill_zeros)\n",
    "        ts.append(ts_)\n",
    "        name.append(name_)\n",
    "        \n",
    "    return ts, name\n",
    "\n",
    "\n",
    "def find_data_gaps(dataframe, plot=True, title=None):\n",
    "    \"\"\"\n",
    "    Finds NaNs and zeros in dataframe, with optional plot to show their locations\n",
    "    \n",
    "    \"\"\"\n",
    "    # Put dataframe into numpy array, ignoring date variable\n",
    "    df_nodate = df.drop(['Date'], axis=1)\n",
    "    all_data = df_nodate.to_numpy(na_value=np.nan)\n",
    "    col_names = df_nodate.columns\n",
    "    numcol = len(col_names)\n",
    "\n",
    "    # Find missing values\n",
    "    is_nan = np.isnan(all_data)\n",
    "    is_zero = (all_data == 0)\n",
    "    nan_array = np.where(is_nan, 1, np.nan)\n",
    "    zero_array = np.where(is_zero, 1, np.nan)\n",
    "\n",
    "    # Plot showing missing values and zeros\n",
    "    if plot:\n",
    "        if title is None: title = 'Location of NaNs and zeros in dataset'\n",
    "        fig = plt.figure(figsize=(9, 5))\n",
    "        ax = plt.subplot(111)\n",
    "        # divider = make_axes_locatable(ax)\n",
    "        # cax = divider.append_axes(\"top\", size=\"5%\", pad=0.08)\n",
    "        norm = mpl.colors.Normalize(vmin=-1, vmax=1)\n",
    "        sc_map = mpl.cm.ScalarMappable(norm=norm, cmap='PiYG')\n",
    "        ms = ax.imshow(nan_array.T, aspect='auto', cmap='Pastel1', interpolation='none')\n",
    "        ms2 = ax.imshow(zero_array.T, aspect='auto', cmap='Set3', interpolation='none')\n",
    "        ax.set_yticks(np.arange(numcol))\n",
    "        ax.set_yticklabels(col_names)\n",
    "        ax.set_xlabel('Time [days]')\n",
    "        cmap_nan = plt.cm.Pastel1\n",
    "        cmap_zero = plt.cm.Set3\n",
    "        custom_lines = [Line2D([0], [0], color=cmap_zero(0.), lw=5),\n",
    "                        Line2D([0], [0], color=cmap_nan(0.), lw=5)]\n",
    "        ax.legend(custom_lines, ['Zero', 'NaN'], loc='lower right')\n",
    "        plt.title(title)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return all_data, col_names, is_nan, is_zero\n",
    "\n",
    "\n",
    "def normalise_sum(signal):\n",
    "    \"\"\"\n",
    "    Normalises a signal so that the sum of all samples is unity.\n",
    "\n",
    "    :param signal: An array of values representing the signal to be normalised.\n",
    "    :returns: An array of values representing the normalised signal.\n",
    "\n",
    "    \"\"\"\n",
    "    signal = np.asarray(signal, dtype=float)\n",
    "    sum_ = np.sum(signal) if signal.any() else 1.\n",
    "\n",
    "    return signal / sum_\n",
    "\n",
    "\n",
    "def movingav(signal, winwidth, winfunc=None):\n",
    "    \"\"\"\n",
    "    Calculates the moving average of a signal using chosen window function.\n",
    "\n",
    "    :param signal: An array of values representing the signal to be smoothed.\n",
    "    :param winwidth: The width of the smoothing window (number of samples).\n",
    "    :param winfunc: The window function to use for smoothing. By default, a\n",
    "        rectangular window will be used.\n",
    "    :returns: An array of values representing the smoothed signal.\n",
    "    :raises ValueError: If window width is negative.\n",
    "    :raises ValueError: If window width exceeds length of input array.\n",
    "\n",
    "    \"\"\"\n",
    "    numsamples = len(signal)\n",
    "\n",
    "    hww = int(winwidth / 2.)\n",
    "    winwidth = 2 * hww + 1\n",
    "\n",
    "    if winwidth < 0:\n",
    "        raise ValueError(\"window width must not be negative\")\n",
    "\n",
    "    if winwidth >= numsamples:\n",
    "        raise ValueError(\"window width must not exceed length of input array\")\n",
    "\n",
    "    win = np.ones(winwidth) if winfunc is None else winfunc(winwidth)\n",
    "\n",
    "    win = normalise_sum(win)\n",
    "\n",
    "    halfwin2 = normalise_sum(win[hww:])\n",
    "    halfwin1 = normalise_sum(win[:hww+1])\n",
    "\n",
    "    valstart = np.dot(halfwin2, signal[0:hww+1])\n",
    "    valend = np.dot(halfwin1, signal[numsamples - hww - 1:])\n",
    "\n",
    "    signal = np.concatenate((np.ones(hww) * valstart, signal,\n",
    "                             np.ones(hww) * valend))\n",
    "\n",
    "    wpos = hww\n",
    "    wend = len(signal) - hww - 1\n",
    "\n",
    "    sig_smooth = np.empty(numsamples)\n",
    "\n",
    "    while wpos <= wend:\n",
    "        sig_smooth[wpos - hww] = np.dot(signal[wpos-hww: wpos+hww+1], win)\n",
    "        wpos += 1\n",
    "\n",
    "    return sig_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical functions \n",
    "\n",
    "def spearman_lag(data1, data2, lag):\n",
    "    \"\"\"Calculate Spearman's rank correlation coefficient between 2 datasets,\n",
    "    with a lag applied to data2\"\"\"\n",
    "    \n",
    "    data_length = data1.size\n",
    "    if lag > 0:\n",
    "        data2_lag = np.zeros(data_length)\n",
    "        data2_lag[lag:] = data2[:-lag] \n",
    "        data2_lag[:lag] = data2[0]\n",
    "    else:\n",
    "        data2_lag = data2\n",
    "    src, _ = stats.spearmanr(data1, data2_lag)\n",
    "    \n",
    "    return src\n",
    "\n",
    "\n",
    "def cross_corr_lag(data1, data2, lag_array=None):\n",
    "    \"\"\"Calculate Spearman's rank correlation coefficient between 2 datasets,\n",
    "    for a range of different lags applied to data2\"\"\"\n",
    "    if lag_array is None:\n",
    "        lag_array = np.arange(data1.size)\n",
    "    crosscorr_lag = np.empty(len(lag_array))\n",
    "    for n in range(len(lag_array)):\n",
    "        crosscorr_lag[n] = spearman_lag(data1, data2, lag=lag_array[n])\n",
    "        \n",
    "    return crosscorr_lag, lag_array\n",
    "\n",
    "\n",
    "def moving_average(x, w):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w\n",
    "\n",
    "\n",
    "def normalise_0_to_1(signal):\n",
    "#     signal = np.asarray(signal)\n",
    "    sig_min = np.min(signal)\n",
    "    sig_max = np.max(signal)\n",
    "    sig_norm = (signal - sig_min) / (sig_max - sig_min)\n",
    "    return sig_norm\n",
    "\n",
    "\n",
    "def exp_convolve(data, tau, winlength=None):\n",
    "    \"\"\"\n",
    "    Convolve input data with an exponential window function (time constant = tau)\n",
    "    \n",
    "    \"\"\"\n",
    "    data = np.asarray(data)\n",
    "    len_data = data.size\n",
    "    if winlength is None:\n",
    "        winlength = len_data\n",
    "    t = np.linspace(0, winlength-1, winlength)\n",
    "    exp_window = np.exp(-t / tau)\n",
    "    exp_window = exp_window / np.sum(exp_window)\n",
    "    data_conv = np.convolve(data, exp_window, 'full')[:len_data]\n",
    "\n",
    "    return data_conv\n",
    "\n",
    "\n",
    "def find_datatypes(df):\n",
    "    \"\"\"Find the indices of each pf the different datatypes in the dataframe\"\"\"\n",
    "    names = df.columns\n",
    "    datatypes = ['Rainfall',\n",
    "                 'Depth_to_Groundwater',\n",
    "                 'Temperature',\n",
    "                 'Volume',\n",
    "                 'Hydrometry',\n",
    "                 'Flow_Rate',\n",
    "                 'Lake_Level']\n",
    "    col_inds = []\n",
    "    \n",
    "    for n in range(len(datatypes)):\n",
    "        col_ind_type = []\n",
    "        for c in range(len(names)):\n",
    "            if datatypes[n] in names[c]:\n",
    "                col_ind_type.append(c)\n",
    "        col_inds.append(col_ind_type)\n",
    "        \n",
    "    return datatypes, col_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting functions \n",
    "\n",
    "def plot_data_preprocessed(df, col_ind):\n",
    "    data_ts, _ = preprocess(df, col_ind)\n",
    "    data_ts_int, _ = preprocess_int(df, col_ind)\n",
    "    data_ts_size = data_ts.size\n",
    "    data_ts_int_size = data_ts_int.size\n",
    "    time_array = np.linspace(0, data_ts_size-1, data_ts_size)\n",
    "    time_array2 = np.linspace(0, data_ts_int_size-1, data_ts_int_size)\n",
    "    plt.figure()\n",
    "    plt.scatter(time_array, data_ts, s=0.2, alpha=0.6)\n",
    "    plt.scatter(time_array2, data_ts_int, s=0.2, alpha=0.6)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_preprocessing(input_raw, input_pp, input_name, target_ts, target_name,\n",
    "                       crop_min=1600, crop_max=2000, title=None):\n",
    "    \"\"\"\n",
    "    Plot raw data and preprocessed data compared to target variable\n",
    "    \n",
    "    \"\"\"\n",
    "    cmap = plt.get_cmap('Dark2')\n",
    "    colors = [cmap(i) for i in np.linspace(0, 1, 8)]\n",
    "    len_data = input_raw.size\n",
    "    fig, ax = plt.subplots(3, 2, sharey=True, figsize=(9, 4))\n",
    "    y_max = 1.2\n",
    "    \n",
    "    ax[1, 0].set_ylim(0, y_max)\n",
    "    ax[0, 0].plot(normalise_0_to_1(input_raw), color=colors[0], alpha=0.8)\n",
    "    ax[1, 0].plot(normalise_0_to_1(input_pp), color=colors[1], alpha=0.8)\n",
    "    ax[2, 0].plot(normalise_0_to_1(target_ts), color=colors[2], alpha=0.8)\n",
    "    label1 = '{}, raw data'.format(input_name)\n",
    "    label2 = '{}, pre-processed'.format(input_name)\n",
    "    label3 = target_name\n",
    "    ax[0, 1].text(0.01, 0.95, label1, transform=ax[0, 1].transAxes, va='top', ha='left')\n",
    "    ax[1, 1].text(0.01, 0.95, label2, transform=ax[1, 1].transAxes, va='top', ha='left')\n",
    "    ax[2, 1].text(0.01, 0.95, label3, transform=ax[2, 1].transAxes, va='top', ha='left')\n",
    "    \n",
    "    xmin = crop_min\n",
    "    xmax = crop_max\n",
    "    for n in range(3):\n",
    "        ax[n, 0].plot([xmin, xmin], [0, y_max], 'k--', alpha=0.6)\n",
    "        ax[n, 0].plot([xmax, xmax], [0, y_max], 'k--', alpha=0.6)\n",
    "        ax[n, 0].set_xlim(0, len_data)\n",
    "        ax[n, 1].set_xlim(xmin, xmax)\n",
    "        con = mpl.patches.ConnectionPatch(xyA=[len_data, y_max/2], coordsA=ax[n, 0].transData,\n",
    "                                          xyB=[xmin, y_max/2], coordsB=ax[n, 1].transData,\n",
    "                                          arrowstyle='->')\n",
    "        fig.add_artist(con)\n",
    "        \n",
    "    days = np.arange(xmin, xmax) \n",
    "    ax[0, 1].plot(days, normalise_0_to_1(input_raw)[xmin: xmax], color=colors[0], alpha=0.8)\n",
    "    ax[1, 1].plot(days, normalise_0_to_1(input_pp)[xmin: xmax], color=colors[1], alpha=0.8)\n",
    "    ax[2, 1].plot(days, normalise_0_to_1(target_ts)[xmin: xmax], color=colors[2], alpha=0.8)\n",
    "    \n",
    "    for n in range(2):\n",
    "        ax[n, 0].axes.xaxis.set_ticklabels([])\n",
    "        ax[n, 1].axes.xaxis.set_ticklabels([])\n",
    "        ax[2, n].set_xlabel('Time [days]')\n",
    "        \n",
    "    if title is None:\n",
    "        title = input_name + ' raw and pre-processed data, compared to ' + target_name\n",
    "    fig.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def plot_correl_matrix(data, names, title=None):\n",
    "    \"\"\"\n",
    "    Plot cross-correlation matrix for input data, using Spearman's rank\n",
    "    data should have shape: (num variables, num samples)\n",
    "    names should be list of length (num variables)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Calculate cross-correlation\n",
    "    cross_corr, p_value = stats.spearmanr(data.T, nan_policy='omit')\n",
    "    \n",
    "    # Plot results\n",
    "    numdata = len(names)\n",
    "    if title is None:\n",
    "        title = 'Cross-correlation matrix'\n",
    "    fig = plt.figure(figsize=(9, 9))\n",
    "    ax = plt.subplot(111)\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.08)\n",
    "    norm = mpl.colors.Normalize(vmin=-1, vmax=1)\n",
    "    sc_map = mpl.cm.ScalarMappable(norm=norm, cmap='PiYG')\n",
    "    ms = ax.imshow(cross_corr, cmap='PiYG', interpolation='none', vmin=-1, vmax=1)\n",
    "    fig.suptitle(title)\n",
    "    ax.set_xticks(np.arange(numdata))\n",
    "    ax.set_yticks(np.arange(numdata))\n",
    "    ax.set_xticklabels(names)\n",
    "    ax.set_yticklabels(names)\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    plt.setp(ax.get_xticklabels(), rotation=60, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "    plt.colorbar(mappable=sc_map, cax=cax)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "      \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tau analysis functions \n",
    "\n",
    "def find_tau_correlation(target_ts, rain_ts, tau_array=None):\n",
    "    \"\"\"\n",
    "    Calculate convolution of rainfall data with an exponential window\n",
    "    with time constant tau, for a range of values of tau.\n",
    "    Then determine how correlated these convolved signals are to the\n",
    "    target data by calculating Spearman's rank correlation coefficient\n",
    "    for each value of tau\n",
    "    \n",
    "    \"\"\"\n",
    "    if tau_array is None:\n",
    "        tau_array = np.linspace(2, 120, 60)\n",
    "        \n",
    "    rain_ts = np.asarray(rain_ts)\n",
    "    target_ts = np.asarray(target_ts)\n",
    "    winlength = rain_ts.size\n",
    "    target_len = target_ts.size\n",
    "    \n",
    "    t = np.linspace(0, winlength-1, winlength)\n",
    "    src = np.empty(len(tau_array))\n",
    "    \n",
    "    for n in range(len(tau_array)):\n",
    "        exp_win = np.exp(-t/tau_array[n])\n",
    "        rain_conv = np.convolve(rain_ts, exp_win, 'full')[:target_len]\n",
    "        rain_conv = rain_conv / np.sum(exp_win)\n",
    "        # if n==40:\n",
    "        #     plt.figure()\n",
    "        #     plt.plot(rain_ts, label='rain_ts')\n",
    "        #     plt.plot(rain_conv, label='rain_conv')\n",
    "        #     plt.plot(target_ts, label='target_ts')\n",
    "        #     plt.legend()\n",
    "        #     plt.show()\n",
    "        src[n], _ = stats.spearmanr(target_ts, rain_conv)\n",
    "        \n",
    "    return src, tau_array\n",
    "\n",
    "\n",
    "def find_best_tau(target_ts, rain_ts, plot=True, rain_name=None, target_name=None, tau_array=None):\n",
    "    \"\"\"\n",
    "    Calculate correlation of convolved rainfall signal with the\n",
    "    target signal for different time constants of exponential window\n",
    "    and select the tau value that gives the best correlation.\n",
    "    Optional plot of correlation for different tau values.\n",
    "    \n",
    "    \"\"\"\n",
    "    tau_best = []\n",
    "    src_best = []\n",
    "    if plot: plt.figure()\n",
    "    for n in range(len(rain_ts)):\n",
    "        src, tau_array = find_tau_correlation(\n",
    "                                normalise_0_to_1(target_ts),\n",
    "                                normalise_0_to_1(rain_ts[n]),\n",
    "                                tau_array=tau_array)\n",
    "        tau_best.append(tau_array[np.argmax(src)])\n",
    "        src_best.append(src[np.argmax(src)])\n",
    "        if plot: plt.plot(tau_array, src, label=rain_name[n], lw=1.5, alpha=0.7)  \n",
    "            \n",
    "    if plot:\n",
    "        plt.xlabel(\"Time constant (tau) for convolution with exponential window\")\n",
    "        plt.ylabel(\"Spearman's Rank correlation coefficient\")\n",
    "        title_text = 'Correlation of convolved rainfall data with {} for different tau values'.format(target_name)\n",
    "        plt.title(title_text, wrap=True)\n",
    "        min_corr = np.min(src)\n",
    "        if min_corr < 0:\n",
    "            plt.ylim(min_corr, 1)\n",
    "        else:\n",
    "            plt.ylim(0, 1)\n",
    "        plt.legend(fontsize=9)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    return tau_best, src_best\n",
    "\n",
    "\n",
    "def find_all_best_tau(input_ts, input_name, target_ts, target_name, tau_array=None,\n",
    "                      target_0_tau_array=None):\n",
    "    \"\"\"\n",
    "    Calculating optimum tau for each input/target combination that\n",
    "    maximises the Spearman's Rank correlation coefficient when the input is\n",
    "    convolved with an exponential window (time constant tau)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Find best tau values\n",
    "    all_tau_best = []\n",
    "    all_src_best = []\n",
    "    for n in range(len(target_ts)):\n",
    "        if n == 0 and target_0_tau_array is not None:\n",
    "            tau_ = target_0_tau_array\n",
    "        else:\n",
    "            tau_ = tau_array\n",
    "        tau_best, src_best = find_best_tau(target_ts[n], input_ts, plot=False,\n",
    "                                           tau_array=tau_)\n",
    "        all_tau_best.append(tau_best)\n",
    "        all_src_best.append(src_best)\n",
    "    all_tau_best = np.asarray(all_tau_best)\n",
    "    all_src_best = np.asarray(all_src_best)\n",
    "\n",
    "    # Plot results\n",
    "    fig = plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=0.3, pad=0.08)\n",
    "#     cbar_min = np.min(all_tau_best)\n",
    "#     cbar_max = np.max(all_tau_best[1:, :]) + 5\n",
    "    cbar_min = 0\n",
    "    cbar_max = 1\n",
    "    norm = mpl.colors.Normalize(vmin=cbar_min, vmax=cbar_max)\n",
    "    sc_map = mpl.cm.ScalarMappable(norm=norm, cmap='viridis')\n",
    "    ms = ax.imshow(all_src_best, norm=norm, cmap='viridis')\n",
    "    for i in range(len(target_name)):\n",
    "        for j in range(len(input_name)):\n",
    "            if (cbar_max - all_tau_best[i, j]) / (cbar_max - cbar_min) < 0.2:\n",
    "                text_col = 'k'\n",
    "            else:\n",
    "                text_col = 'w'\n",
    "            text = ax.text(j, i, np.int(all_tau_best[i, j]),\n",
    "                           ha=\"center\", va=\"center\", color=text_col,\n",
    "                           fontsize=8)\n",
    "    fig.suptitle('Correlation of convolved rainfall with target, for optimum tau values')\n",
    "    ax.set_xticks(np.arange(len(input_name)))\n",
    "    ax.set_yticks(np.arange(len(target_name)))\n",
    "    ax.set_xticklabels(input_name)\n",
    "    ax.set_yticklabels(target_name)\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    plt.setp(ax.get_xticklabels(), rotation=60, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "#     plt.colorbar(mappable=sc_map, cax=cax, extend='max')\n",
    "    plt.colorbar(mappable=sc_map, cax=cax)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return all_tau_best\n",
    "\n",
    "\n",
    "def tau_and_lag_correl(target_ts, rain_ts, lag_array=None, tau_array=None,\n",
    "                       plot=True):\n",
    "    \"\"\"\n",
    "    Calculate correlation coefficients for different time lag and tau\n",
    "    values for rainfall. Optional plot.\n",
    "\n",
    "    \"\"\"\n",
    "    # Set default lag and tau values to analyse\n",
    "    if lag_array is None:\n",
    "        lag_array = np.arange(20)\n",
    "    if tau_array is None:\n",
    "        tau_array = np.linspace(22, 90, 35).astype(np.int)\n",
    "        \n",
    "    # Calculate Spearman's Rank coefficient for each tau and lag combination\n",
    "    sp_rank_cc = []\n",
    "    for n in range(len(lag_array)):\n",
    "        lag = lag_array[n]\n",
    "        data_length = rain_ts.size\n",
    "        if lag > 0:\n",
    "            data_lag = np.zeros(data_length)\n",
    "            data_lag[lag:] = rain_ts[:-lag]\n",
    "            data_lag[:lag] = rain_ts[0]\n",
    "        else:\n",
    "            data_lag = rain_ts\n",
    "        src, _ = find_tau_correlation(target_ts, data_lag,\n",
    "                                      tau_array=tau_array)\n",
    "        sp_rank_cc.append(src)\n",
    "    sp_rank_cc = np.asarray(sp_rank_cc)\n",
    "    \n",
    "    # Plot matrix showing correlation for each tau and lag combination\n",
    "    if plot:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        cax = ax.matshow(sp_rank_cc)\n",
    "        fig.colorbar(cax)\n",
    "        ax.set_xticks(np.arange(len(tau_array)))\n",
    "        ax.set_yticks(np.arange(len(lag_array)))\n",
    "        ax.set_xticklabels(tau_array)\n",
    "        ax.set_yticklabels(lag_array)\n",
    "        ax.xaxis.set_ticks_position('bottom')\n",
    "        plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n",
    "                 rotation_mode=\"anchor\")\n",
    "        ax.set_xlabel('Tau for exponential window')\n",
    "        ax.set_ylabel('Time lag [days]')\n",
    "        plt.title('Correlation of rainfall with target for different values of tau and time lag')\n",
    "        fig.show()\n",
    "        \n",
    "    return sp_rank_cc, lag_array, tau_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volume pre-processing\n",
    "\n",
    "def preprocess_vol(vol_data, smooth_amt_1=800, smooth_amt_2=30):\n",
    "    \"\"\"Pre-process volume data by smoothing, calculating deviation from the smoothed data,\n",
    "    and then smoothing again\"\"\"\n",
    "    deviation = vol_data - movingav(vol_data, smooth_amt_1, winfunc=np.hanning)\n",
    "    smoothed = movingav(deviation, smooth_amt_2, winfunc=np.hanning)\n",
    "    return normalise_0_to_1(smoothed)\n",
    "    \n",
    "\n",
    "def find_best_vol_params(vol_ts, target_ts, smoothing_1=None, smoothing_2=None, plot=True):\n",
    "    \"\"\"Calculate the correlation of the preprocessed volume data with the target data for\n",
    "    different lengths of smoothing window, in order to find the most appropropriate settings\"\"\"\n",
    "    if smoothing_1 is None:\n",
    "        smoothing_1 = np.linspace(50, 2000, 40).astype(np.int)\n",
    "    if smoothing_2 is None:\n",
    "        smoothing_2 = np.linspace(2, 40, 20).astype(np.int)\n",
    "    num_smth_1 = smoothing_1.size\n",
    "    num_smth_2 = smoothing_2.size\n",
    "    coeffs = np.empty((num_smth_1, num_smth_2))\n",
    "    for i in range(num_smth_1):\n",
    "        for j in range(num_smth_2):\n",
    "            vol_processed = preprocess_vol(vol_ts, smooth_amt_1=smoothing_1[i],\n",
    "                                           smooth_amt_2=smoothing_2[j])\n",
    "            corr_coeff, _ = stats.spearmanr(vol_processed, target_ts)\n",
    "            coeffs[i, j] = corr_coeff\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        ax = plt.subplot(111)\n",
    "        im = ax.imshow(coeffs.T)\n",
    "#         norm = mpl.colors.Normalize(vmin=0, vmax=1)\n",
    "#         sc_map = mpl.cm.ScalarMappable(norm=norm, cmap='viridis')\n",
    "#         plt.colorbar(mappable=sc_map)\n",
    "        ax.figure.colorbar(im, ax=ax)\n",
    "        ax.set_xticks(np.arange(num_smth_1))\n",
    "        ax.set_yticks(np.arange(num_smth_2))\n",
    "        ax.set_xticklabels(smoothing_1)\n",
    "        ax.set_yticklabels(smoothing_2)\n",
    "        ax.xaxis.set_ticks_position('bottom')\n",
    "        plt.setp(ax.get_xticklabels(), rotation=60, ha=\"right\",\n",
    "                 rotation_mode=\"anchor\")\n",
    "        plt.show()\n",
    "        \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelling functions \n",
    "\n",
    "\n",
    "def run_model(target_data, input_data, model_type='LSTM', use_target=True, look_back=30,\n",
    "              chunk_step=50, train_ratio=0.67, num_epochs=30, batch_size=5, predict_steps=15,\n",
    "              plot=True, verbose=0, MLP_layers=None, print_score=False):\n",
    "    \"\"\"\n",
    "    Wrapper function for machine learning model\n",
    "    \n",
    "    model_type defines which machine learning model to use:\n",
    "        - If model_type is 'MLP', model will be a multi-layer perceptron model.\n",
    "        - Otherwise (e.g. for default value 'LSTM'), model will be an LSTM model.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Normalize the target dataset\n",
    "    target_data = np.reshape(target_data, (target_data.size, 1))\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    target_scaled = scaler.fit_transform(target_data)\n",
    "    target_scaled = np.squeeze(target_scaled)\n",
    "\n",
    "    # Normalise the input variables\n",
    "    num_input = model_input_data.shape[0]\n",
    "    input_data_scaled = np.empty(input_data.shape)\n",
    "    for n in range(num_input):\n",
    "        input_data_scaled[n, :] = normalise_0_to_1(input_data[n, :])\n",
    "\n",
    "    # Combine target and inputs into single array\n",
    "    num_timesteps = target_data.size\n",
    "    model_data = np.empty((num_input + 1, num_timesteps))\n",
    "    model_data[0, :] = target_scaled\n",
    "    model_data[1:, :] = input_data_scaled\n",
    "#     print('Model_data shape: {}'.format(model_data.shape))\n",
    "\n",
    "    # Plot preprocessed data\n",
    "    if plot:\n",
    "        plt.figure(figsize=(8, 3))\n",
    "        plt.plot(model_data[0, :], label=target_name[target_ind])\n",
    "        plt.plot(model_data[1, :], label='input')\n",
    "        plt.legend()\n",
    "        plt.title('Preprocessed data')\n",
    "        plt.show()\n",
    "\n",
    "    # Split data into chunks with random order\n",
    "    x, y, y_ind = create_dataset(model_data, use_target=use_target, look_back=look_back,\n",
    "                                 chunk_step=chunk_step, predict_steps=predict_steps)\n",
    "    numchunk = y.shape[0]\n",
    "#     print('x shape: {}'.format(x.shape))\n",
    "#     print('y shape: {}'.format(y.shape))\n",
    "\n",
    "    # split into train and test sets\n",
    "    train_size = int(numchunk * train_ratio)\n",
    "    test_size = numchunk - train_size\n",
    "    trainX, testX = x[0:train_size, :, :], x[train_size:numchunk, :, :]\n",
    "    trainY, testY = y[0:train_size, :], y[train_size:numchunk, :]\n",
    "    trainYind, testYind = y_ind[0:train_size], y_ind[train_size:numchunk]\n",
    "    \n",
    "    # Plot one example of chunk\n",
    "    if plot:\n",
    "        sample_num = 0\n",
    "        plt.figure(figsize=(8, 3))\n",
    "        time_ = np.arange(look_back)\n",
    "        plt.plot(time_, trainX[sample_num, :, 0], label='Input: target')\n",
    "        plt.plot(time_, trainX[sample_num, :, 1], label='Input: rain')\n",
    "        plt.plot([look_back], trainY[sample_num, 0], 'xr', label='Output: target')\n",
    "        plt.legend()\n",
    "        plt.title(\"Example of one chunk from dataset\")\n",
    "        plt.show()\n",
    "\n",
    "    # create and fit the LSTM network\n",
    "    num_features = x.shape[2]\n",
    "    \n",
    "    if model_type == 'MLP':\n",
    "        # flatten inputs for MLP model\n",
    "        n_input = x.shape[1] * x.shape[2]\n",
    "        trainX = trainX.reshape((trainX.shape[0], n_input))\n",
    "        testX = testX.reshape((testX.shape[0], n_input))\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    if model_type == 'MLP':\n",
    "        model.add(InputLayer(input_shape=(n_input, )))\n",
    "        if MLP_layers is not None:\n",
    "            for n in range(len(MLP_layers)):\n",
    "                model.add(Dense(MLP_layers[n], activation='relu'))\n",
    "        else:\n",
    "            model.add(Dense(500, activation='relu'))\n",
    "            model.add(Dense(100, activation='relu'))\n",
    "    else:\n",
    "        model.add(InputLayer(input_shape=(look_back, num_features)))\n",
    "        model.add(LSTM(4))\n",
    "    model.add(Dense(predict_steps))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.fit(trainX, trainY, epochs=num_epochs, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "    # make predictions\n",
    "    trainPredict = model.predict(trainX)\n",
    "    testPredict = model.predict(testX)\n",
    "\n",
    "    # invert predictions\n",
    "    trainPredict = scaler.inverse_transform(trainPredict)\n",
    "    trainY = scaler.inverse_transform(trainY)\n",
    "    testPredict = scaler.inverse_transform(testPredict)\n",
    "    testY = scaler.inverse_transform(testY)\n",
    "\n",
    "    # calculate RMSE and MSE\n",
    "    trainRMSE = mean_squared_error(trainY, trainPredict, multioutput='raw_values', squared=False)\n",
    "    testRMSE = mean_squared_error(testY, testPredict, multioutput='raw_values', squared=False)\n",
    "    trainMSE = mean_squared_error(trainY, trainPredict, multioutput='raw_values', squared=True)\n",
    "    testMSE = mean_squared_error(testY, testPredict, multioutput='raw_values', squared=True)\n",
    "    \n",
    "    # Print RMSE scores\n",
    "    if print_score:\n",
    "        print('Train Score: %.2f RMSE' % (trainRMSE[0]))\n",
    "        print('Test Score: %.2f RMSE' % (testRMSE[0]))\n",
    "    \n",
    "    return model, trainRMSE, testRMSE, trainMSE, testMSE, testYind, testPredict, \\\n",
    "            trainYind, trainPredict\n",
    "\n",
    "\n",
    "class ML_model(object):\n",
    "    \"\"\"\n",
    "    Stores a set of input data, target data, hyperparams and results for machine learning model.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, target_data, input_data, use_target=True, look_back=30,\n",
    "                 chunk_step=50, train_ratio=0.67, num_epochs=30, batch_size=5,\n",
    "                 predict_steps=15, plot=False, verbose=0):\n",
    "        \"\"\"\n",
    "        Initialise ML_model object\n",
    "        target_data: output variable for model, should have shape (num_timesteps)\n",
    "        input_data: input data for model, should have shape (numfeatures, num_timesteps)\n",
    "        use_target: if True, target variable will be used as an input, otherwise it will be excluded\n",
    "        look_back: number of timesteps behind output that the model will use\n",
    "        chunk_step: the spacing between chunks of input data (smaller number means more chunks)\n",
    "        train_ratio: proportion of total chunks that are used to train the model\n",
    "        num_epochs: Number of epochs in the model training\n",
    "        batch_size: batch size for the model training\n",
    "        predict_steps: number of timesteps that the model will predict ahead of input data\n",
    "        plot: if True, plots illustrating model will be shown\n",
    "        verbose: defines how much info on model training is output (can be 0, 1 or 2)\n",
    "        \n",
    "        \"\"\"\n",
    "        self.target_data = target_data\n",
    "        self.input_data = input_data\n",
    "        self.use_target = use_target\n",
    "        self.look_back = look_back\n",
    "        self.chunk_step = chunk_step\n",
    "        self.train_ratio = train_ratio\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.predict_steps = predict_steps\n",
    "        self.model_plot = plot\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def run(self, model_type='LSTM', print_score=False):\n",
    "        \"\"\"\n",
    "        Creates and runs a model, of the type specified by 'model_type':\n",
    "          - If model_type is 'MLP', model will be a multi-layer perceptron model.\n",
    "          - Otherwise (e.g. for default value 'LSTM'), model will be an LSTM model.        \n",
    "        Outputs are stored in class variables:\n",
    "        self.trainRMSE: RMSE score for predictions from training data\n",
    "        self.testRMSE: RMSE score for predictions from test data\n",
    "        self.testYind: timestep indices of the prediction from test chunks\n",
    "        self.trainYind: timestep indices of the prediction from training chunks\n",
    "        self.testPredict: Predictions from test data\n",
    "        self.trainPredict: Predictions from training data\n",
    "        \"\"\"\n",
    "        self.model_type = model_type\n",
    "        self.model, self.trainRMSE, self.testRMSE, self.trainMSE, \\\n",
    "        self.testMSE, self.testYind, self.testPredict, \\\n",
    "        self.trainYind, self.trainPredict = run_model(target_data=self.target_data,\n",
    "                                                      input_data=self.input_data,\n",
    "                                                      model_type=self.model_type,\n",
    "                                                      use_target=self.use_target,\n",
    "                                                      look_back=self.look_back,\n",
    "                                                      chunk_step=self.chunk_step,\n",
    "                                                      train_ratio=self.train_ratio,\n",
    "                                                      num_epochs=self.num_epochs,\n",
    "                                                      batch_size=self.batch_size,\n",
    "                                                      predict_steps=self.predict_steps,\n",
    "                                                      plot=self.model_plot,\n",
    "                                                      verbose=self.verbose,\n",
    "                                                      print_score=print_score)\n",
    "        \n",
    "        self.weights = []\n",
    "        for n in range(len(self.model.layers)):\n",
    "            self.weights.append(self.model.layers[n].get_weights()[0])\n",
    "            \n",
    "        return\n",
    "\n",
    "    \n",
    "    def plot_predict_one_step(self, crop_min=None, crop_max=None, title=None):\n",
    "        \"\"\"\n",
    "        Plot predictions from test and training samples against original data,\n",
    "        for first timestep prediction only (i.e. no lookahead)\n",
    "        x-axis (time) can be cropped to between 'crop_min' and 'crop_max' to show\n",
    "        a particular time range if desired.\n",
    "        \n",
    "        \"\"\"\n",
    "        if title == None:\n",
    "            title = \"Predictions (single timestep) compared to original dataset\"\n",
    "        \n",
    "        \n",
    "        plt.figure(figsize=(9, 4))\n",
    "        plt.plot(np.linspace(0, self.target_data.size-1, self.target_data.size),\n",
    "                 self.target_data,\n",
    "                 label='Original data')\n",
    "        plt.plot(self.testYind, self.testPredict[:, 0], 'xr', markeredgewidth=0.8,\n",
    "                 markersize=4, markeredgecolor=[0.9, 0.2, 0.2, 0.8],\n",
    "                 label='Test predictions')\n",
    "        plt.plot(self.trainYind, self.trainPredict[:, 0], 'xg', markeredgewidth=0.8,\n",
    "                 markersize=4, markeredgecolor=[0.2, 0.7, 0.3, 0.8],\n",
    "                 label='Train predictions')\n",
    "        plt.legend()\n",
    "        plt.title(title)\n",
    "        if crop_min is not None and crop_max is not None:\n",
    "            plt.xlim(crop_min, crop_max)\n",
    "        plt.show()\n",
    "        \n",
    "        return\n",
    "\n",
    "    \n",
    "    def plot_predict_all_steps(self, crop_min=None, crop_max=None):\n",
    "        \"\"\"\n",
    "        Plot predictions from test and training samples against original data,\n",
    "        for all timesteps of prediction (i.e. with lookahead)\n",
    "        x-axis (time) can be cropped to between 'crop_min' and 'crop_max' to show\n",
    "        a particular time range if desired.\n",
    "        \n",
    "        \"\"\"        \n",
    "        plt.figure(figsize=(9, 4))\n",
    "        plt.plot(np.linspace(0, self.target_data.size-1, self.target_data.size),\n",
    "                 self.target_data,\n",
    "                 label='Original data')\n",
    "        for n in range(len(self.testYind)):\n",
    "            if n == 0:\n",
    "                plt.plot(np.arange(self.testYind[n], self.testYind[n] + self.predict_steps),\n",
    "                         self.testPredict[n, :], '-r', lw=1, color=[0.9, 0.2, 0.2, 0.8], label='Test predictions')\n",
    "            else:\n",
    "                plt.plot(np.arange(self.testYind[n], self.testYind[n] + self.predict_steps),\n",
    "                         self.testPredict[n, :], '-r', lw=1, color=[0.9, 0.2, 0.2, 0.8])\n",
    "        for n in range(len(self.trainYind)):\n",
    "            if n == 0:\n",
    "                plt.plot(np.arange(self.trainYind[n], self.trainYind[n] + self.predict_steps),\n",
    "                         self.trainPredict[n, :], '-g', lw=1, color=[0.2, 0.7, 0.3, 0.8], label='Train predictions')\n",
    "            else:\n",
    "                plt.plot(np.arange(self.trainYind[n], self.trainYind[n] + self.predict_steps),\n",
    "                         self.trainPredict[n, :], '-g', lw=1, color=[0.2, 0.7, 0.3, 0.8])\n",
    "        plt.legend()\n",
    "        plt.title(\"Predictions (all timesteps) compared to original dataset\")\n",
    "        if crop_min is not None and crop_max is not None:\n",
    "            plt.xlim(crop_min, crop_max)\n",
    "        plt.show()\n",
    "        \n",
    "        return\n",
    "    \n",
    "        \n",
    "    def plot_weights(self, feature_names):\n",
    "        \"\"\"\n",
    "        Plot weights matrix\n",
    "        \n",
    "        \"\"\"\n",
    "        layer_1_mean = np.mean(self.weights[0], axis=1)\n",
    "        if self.use_target:\n",
    "            layer_1_mean = layer_1_mean.reshape((self.look_back, self.input_data.shape[0] + 1))\n",
    "        else:\n",
    "            layer_1_mean = layer_1_mean.reshape((self.look_back, self.input_data.shape[0]))\n",
    "            \n",
    "        plt.figure()\n",
    "        ax = plt.subplot(111)\n",
    "        im = ax.imshow(layer_1_mean.T)\n",
    "        ax.set_yticks(np.arange(len(feature_names)))\n",
    "        ax.set_xticks(np.arange(self.look_back))\n",
    "        ax.set_yticklabels(feature_names)\n",
    "        ax.set_xticklabels(np.linspace(-self.look_back, -1, self.look_back).astype(np.int))\n",
    "        ax.xaxis.set_ticks_position('bottom')\n",
    "        plt.setp(ax.get_xticklabels()[::2], visible=False)\n",
    "        plt.setp(ax.get_yticklabels(), ha=\"right\")\n",
    "        ax.set_title(\"First model layer: average weights\")\n",
    "        ax.set_xlabel('Input sample timestep before prediction [days]')\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes(\"right\", \"5%\", pad=\"5%\")\n",
    "        plt.colorbar(im, cax=cax)\n",
    "        plt.show()\n",
    "        \n",
    "        return\n",
    "    \n",
    "\n",
    "def compile_model_input_variables(model_rain_ind,\n",
    "                                  model_vol_ind,\n",
    "                                  model_temp_ind,\n",
    "                                  model_hydro_ind,\n",
    "                                  rain_pp,\n",
    "                                  vol_pp,\n",
    "                                  temp_pp,\n",
    "                                  hydro_pp,\n",
    "                                  num_timesteps):\n",
    "    \"\"\"\n",
    "    Extract the selected variables from the preprocessed data by their indices\n",
    "    and populate an input array to be used by the model.\n",
    "    The input array will have shape: (num features, num timesteps)\n",
    "    \n",
    "    \"\"\"    \n",
    "    num_inputs = len(model_rain_ind) + len(model_vol_ind) + \\\n",
    "                 len(model_temp_ind) + len(model_hydro_ind)\n",
    "    model_input_data = np.empty((num_inputs, num_timesteps))\n",
    "    count = 0\n",
    "    for i in model_rain_ind:\n",
    "        model_input_data[count, :] = rain_pp[i]\n",
    "        count += 1\n",
    "    for i in model_vol_ind:\n",
    "        model_input_data[count, :] = vol_pp[i]\n",
    "        count += 1\n",
    "    for i in model_temp_ind:\n",
    "        model_input_data[count, :] = temp_pp[i]\n",
    "        count += 1\n",
    "    for i in model_hydro_ind:\n",
    "        print()\n",
    "        model_input_data[count, :] = hydro_pp[i]\n",
    "        count += 1\n",
    "    \n",
    "    return model_input_data\n",
    "\n",
    "\n",
    "def create_dataset(dataset, look_back=1, chunk_step=1, predict_steps=1, use_target=True):\n",
    "    \"\"\"\n",
    "    Convert data set into a dataset matrix by splitting the data into a number of 'chunks'\n",
    "    that are randomly ordered. Each chunk has an input (x) of length 'look_back', and an\n",
    "    output (y) of length 'predict_steps'. The indices of each chunk within the original array\n",
    "    are stored in 'y_ind'.\n",
    "    The target should be the first feature in the dataset.\n",
    "    If 'use_target' is True, the target will be used as an input (i.e. will be part of x),\n",
    "    otherwise it will be excluded from x and only used in y.\n",
    "    The shape of the input and output arrays is as follows:\n",
    "    dataset: (num features, num timesteps)\n",
    "    x:       (num chunks, look_back, num features)\n",
    "    y:       (num chunks, predict_steps)\n",
    "    y_ind:   (num chunks)\n",
    "    \n",
    "    \"\"\"\n",
    "    numchunk = int(np.floor((dataset.shape[1] - look_back - predict_steps - 1) / chunk_step))\n",
    "    if use_target:\n",
    "        dataX = np.empty((numchunk, look_back, dataset.shape[0]))\n",
    "    else:\n",
    "        if dataset.shape[0] == 1:\n",
    "            print('No input data in dataset, try adding features or setting use_target=True')\n",
    "        dataX = np.empty((numchunk, look_back, dataset.shape[0] - 1))\n",
    "    dataY = np.empty((numchunk, predict_steps))\n",
    "    y_ind = []\n",
    "    \n",
    "    # Create chunks of data with the specified look back\n",
    "    for i in range(numchunk):\n",
    "        start_ind = chunk_step*i\n",
    "        if use_target:\n",
    "            dataX[i, :, :] = dataset[:, start_ind:(start_ind + look_back)].T\n",
    "        else:\n",
    "            dataX[i, :, :] = dataset[1:, start_ind:(start_ind + look_back)].T\n",
    "        dataY[i, :] = dataset[0, start_ind+look_back:start_ind+look_back+predict_steps] #MG\n",
    "        #dataY.append(dataset[0, start_ind + look_back])\n",
    "        y_ind.append(start_ind + look_back)\n",
    "        \n",
    "    # Randomise order of chunks\n",
    "    rand_indices = np.random.permutation(numchunk)\n",
    "    x = np.array(dataX)\n",
    "    y = np.array(dataY)\n",
    "    y_ind = np.array(y_ind)\n",
    "    x = x[rand_indices, :]\n",
    "    y = y[rand_indices,:]\n",
    "    #y = np.reshape(y, (y.size, 1)) # MG\n",
    "    y_ind = y_ind[rand_indices]\n",
    "    \n",
    "    return x, y, y_ind\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aquifier Auser\n",
    "## Data pre-processing\n",
    "### Handling missing data\n",
    "First, the data was read in and an analysis of missing data was carried out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Pre-processing data\n",
    "\n",
    "# Importing all data into pandas dataframe\n",
    "foldpath = r\"acea-water-prediction\"\n",
    "files = sorted(list(Path(foldpath).rglob('*.csv')))\n",
    "df = pd.read_csv(files[0])\n",
    "\n",
    "# Find and plot gaps in the data\n",
    "all_data, col_names, is_nan, is_zero = find_data_gaps(df, plot=True, title=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows that there is a significant amount of data missing over the total time period, either in the form of NaNs, or zeros. In the case of the rainfall data, the zeros are considered to be 'real data' since there are many days with zero rainfall, however for the other data it is assumed that zeros imply missing data.\n",
    "\n",
    "When deciding how to handle these gaps in the data, the aim was to find a compromise between maintaining as much of the original data as possible, and not introducing too much inaccuracy from filling in the gaps. For each variable, we consider a combination of three different options for handling these gaps:\n",
    "1. Crop the entire dataset over a certain time range to avoid the gaps. If many of the variables contain missing data for the same time period, then this option is sensible as this time period is unlikely to be useful for the model.\n",
    "2. Remove the variable completely. This may be necessary if there is missing data for large time periods, particularly if many other variables contain data in those time periods. For example, 'Volume_CSA' and 'Volume CSAL' both contain large gaps in the data where there is data for most other variables (from days ~3000-6000, so it makes sense to remove these from the analysis.\n",
    "3. Fill in the missing data for that variable by prediction (e.g. interpolation, backpropagation). Where the gaps in the data are relatively short and make up a small proportion of the total data, this option is likely to be preferable.\n",
    "\n",
    "Since there are NaNs in most of the data for the first ~3000 days, including all of the target variables, it was decided that this time period would not be used for the model at all.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find column indices for different datatypes\n",
    "datatypes, col_inds = find_datatypes(df)\n",
    "\n",
    "# Get time series data for all variable types\n",
    "start_ind = 3955\n",
    "rain_ts, rain_name = get_time_series(col_inds[0], start_ind=start_ind, fill_zeros=False)\n",
    "target_ts, target_name = get_time_series(col_inds[1], start_ind=start_ind)\n",
    "temp_ts, temp_name = get_time_series(col_inds[2], start_ind=start_ind)\n",
    "vol_ts, vol_name = get_time_series(col_inds[3], start_ind=start_ind)\n",
    "hydro_ts, hydro_name = get_time_series(col_inds[4], start_ind=start_ind)  \n",
    "\n",
    "print(target_ts[1][-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove zeros from target\n",
    "# N.B. SHOULD REPLACE THIS WITH A DIFFERENT METHOD\n",
    "for m in range(len(target_ts)):\n",
    "    for n in range(len(target_ts[m])):\n",
    "        if n > 0 and target_ts[m][n] == 0:\n",
    "            target_ts[m][n] = target_ts[m][n-1]\n",
    "\n",
    "# Extend target with 2 values missing from the end\n",
    "# to_append = target_ts[1][-1]*np.ones(2)\n",
    "# target_ts[1] = np.append(target_ts[1], target_ts[1][-1]*np.ones(2), 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rainfall\n",
    "Rainfall was expected to be one of the most useful variables in predicting groundwater levels, since it is one the key physical mechanisms that drives the inflow of water into aquifiers. From a visual inspection of the data, it was clear that many of the peaks in rainfall correspond approximately to peaks in the groundwater level. This was seen to apply both to the longer term trend, as well as on a smaller day-by-day scale. However, while the peaks in rainfall are often immediately followed by days of zero rainfall, the peaks in the groundwater data seem to decay more slowly over time.\n",
    "\n",
    "This led to the hypothesis that the rainfall on a particular day adds to the groundwater level relatively quickly, and then this level decays more slowly over several weeks. The physical explanation for this seems to be logical - if the rate of the rainfall is significantly larger than the rate that the water drains out of the groundwater subsystem, then the level will rise quickly and fall slowly (similar to the level of a slowly leaking cup when water is poured into it). For shallower subsystems, we may expect to see more short term reactions to the rainfall, as it would reach the subsystem more quickly and drain out more quickly; and conversely for deeper subsystems, we would expect the levels to react more slowly, with smaller short term effects.\n",
    "\n",
    "To test this hypothesis, the rainfall data was convolved with an exponential window function, for a range of different time constants. The correlation of these convolved signals with the 'depth to groundwater' variables was calculated to find the time constant that gave the best correlation for each combination of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Rainfall data analysis\n",
    "\n",
    "r_i = 0\n",
    "t_i = 1\n",
    "rain_pp = exp_convolve(rain_ts[r_i], tau=54)\n",
    "plot_preprocessing(rain_ts[r_i], rain_pp, rain_name[r_i], target_ts[t_i], target_name[t_i],\n",
    "                   crop_min=1600, crop_max=2000, title=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot example of correlation for different tau values\n",
    "\n",
    "tau_best, src_best = find_best_tau(target_ts=target_ts[1], rain_ts=rain_ts,\n",
    "                         rain_name=rain_name, plot=True,\n",
    "                         target_name=target_name[1],\n",
    "                         tau_array=np.linspace(2, 160, 80))\n",
    "\n",
    "tau_best, src_best = find_best_tau(target_ts=target_ts[0], rain_ts=rain_ts, plot=True,\n",
    "                         target_name=target_name[0],\n",
    "                         rain_name=rain_name,\n",
    "                         tau_array=np.linspace(20, 1600, 80))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating optimum tau for each rainfall/target combination that\n",
    "# maximises the Spearman's Rank correlation coefficient\n",
    "\n",
    "rain_tau = find_all_best_tau(rain_ts, rain_name, target_ts, target_name,\n",
    "                             target_0_tau_array=np.linspace(1, 2401, 80))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'Rainfall_Piaggione' from pre-processed data list due to missing values\n",
    "rain_filt = rain_ts.copy()\n",
    "rain_name_pp = rain_name.copy()\n",
    "del rain_filt[5]\n",
    "del rain_name_pp[5]\n",
    "\n",
    "# Create 3 different sets of pre-processed data\n",
    "#   - rain_pp_shallow is a list of size (num selected rain), where each element is a time series array. This\n",
    "#     array is just for the cross-correlation anaysis, so tau value is an average value of the best tau values for\n",
    "#     the shallow groundwater variables (all but LT2).\n",
    "#   - rain_pp_deep is a list of size (num selected rain), where each element is a time series array. This\n",
    "#     array is just for the cross-correlation anaysis, so tau value is the best tau values for the deep\n",
    "#     groundwater variable (LT2).\n",
    "#   - rain_pp_all is a list of size (num target variables, num selected hydro), where each element is a time\n",
    "#     series array. Each array has the preprocessed data with the optimum tau value for that particular\n",
    "#     combination of hydro/target. This will be used for the model.\n",
    "\n",
    "# Rainfall data preprocessing\n",
    "rain_pp_shallow = []\n",
    "rain_pp_deep = []\n",
    "rain_pp_all = []\n",
    "for n in range(len(rain_filt)):\n",
    "    tau_shallow = np.mean(rain_tau[1:, n])\n",
    "    tau_deep = np.mean(rain_tau[0, n])\n",
    "    rain_pp_shallow.append(exp_convolve(rain_filt[n], tau_shallow))\n",
    "    rain_pp_deep.append(exp_convolve(rain_filt[n], tau_deep))\n",
    "    rain_pp_ = []\n",
    "    for m in range(len(target_ts)):\n",
    "        tau_ = rain_tau[m, n]\n",
    "        rain_pp_.append(exp_convolve(rain_filt[n], tau_))\n",
    "    rain_pp_all.append(rain_pp_)\n",
    "\n",
    "# Flip the dimensions of the list\n",
    "rain_pp_all = [list(x) for x in zip(*rain_pp_all)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volume\n",
    "Initial inspection of the 'volume' data showed the following:\n",
    "- There is a relatively strong weekly pattern, and typically step changes every month. These are presumably linked to human activity/demand, and similar patterns are not clearly visible in the 'depth to groundwater' target variables. This suggests that it may be helpful to apply some smoothing to the volume data prior to modelling, in order to reduce the effect of the weekly/monthly variation.\n",
    "- There are some very long timescale trends in the ampltidue, for example both Volume_CC1 and Volume_CC2 both increase gradually over the whole time period of the data. This timescale of change (perhaps linked to human activity/demand changes) is not present in the depth to groundwater variables, so there may be some benefit in removing it from the volume data prior to modelling.\n",
    "\n",
    "In order to try to make the volume data more useful for modelling depth to groundwater, the following pre-processing was proposed based on the findings above:\n",
    "- Caclulate the deviation of the volume data from a smoothed version of the data. The smoothed version of the data should be created with a relatively long smoothing window so that it represents the long term trend of the volume data.\n",
    "- Apply some smoothing to the deviation calculated above, with a relatively short smoothing window, in order to reduce the effect of the weekly/monthly variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all volume data - for reference, not for final notebook\n",
    "\n",
    "plt.figure()\n",
    "for n in range(len(vol_ts)):\n",
    "    plt.plot(vol_ts[n], label=vol_name[n])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Volume pre-processing\n",
    "\n",
    "# find_best_vol_params(vol_ts[0], np.sum(target_ts, 0), smoothing_1=None, smoothing_2=None, plot=True)\n",
    "# find_best_vol_params(vol_ts[0], np.sum(target_ts, 0), smoothing_1=None, smoothing_2=None, plot=True)\n",
    "# find_best_vol_params(vol_ts[1], np.sum(target_ts, 0), smoothing_1=None, smoothing_2=None, plot=True)\n",
    "# find_best_vol_params(vol_ts[2], np.sum(target_ts, 0), smoothing_1=None, smoothing_2=None, plot=True)\n",
    "\n",
    "vol_pp = []\n",
    "for n in range(len(vol_ts)):\n",
    "    vol_pp.append(preprocess_vol(vol_ts[n]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparing original and pre-processed volume data\n",
    "\n",
    "v_i = 0\n",
    "t_i = 2\n",
    "plot_preprocessing(vol_ts[v_i], vol_pp[v_i], vol_name[v_i], target_ts[t_i], target_name[t_i],\n",
    "                   crop_min=2300, crop_max=2800, title=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature\n",
    "All of the temperature variables appear to be reasonably well correlated to each other, showing similar seasonal variation.\n",
    "However, 'Temperature_Lucca_Orto_Botanico' has long periods of missing data, including the most recent ~1000 days, so it will not be used for the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot all temp data - for reference, not for final notebook\n",
    "\n",
    "plt.figure(figsize=(9, 4))\n",
    "for n in range(len(temp_ts)):\n",
    "    plt.plot(temp_ts[n], label=temp_name[n])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 4))\n",
    "plt.plot(normalise_0_to_1(-temp_ts[0]), label=temp_name[0])\n",
    "plt.plot(normalise_0_to_1(target_ts[1]), label=target_name[1])\n",
    "# plt.xlim(2000, 2500)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure(figsize=(9, 4))\n",
    "plt.plot(normalise_0_to_1(-temp_ts[0]), label=temp_name[0])\n",
    "plt.plot(normalise_0_to_1(target_ts[1]), label=target_name[1])\n",
    "plt.xlim(2000, 2500)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove 'Temperature_Lucca_Orto_Botanico' from pre-processed data list\n",
    "temp_pp = temp_ts.copy()\n",
    "temp_name_pp = temp_name.copy()\n",
    "del temp_pp[2]\n",
    "del temp_name_pp[2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hydrometry\n",
    "The hydrometry data appears to be reasonably similar to the target variables, with a similar overall trend and with peaks in similar locations. Peaks in the hydrometry data tend to have a relatively steep 'ramp up', which matches the target variables well, however the 'ramp down' is  and a slightly less steep ramp down "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot all hydrometry data - for reference, not for final notebook\n",
    "\n",
    "plt.figure(figsize=(9, 4))\n",
    "for n in range(len(hydro_ts)):\n",
    "    plt.plot(hydro_ts[n], label=hydro_name[n])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 4))\n",
    "for n in range(len(target_ts)):\n",
    "    plt.plot(target_ts[n], label=target_name[n])\n",
    "plt.xlim(4100, 4300)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot showing comparison of raw hydrometry data with pre-processed (smoothed) data\n",
    "\n",
    "h_i = 0\n",
    "t_i = 1\n",
    "hydro_pp = exp_convolve(hydro_ts[h_i], tau=7, winlength=None)\n",
    "plot_preprocessing(hydro_ts[h_i], hydro_pp, hydro_name[h_i], target_ts[t_i], target_name[t_i],\n",
    "                   crop_min=1700, crop_max=2200, title=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calculating optimum tau for each hydrometry/target combination that\n",
    "# maximises the Spearman's Rank correlation coefficient\n",
    "\n",
    "hydro_tau = find_all_best_tau(hydro_ts, hydro_name, target_ts, target_name,\n",
    "                             target_0_tau_array=np.linspace(1, 2401, 80))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hydrometry data preprocessing\n",
    "\n",
    "# Remove Piaggione from data list due to missing values\n",
    "hydro_filt = hydro_ts.copy()\n",
    "hydro_name_pp = hydro_name.copy()\n",
    "if len(hydro_filt) == len(hydro_ts):\n",
    "    del hydro_filt[1]\n",
    "    del hydro_name_pp[1]\n",
    "\n",
    "# Create 3 different sets of pre-processed data\n",
    "#   - hydro_pp_shallow is a list of size (num selected hydro = 1), where each element is a time series array. This\n",
    "#     array is just for the cross-correlation anaysis, so tau value is an average value of the best tau values for\n",
    "#     the shallow groundwater variables (all but LT2).\n",
    "#   - hydro_pp_deep is a list of size (num selected hydro = 1), where each element is a time series array. This\n",
    "#     array is just for the cross-correlation anaysis, so tau value is the best tau values for the deep\n",
    "#     groundwater variable (LT2).\n",
    "#   - hydro_pp_all is a list of size (num target variables, num selected hydro), where each element is a time\n",
    "#     series array. Each array has the preprocessed data with the optimum tau value for that particular\n",
    "#     combination of hydro/target. This will be used for the model.\n",
    "\n",
    "hydro_pp_shallow = []\n",
    "hydro_pp_deep = []\n",
    "hydro_pp_all = []\n",
    "for n in range(len(hydro_filt)):\n",
    "    tau_shallow = np.mean(hydro_tau[1:, n])\n",
    "    tau_deep = np.mean(hydro_tau[0, n])\n",
    "    hydro_pp_shallow.append(exp_convolve(hydro_filt[n], tau_shallow))\n",
    "    hydro_pp_deep.append(exp_convolve(hydro_filt[n], tau_deep))\n",
    "    hydro_pp_ = []\n",
    "    for m in range(len(target_ts)):\n",
    "        tau_ = hydro_tau[m, n]\n",
    "        hydro_pp_.append(exp_convolve(hydro_filt[n], tau_))\n",
    "    hydro_pp_all.append(hydro_pp_)\n",
    "\n",
    "# Flip the dimensions of the list\n",
    "hydro_pp_all = [list(x) for x in zip(*hydro_pp_all)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of how lag applied to rainfall affects correlation with target\n",
    "\n",
    "lag_array = np.arange(200)\n",
    "plt.figure()\n",
    "for n in range(len(rain_ts)):\n",
    "    crosscorr_lag, lag_array = cross_corr_lag(\n",
    "                                    normalise_0_to_1(target_ts[0]),\n",
    "                                    normalise_0_to_1(rain_ts[n]),\n",
    "                                    lag_array=lag_array)\n",
    "    plt.plot(lag_array, crosscorr_lag, label=rain_name[n])\n",
    "plt.legend()\n",
    "plt.xlabel('Lag applied to rainfall data [days]')\n",
    "plt.ylabel('Spearman''s rank correlation coefficient')\n",
    "plt.title('Correlation of rainfall with target for different lag amounts')\n",
    "plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "target_test_ind = 0\n",
    "rain_test_ind = 0\n",
    "# lag_array = np.arange(30)\n",
    "# tau_array = np.linspace(12, 90, 40).astype(np.int)\n",
    "lag_array = np.linspace(0, 60, 31).astype(np.int)\n",
    "tau_array = np.linspace(40, 2000, 50).astype(np.int)\n",
    "data1 = target_ts[target_test_ind]\n",
    "data2 = rain_ts[rain_test_ind]\n",
    "sp_rank_cc, lag_array, tau_array = tau_and_lag_correl(data1,\n",
    "                                                      data2,\n",
    "                                                      lag_array=lag_array,\n",
    "                                                      tau_array=tau_array,\n",
    "                                                      plot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data down-selection\n",
    "Since many of the variables seem to show a high correlation with each other, using all of them to train the model is likely to add unnecessary computation time, without improvement over a model trained with a smaller selection of variables. Therefore, some down-selection of variables was carried out prior to modelling, in order to make the machine learning model as computationally efficient as possible. This was done by calculating the cross-correlation of the pre-processed data, and evaluating the expected usefulness of each variable based on the following principles:\n",
    "1. If input variables have a strong correlation to the target variable, they are likely to be useful in training the model.\n",
    "2. If input variables have a strong correlation with other input variables, there may be limited benefit in including all of them in the model.\n",
    "\n",
    "During the pre-processing of the rainfall and hydrometry data, it was found that for 'Depth_to_Groundwater_LT2', the optimum time constant (tau) for the convolution was much higher than the optimum value for the other groundwater variables. This was thought to be because the LT2 sub-system is significantly deeper underground than the other variables, and therefore it takes longer for the other variables to impact the level. In order to best evaluate the correlation of the variables, two different correlation matrices were calculated: one with variable pre-processing optimised for the shallower groundwater variables; and one with variable pre-processing optimised for the deepest groundwater variable LT2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot correlation matrices\n",
    "\n",
    "# Combine all pre-processed data for the shallow groundwater variables\n",
    "data_pp_shallow = np.concatenate((np.asarray(target_ts),\n",
    "                                  np.asarray(rain_pp_shallow),\n",
    "                                  np.asarray(vol_pp),\n",
    "                                  np.asarray(temp_pp),\n",
    "                                  np.asarray(hydro_pp_shallow)))\n",
    "name_pp_shallow = target_name + rain_name_pp + vol_name + temp_name_pp + hydro_name_pp\n",
    "\n",
    "# Combine all pre-processed data for the deep groundwater variables\n",
    "data_pp_deep = np.concatenate((np.asarray(target_ts),\n",
    "                               np.asarray(rain_pp_deep),\n",
    "                               np.asarray(vol_pp),\n",
    "                               np.asarray(temp_pp),\n",
    "                               np.asarray(hydro_pp_deep)))\n",
    "name_pp_deep = target_name + rain_name_pp + vol_name + temp_name_pp + hydro_name_pp\n",
    "\n",
    "# Plot correlation matrices\n",
    "plot_correl_matrix(data_pp_deep, name_pp_deep,\n",
    "                   title='Correlation matrix, data pre-processed for deep LT2 groundwater variable')\n",
    "plot_correl_matrix(data_pp_shallow, name_pp_shallow,\n",
    "                   title='Correlation matrix, data pre-processed for shallow groundwater variables')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Prediction \n",
    "\n",
    "We use a machine learning model to predict the depth to ground water given past timeseries information. We explore the following questions:\n",
    "- LSTM vs. MLP semplice  meglio\n",
    "\n",
    "- Including versus excluding the target timeseries as an input to the predictive model\n",
    "- Importance of the various features in the predictive skill\n",
    "- Daily versus weekly predictions\n",
    "- Predicting several timesteps (days or weeks) in advance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Including versus excluding the target timeseries as a model input\n",
    "\n",
    "The plot below shows the or\n",
    "\n",
    "Not using other targets\n",
    "\n",
    "Weights plots:\n",
    "- target has biggest weighting\n",
    "- rainfall and hydrometry have biggest weights with target excliuded\n",
    "- Most recent days significantly larger weights\n",
    "\n",
    "2 example plots w and wthout\n",
    "\n",
    "(table summary for all targets: target, target incl. test RMSE, test MSE, target incl. test RMSE, test MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# LSTM with or without target data\n",
    "\n",
    "\n",
    "# Including the target data \n",
    "\n",
    "# Choose variables to include in model by their indices\n",
    "target_ind = 1\n",
    "model_rain_ind = [1, 3, 4, 7, 8]\n",
    "model_vol_ind = [0, 1, 2]\n",
    "model_temp_ind = [0, 1, 2]\n",
    "model_hydro_ind = [0]\n",
    "\n",
    "# Getting the names of the selected features\n",
    "a1 = [rain_name[i] for i in model_rain_ind]\n",
    "a2 = [vol_name[i] for i in model_vol_ind]\n",
    "a3 = [temp_name[i] for i in model_temp_ind]\n",
    "a4 = [hydro_name[i] for i in model_hydro_ind]\n",
    "input_names = [target_name[target_ind]] + a1 + a2 + a3 + a4\n",
    "\n",
    "# Compile target data and inputs to arrays\n",
    "target_data = target_ts[target_ind]\n",
    "model_input_data = compile_model_input_variables(model_rain_ind,\n",
    "                                                 model_vol_ind,\n",
    "                                                 model_temp_ind,\n",
    "                                                 model_hydro_ind,\n",
    "                                                 rain_pp_all[target_ind],\n",
    "                                                 vol_pp,\n",
    "                                                 temp_pp,\n",
    "                                                 hydro_pp_all[target_ind],\n",
    "                                                 target_ts[target_ind].size)\n",
    "\n",
    "# Create ML model class and run it\n",
    "Model_using_target = ML_model(target_data=target_data,\n",
    "                                input_data=model_input_data,\n",
    "                                use_target=True,\n",
    "                                look_back=30,\n",
    "                                chunk_step=3,\n",
    "                                train_ratio=0.67,\n",
    "                                num_epochs=50,\n",
    "                                batch_size=5,\n",
    "                                predict_steps=5,\n",
    "                                plot=False,\n",
    "                                verbose=0)\n",
    "Model_using_target.run(model_type='MLP')\n",
    "Model_using_target.plot_weights(input_names)\n",
    "Model_using_target.plot_predict_one_step(title=\"Including the target data (MLP)\")\n",
    "\n",
    "# # Excluding the target data\n",
    "# Model_excluding_target = ML_model(target_data=target_data,\n",
    "#                                 input_data=model_input_data,\n",
    "#                                 use_target=False,\n",
    "#                                 look_back=30,\n",
    "#                                 chunk_step=3,\n",
    "#                                 train_ratio=0.67,\n",
    "#                                 num_epochs=50,\n",
    "#                                 batch_size=5,\n",
    "#                                 predict_steps=5,\n",
    "#                                 plot=False,\n",
    "#                                 verbose=0)\n",
    "# Model_excluding_target.run(model_type='LSTM')\n",
    "# Model_excluding_target.plot_predict_one_step(title=\"Excluding the target data (LSTM)\")\n",
    "\n",
    "# Excluding the target data\n",
    "Model_excluding_target = ML_model(target_data=target_data,\n",
    "                                input_data=model_input_data,\n",
    "                                use_target=False,\n",
    "                                look_back=30,\n",
    "                                chunk_step=3,\n",
    "                                train_ratio=0.67,\n",
    "                                num_epochs=70,\n",
    "                                batch_size=5,\n",
    "                                predict_steps=5,\n",
    "                                plot=False,\n",
    "                                verbose=0)\n",
    "Model_excluding_target.run(model_type='MLP')\n",
    "Model_excluding_target.plot_weights(input_names[1:])\n",
    "# layer_1_mean = np.mean(Model_excluding_target.weights[0], axis=1)\n",
    "# layer_1_mean = layer_1_mean.reshape((Model_excluding_target.look_back, model_input_data.shape[0]))\n",
    "# plt.figure()\n",
    "# plt.imshow(layer_1_mean.T)\n",
    "# plt.colorbar()\n",
    "# plt.show()\n",
    "# Model_excluding_target.plot_predict_one_step(title=\"Excluding the target data (MLP)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing variable importance\n",
    "In order to quantify how important different variables are in terms of model accuracy, the following method was used:\n",
    "1. A multi-layer perceptron model was trained using only one input variable to predict the target one day ahead.\n",
    "2. The RMSE of the predictions made using this model were calculated.\n",
    "3. The above steps were repeated for each of the variables in the dataset in turn and the RMSE scores were compared to assess which variables were most effective at modelling the target.\n",
    "\n",
    "Bar plot (add target only)\n",
    "\n",
    "Refer back to correlation matrix and weights matrices - good match\n",
    "\n",
    "Results suggest that smaller number of features may be similarly effective than using all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose variables to include in model by their indices\n",
    "target_ind = 0\n",
    "model_rain_ind = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "model_vol_ind = [0, 1, 2]\n",
    "model_temp_ind = [0, 1, 2]\n",
    "model_hydro_ind = [0]\n",
    "\n",
    "# Getting the names of the selected features\n",
    "a1 = [rain_name[i] for i in model_rain_ind]\n",
    "a2 = [vol_name[i] for i in model_vol_ind]\n",
    "a3 = [temp_name[i] for i in model_temp_ind]\n",
    "a4 = [hydro_name[i] for i in model_hydro_ind]\n",
    "input_names = [target_name[target_ind]] + a1 + a2 + a3 + a4\n",
    "\n",
    "# Compile target data and inputs to arrays\n",
    "target_data = target_ts[target_ind]\n",
    "model_input_data_ref = compile_model_input_variables(model_rain_ind,\n",
    "                                                 model_vol_ind,\n",
    "                                                 model_temp_ind,\n",
    "                                                 model_hydro_ind,\n",
    "                                                 rain_pp_all[target_ind],\n",
    "                                                 vol_pp,\n",
    "                                                 temp_pp,\n",
    "                                                 hydro_pp_all[target_ind],\n",
    "                                                 target_ts[target_ind].size)\n",
    "\n",
    "\n",
    "# Running LSTM a total of tot_idx times, each time removing one of the input features\n",
    "tot_idx = 1 + model_input_data_ref.shape[0]\n",
    "all_scores = np.empty(tot_idx)\n",
    "model_input_data = model_input_data_ref\n",
    "\n",
    "for i in range(tot_idx):\n",
    "    model_input_data = np.expand_dims(model_input_data_ref[i-1, :], axis=0)\n",
    "            \n",
    "    # Making the model instance\n",
    "    model_tmp = ML_model(target_data=target_data,\n",
    "                                    input_data=model_input_data,\n",
    "                                    use_target=False,\n",
    "                                    look_back=30,\n",
    "                                    chunk_step=20,\n",
    "                                    train_ratio=0.67,\n",
    "                                    num_epochs=30,\n",
    "                                    batch_size=5,\n",
    "                                    predict_steps=1,\n",
    "                                    plot=False,\n",
    "                                    verbose=0)\n",
    "    model_tmp.run(model_type='MLP')\n",
    "    \n",
    "    all_scores[i] = (model_tmp.trainRMSE + model_tmp.testRMSE)/2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting ranking \n",
    "ranking = np.argsort(1*all_scores) # Sorting in ascending order\n",
    "sorted_scores = all_scores[ranking]\n",
    "sorted_names = [input_names[i] for i in ranking]\n",
    "\n",
    "# Plotting bar chart\n",
    "plt.figure(figsize=(8, 4))\n",
    "ax = plt.subplot(111)\n",
    "x_pos = np.arange(0,len(sorted_names))\n",
    "plt.bar(x_pos, sorted_scores, color=[0.3, 0.5, 0.6])\n",
    "plt.ylabel(\"RMSE score\")\n",
    "plt.title(\"Variable importance for predicting {}\".format(input_names[0]))\n",
    "ax.text(0.01, 0.98, 'Note: lower scores suggest higher importance',\n",
    "        transform=ax.transAxes, va='top', ha='left', color=[0.3, 0.3, 0.3])\n",
    "plt.xticks(x_pos, sorted_names, rotation=40, ha=\"right\",\n",
    "                 rotation_mode=\"anchor\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose variables to include in model by their indices\n",
    "target_ind = 1\n",
    "model_rain_ind = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "model_vol_ind = [0, 1, 2]\n",
    "model_temp_ind = [0, 1, 2]\n",
    "model_hydro_ind = [0]\n",
    "\n",
    "# Getting the names of the selected features\n",
    "a1 = [rain_name[i] for i in model_rain_ind]\n",
    "a2 = [vol_name[i] for i in model_vol_ind]\n",
    "a3 = [temp_name[i] for i in model_temp_ind]\n",
    "a4 = [hydro_name[i] for i in model_hydro_ind]\n",
    "input_names = [target_name[target_ind]] + a1 + a2 + a3 + a4\n",
    "\n",
    "# Compile target data and inputs to arrays\n",
    "target_data = target_ts[target_ind]\n",
    "model_input_data_ref = compile_model_input_variables(model_rain_ind,\n",
    "                                                 model_vol_ind,\n",
    "                                                 model_temp_ind,\n",
    "                                                 model_hydro_ind,\n",
    "                                                 rain_pp_all[target_ind],\n",
    "                                                 vol_pp,\n",
    "                                                 temp_pp,\n",
    "                                                 hydro_pp_all[target_ind],\n",
    "                                                 target_ts[target_ind].size)\n",
    "\n",
    "\n",
    "# Running LSTM a total of tot_idx times, each time removing one of the input features\n",
    "tot_idx = 1 + model_input_data_ref.shape[0]\n",
    "all_scores = np.empty(tot_idx)\n",
    "model_input_data = model_input_data_ref\n",
    "\n",
    "for i in range(tot_idx):\n",
    "    model_input_data = np.expand_dims(model_input_data_ref[i-1, :], axis=0)\n",
    "            \n",
    "    # Making the model instance\n",
    "    model_tmp = ML_model(target_data=target_data,\n",
    "                                    input_data=model_input_data,\n",
    "                                    use_target=False,\n",
    "                                    look_back=30,\n",
    "                                    chunk_step=20,\n",
    "                                    train_ratio=0.67,\n",
    "                                    num_epochs=30,\n",
    "                                    batch_size=5,\n",
    "                                    predict_steps=1,\n",
    "                                    plot=False,\n",
    "                                    verbose=0)\n",
    "    model_tmp.run(model_type='MLP')\n",
    "    \n",
    "    all_scores[i] = (model_tmp.trainRMSE + model_tmp.testRMSE)/2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting ranking \n",
    "ranking = np.argsort(1*all_scores) # Sorting in ascending order\n",
    "sorted_scores = all_scores[ranking]\n",
    "sorted_names = [input_names[i] for i in ranking]\n",
    "\n",
    "# Plotting bar chart\n",
    "plt.figure(figsize=(8, 4))\n",
    "ax = plt.subplot(111)\n",
    "x_pos = np.arange(0,len(sorted_names))\n",
    "plt.bar(x_pos, sorted_scores, color=[0.3, 0.5, 0.6])\n",
    "plt.ylabel(\"RMSE score\")\n",
    "plt.title(\"Variable importance for predicting {}\".format(input_names[0]))\n",
    "ax.text(0.01, 0.98, 'Note: lower scores suggest higher importance',\n",
    "        transform=ax.transAxes, va='top', ha='left', color=[0.3, 0.3, 0.3])\n",
    "plt.xticks(x_pos, sorted_names, rotation=40, ha=\"right\",\n",
    "                 rotation_mode=\"anchor\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictability horizon\n",
    "\n",
    "Here we evaluate the performance of the model as a function of the number of days in advance it predicts. As expected, the performance tails off with the number of days, but remains above 90% for over 15 days. \n",
    "\n",
    "One example plot of raw predictions\n",
    "\n",
    "Horizon with one line per target (at least one LT2 and one other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_predict_horizon(target_data, testPredict, testYind, predict_steps):\n",
    "    testTarget = np.empty(np.shape(testPredict))\n",
    "    for ii in range(len(testYind)):\n",
    "        testTarget[ii, :] = target_data[testYind[ii]: testYind[ii] + predict_steps]\n",
    "    predict_horizon = np.empty((predict_steps))\n",
    "    for ii in range(predict_steps):\n",
    "#         predict_horizon[ii], _ = stats.spearmanr(testTarget[:,ii], testPredict[:,ii])\n",
    "        predict_horizon[ii] = math.sqrt(mean_squared_error(testTarget[:,ii], testPredict[:,ii]))\n",
    "    return predict_horizon\n",
    "\n",
    "# Choose variables to include in model by their indices\n",
    "target_ind = 1\n",
    "model_rain_ind = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "model_vol_ind = [0, 1, 2]\n",
    "model_temp_ind = [0, 1, 2]\n",
    "model_hydro_ind = [0]\n",
    "\n",
    "# Getting the names of the selected features\n",
    "a1 = [rain_name[i] for i in model_rain_ind]\n",
    "a2 = [vol_name[i] for i in model_vol_ind]\n",
    "a3 = [temp_name[i] for i in model_temp_ind]\n",
    "a4 = [hydro_name[i] for i in model_hydro_ind]\n",
    "input_names = a1 + a2 + a3 + a4\n",
    "\n",
    "# Compile target data and inputs to arrays\n",
    "target_data = target_ts[target_ind]\n",
    "model_input_data = compile_model_input_variables(model_rain_ind,\n",
    "                                                 model_vol_ind,\n",
    "                                                 model_temp_ind,\n",
    "                                                 model_hydro_ind,\n",
    "                                                 rain_pp_all[target_ind],\n",
    "                                                 vol_pp,\n",
    "                                                 temp_pp,\n",
    "                                                 hydro_pp_all[target_ind],\n",
    "                                                 target_ts[target_ind].size)\n",
    "# # Model parameters\n",
    "# look_back = 80\n",
    "# chunk_step = 10\n",
    "# train_ratio = 0.67\n",
    "# num_epochs = 200\n",
    "# batch_size = 5\n",
    "\n",
    "\n",
    "predict_steps = 15\n",
    "\n",
    "# Run ML model\n",
    "model_day_horizon = ML_model(target_data=target_data,\n",
    "                                input_data=model_input_data,\n",
    "                                use_target=True,\n",
    "                                look_back=60,\n",
    "                                chunk_step=10,\n",
    "                                train_ratio=0.67,\n",
    "                                num_epochs=100,\n",
    "                                batch_size=5,\n",
    "                                predict_steps=predict_steps,\n",
    "                                plot=False,\n",
    "                                verbose=0)\n",
    "model_day_horizon.run(model_type='MLP')\n",
    "model_day_horizon.plot_predict_all_steps(crop_min=2000, crop_max=2500)\n",
    "\n",
    "# _, trainRMSE_advance, testRMSE_advance, testYind_advance, testPredict_advance, \\\n",
    "# trainYind_advance, trainPredict_advance = run_model(target_data,\n",
    "#                                                  model_input_data_ref,\n",
    "#                                                  model_type='MLP',\n",
    "#                                                  use_target=True,\n",
    "#                                                  look_back=look_back,\n",
    "#                                                  chunk_step=chunk_step,\n",
    "#                                                  train_ratio=train_ratio,\n",
    "#                                                  num_epochs=num_epochs,\n",
    "#                                                  batch_size=batch_size,\n",
    "#                                                  predict_steps=predict_steps,\n",
    "#                                                  plot=False,\n",
    "#                                                  verbose=0,\n",
    "#                                                    MLP_layers=[500, 1000, 300])\n",
    "\n",
    "# # Plot all predictions from test samples against original data\n",
    "# plt.figure(figsize=(9, 4))\n",
    "# plt.plot(np.linspace(0, target_data.size-1, target_data.size),\n",
    "#          target_data,\n",
    "#          label='Original data')\n",
    "# for n in range(len(testYind_advance)):\n",
    "#     if n == 0:\n",
    "#         plt.plot(np.arange(testYind_advance[n], testYind_advance[n] + predict_steps),\n",
    "#                  testPredict_advance[n, :], '-r', label='Test predictions')\n",
    "#     else:\n",
    "#         plt.plot(np.arange(testYind_advance[n], testYind_advance[n] + predict_steps),\n",
    "#                  testPredict_advance[n, :], '-r')\n",
    "# for n in range(len(trainYind_advance)):\n",
    "#     if n == 0:\n",
    "#         plt.plot(np.arange(trainYind_advance[n], trainYind_advance[n] + predict_steps),\n",
    "#                  trainPredict_advance[n, :], '-g', label='Train predictions')\n",
    "#     else:\n",
    "#         plt.plot(np.arange(trainYind_advance[n], trainYind_advance[n] + predict_steps),\n",
    "#                  trainPredict_advance[n, :], '-g')\n",
    "# plt.legend()\n",
    "# plt.title(\"Predictions compared to original dataset (with look-ahead)\")\n",
    "# plt.xlim(2000, 2500)\n",
    "# plt.show()\n",
    "\n",
    "# Calculate and plot RMSE against prediction look ahead\n",
    "# predict_horizon = get_predict_horizon(target_data, testPredict_advance, testYind_advance, predict_steps)\n",
    "predict_horizon = model_day_horizon.testRMSE\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, predict_steps), predict_horizon)\n",
    "plt.title(\"Predictability horizon\")\n",
    "plt.xlabel(\"Steps in advance [days]\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekly analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_data_mean(field, average_fac):\n",
    "    \"\"\"\n",
    "    Calculate mean of data array along last axis. field is a vector,\n",
    "    which can be more than 1D. The last dimension of field will be averaged,\n",
    "    so could be shape: (num_timesteps)\n",
    "    or shape: (num features, num_timesteps)\n",
    "    The output will be an array with the same number of dimensions as\n",
    "    field, but with the size of the last axis reduced depending on the\n",
    "    averaging amount 'average_fac'.\n",
    "    \"\"\"\n",
    "    num_dim = field.ndim\n",
    "    num_timesteps = field.shape[-1]\n",
    "    num_av = np.int(np.floor(num_timesteps / average_fac))\n",
    "    \n",
    "    # Get shape for temporary reshaped array\n",
    "    if num_dim > 1:\n",
    "        feature_shape = list(field.shape[:-1])\n",
    "        tmp_shape = feature_shape + [num_av, average_fac]\n",
    "    else:\n",
    "        tmp_shape = [num_av, average_fac]\n",
    "        \n",
    "    # Remove earliest timesteps if num_timesteps is not divisible by average_fac\n",
    "    field_crop = np.delete(field, np.s_[: -num_av*average_fac], axis=-1)\n",
    "    \n",
    "    # Take mean of data\n",
    "    tmp = np.reshape(field_crop, tmp_shape)\n",
    "    data_mean = np.mean(tmp, axis=-1)\n",
    "    \n",
    "    return data_mean\n",
    "\n",
    "# Choose variables to include in model by their indices\n",
    "target_ind = 1\n",
    "model_rain_ind = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "model_vol_ind = [0, 1, 2]\n",
    "model_temp_ind = [0, 1, 2]\n",
    "model_hydro_ind = [0]\n",
    "\n",
    "# Getting the names of the selected features\n",
    "a1 = [rain_name[i] for i in model_rain_ind]\n",
    "a2 = [vol_name[i] for i in model_vol_ind]\n",
    "a3 = [temp_name[i] for i in model_temp_ind]\n",
    "a4 = [hydro_name[i] for i in model_hydro_ind]\n",
    "input_names = a1 + a2 + a3 + a4\n",
    "\n",
    "# Compile target data and inputs to arrays\n",
    "target_data = target_ts[target_ind]\n",
    "model_input_data = compile_model_input_variables(model_rain_ind,\n",
    "                                                 model_vol_ind,\n",
    "                                                 model_temp_ind,\n",
    "                                                 model_hydro_ind,\n",
    "                                                 rain_pp_all[target_ind],\n",
    "                                                 vol_pp,\n",
    "                                                 temp_pp,\n",
    "                                                 hydro_pp_all[target_ind],\n",
    "                                                 target_ts[target_ind].size)\n",
    "\n",
    "# Calculate 1 week average\n",
    "average_fac = 7\n",
    "target_data_weekly = get_data_mean(target_data, average_fac)\n",
    "input_data_weekly = get_data_mean(model_input_data, average_fac)\n",
    "\n",
    "# Run ML model\n",
    "predict_steps = 15\n",
    "model_weekly = ML_model(target_data=target_data,\n",
    "                                input_data=model_input_data,\n",
    "                                use_target=True,\n",
    "                                look_back=60,\n",
    "                                chunk_step=10,\n",
    "                                train_ratio=0.67,\n",
    "                                num_epochs=100,\n",
    "                                batch_size=5,\n",
    "                                predict_steps=predict_steps,\n",
    "                                plot=False,\n",
    "                                verbose=0)\n",
    "model_weekly.run(model_type='MLP')\n",
    "model_weekly.plot_predict_all_steps(crop_min=2000, crop_max=2500)\n",
    "\n",
    "predict_horizon = model_weekly.testRMSE\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, predict_steps), predict_horizon)\n",
    "plt.title(\"Predictability horizon\")\n",
    "plt.xlabel(\"Steps in advance [weeks]\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot all weekly predictions from test samples against original data\n",
    "# plt.figure(figsize=(9, 4))\n",
    "# plt.plot(np.linspace(0, target_data_weekly.size-1, target_data_weekly.size),\n",
    "#          target_data_weekly,\n",
    "#          label='Original data')\n",
    "# plt.plot(testYind_weekly, testPredict_weekly[:, 0], 'xr', label='Test predictions')\n",
    "# plt.plot(trainYind_weekly, trainPredict_weekly[:, 0], 'xg', label='Train predictions')\n",
    "# plt.legend()\n",
    "# plt.title(\"Weekly predictions compared to original dataset\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # MG: something weird happening here..Probably taking the wrong inputs\n",
    "\n",
    "\n",
    "# # Plot all predictions from test samples against original data\n",
    "# plt.figure(figsize=(9, 4))\n",
    "# plt.plot(np.linspace(0, target_data_weekly.size-1, target_data_weekly.size),\n",
    "#          target_data_weekly,\n",
    "#          label='Original data')\n",
    "# for n in range(len(testYind_weekly)):\n",
    "#     if n == 0:\n",
    "#         plt.plot(np.arange(testYind_weekly[n], testYind_weekly[n] + predict_steps),\n",
    "#                  testPredict_weekly[n, :], '-r', label='Test predictions')\n",
    "#     else:\n",
    "#         plt.plot(np.arange(testYind_weekly[n], testYind_weekly[n] + predict_steps),\n",
    "#                  testPredict_weekly[n, :], '-r')\n",
    "# for n in range(len(trainYind_weekly)):\n",
    "#     if n == 0:\n",
    "#         plt.plot(np.arange(trainYind_weekly[n], trainYind_weekly[n] + predict_steps),\n",
    "#                  trainPredict_weekly[n, :], '-g', label='Train predictions')\n",
    "#     else:\n",
    "#         plt.plot(np.arange(trainYind_weekly[n], trainYind_weekly[n] + predict_steps),\n",
    "#                  trainPredict_weekly[n, :], '-g')\n",
    "# plt.legend()\n",
    "# plt.title(\"Predictions compared to original dataset (with look-ahead)\")\n",
    "# plt.xlim(100, 300)\n",
    "# plt.show()\n",
    "\n",
    "# predict_horizon_weekly = get_predict_horizon(target_data_weekly, testPredict_weekly, testYind_weekly, predict_steps)\n",
    "\n",
    "# plt.figure\n",
    "# plt.plot(np.arange(0,predict_steps), predict_horizon_weekly)\n",
    "# plt.title(\"Weekly predictability horizon\")\n",
    "# plt.ylabel(\"RMSE\")\n",
    "# plt.xlabel(\"Steps in advance [weeks]\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary & conclusions\n",
    "- It was found that when the rainfall and hydrometry data was filtered with an exponential window, the correlation to the target variables increased significantly. In general, the deeper the aquifier subsystems, the greater the optimum time constants for the filter. This makes logical sense based on an understanding of the aquifiers, since it takes more time for the rainfall to reach deeper groundwater.\n",
    "- LSTM vs MLP.\n",
    "- With the past target data included as an input, the model appeared to work well in predictions, with the most recent target data having the biggest weightings in the model.\n",
    "- When the target was excluded as an input, the prediction error increased, although the remaining features still provided a relatively good level of accuracy\n",
    "- The rainfall and hydrometry variables (filtered with an exponential window) seem to be the most useful in predicting the target variables, aside from the target itself. It appears that the rainfall affects the groundwater quite directly (in particular, the shallower groundwater variables), and likely to be the main physical mechanism causing the spikes in the groundwater level. If this is the case, then the accuracy of the model may be limited by the unpredictability of the rainfall in the coming days. Whether or not it rains the next day is likely to have a significant effect on the groundwater levels that day (in the shallower sub-systems), which is fundamentally difficult to predict. It is possible that the inclusion of weather forecast data in the model, such as predicted rainfall, might improve accuracy.\n",
    "- Temperature appears to be able to make reasonable predictions of target on it's own, although not as effective as rainfall or hydrometry variables. This may be because of a link between temperature and rain, particularly over longer timescales, due to local climate fluctuations that affect both variables.\n",
    "- There may be some improvement in accuracy if all target variables are included in the model, but the focus of this study was to investigate the effect of external variables rather than to optimise the model performance.\n",
    "- It may be possible to learn more about the effect of external variables by filtering the rainfall variables with different time constants and assessing the impact on model performance. By using time constants of the order of tens of days, daily fluctuations in rainfall are smoothed out, hence the model may have difficulty resolving all of the daily fluctuations in the target (if those are indeed linked to daily rainfall). It would be interesting to know where the short term fluctuations come from, and if they are indeed predictable.\n",
    "\n",
    "- Predictability horizon and weekly vs daily??????????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Water Spring Amiata\n",
    "## Data pre-processing\n",
    "### Handling missing data\n",
    "First, the data was read in and an analysis of missing data was carried out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Pre-processing data\n",
    "\n",
    "# Importing all data into pandas dataframe\n",
    "df = pd.read_csv(files[6])\n",
    "\n",
    "# Find and plot gaps in the data\n",
    "all_data, col_names, is_nan, is_zero = find_data_gaps(df, plot=True, title=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows that there is a significant amount of data missing over the total time period, either in the form of NaNs, or zeros. In the case of the rainfall data, the zeros are considered to be 'real data' since there are many days with zero rainfall, however for the other data it is assumed that zeros imply missing data.\n",
    "\n",
    "Since there are many NaNs in most of the features for the first ~5500 days, including all of the target variables, it was decided that this time period would not be used for the model at all.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find column indices for different datatypes\n",
    "datatypes, col_inds = find_datatypes(df)\n",
    "\n",
    "# Get time series data for all variable types\n",
    "start_ind = 5760\n",
    "rain_ts, rain_name = get_time_series(col_inds[0], start_ind=start_ind, fill_zeros=False)\n",
    "depth_ts, depth_name = get_time_series(col_inds[1], start_ind=start_ind)\n",
    "temp_ts, temp_name = get_time_series(col_inds[2], start_ind=start_ind)\n",
    "target_ts, target_name = get_time_series(col_inds[5], start_ind=start_ind) \n",
    "\n",
    "plt.figure(figsize=(9, 4))\n",
    "for n in range(len(rain_ts)):\n",
    "    plt.plot(rain_ts[n], label=rain_name[n])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 4))\n",
    "for n in range(len(target_ts)):\n",
    "    plt.plot(target_ts[n], label=target_name[n])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flow Rate\n",
    "There are some large step changes in some of the Flow Rate variables, where the rate becomes zero (or close to zero) for several days in a row. Without understanding more details about the data collection, it is difficult to know whether these deviations represent reality or whether they are missed/incorrect recordings. However, it is likely that including them in the modelling will make it significantly more challenging to make accurate predictions, since they appear to be anomalous and unrelated to other features in this dataset.\n",
    "It was decided that these values should be filled by interpolation, to allow the model to better represent the majority of the data.\n",
    "\n",
    "The potentially anomalous data was identified by the following method:\n",
    "- Calculate the moving average of the time series with a large time window\n",
    "- Calculate the standard deviation of the time series\n",
    "- Set upper and lower thresholds at 'moving average +/- 1.6*std'\n",
    "- Any values outside of these thresholds were deemed to be potentially anomalous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Flow rate target variables\"\"\"\n",
    "\n",
    "def interp_nan(arr):\n",
    "    ser = pd.Series(arr)\n",
    "    ser.interpolate(method='linear', inplace=True)\n",
    "    out = ser.to_numpy()\n",
    "    return out\n",
    "\n",
    "# Find potentially anomalous samples by deviation from moving average\n",
    "flow_ts_filt = []\n",
    "flow_name = []\n",
    "flow_mov_av = []\n",
    "flow_thresh_hi = []\n",
    "flow_thresh_lo = []\n",
    "moving_av_win = 450\n",
    "for n in range(len(target_ts)):\n",
    "    removed_zeros = np.where(target_ts[n]==0, np.nan, target_ts[n])\n",
    "    overall_av = np.nanmean(removed_zeros)\n",
    "    filled_zeros = np.where(np.isnan(removed_zeros), overall_av, removed_zeros)\n",
    "    mov_av = movingav(filled_zeros, moving_av_win, winfunc=None)\n",
    "    thresh_hi = mov_av + 1.6*np.nanstd(removed_zeros)\n",
    "    thresh_lo = mov_av - 1.6*np.nanstd(removed_zeros)\n",
    "    flow_thresh_hi.append(thresh_hi)\n",
    "    flow_thresh_lo.append(thresh_lo)\n",
    "    is_outlier = np.logical_and(target_ts[n] < thresh_hi, target_ts[n] > thresh_lo)\n",
    "    ts_filt = np.where(is_outlier, target_ts[n], np.nan)\n",
    "    flow_mov_av.append(mov_av)\n",
    "    flow_ts_filt.append(ts_filt)\n",
    "\n",
    "# Prob don't show this plot, at least not how it is atm - bit of a mess!\n",
    "# smoothing_win = 1\n",
    "# plt.figure()\n",
    "# for n in range(len(target_ts)):\n",
    "#     smoothed = moving_average(target_ts[n], smoothing_win)\n",
    "#     plt.plot(flow_mov_av[n], lw=1, color=[0, 0, 0, 0.5])\n",
    "#     plt.plot(flow_thresh_hi[n], lw=1, color=[0, 0, 0, 0.5])\n",
    "#     plt.plot(flow_thresh_lo[n], lw=1, color=[0, 0, 0, 0.5])\n",
    "#     plt.plot(smoothed, lw=1, color=[0, 0, 0, 0.7])\n",
    "#     plt.plot(flow_ts_filt[n], label=target_name[n], alpha=0.9)\n",
    "# plt.legend()\n",
    "# plt.title('Flow rates')\n",
    "# plt.show()\n",
    "\n",
    "target_ts_raw = np.copy(target_ts)\n",
    "for n in range(len(target_ts)):\n",
    "    target_ts[n] = interp_nan(flow_ts_filt[n])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# Plot example of filtering out potential anomalies\n",
    "plt.figure(figsize=(9, 4))\n",
    "plt.plot(flow_mov_av[1], lw=1, ls='dashed', color=[0, 0, 0, 0.5], label='Moving average')\n",
    "plt.plot(flow_thresh_hi[1], lw=1, color=[0, 0, 0, 0.5], label='Thresholds')\n",
    "plt.plot(flow_thresh_lo[1], lw=1, color=[0, 0, 0, 0.5])\n",
    "plt.plot(target_ts_raw[1], lw=1, color=[0.7, 0.2, 0.2, 0.7], label='Filtered out samples')\n",
    "plt.plot(target_ts[1], color=[0.2, 0.7, 0.2, 0.7], label='Interpolated', alpha=0.9)\n",
    "plt.plot(flow_ts_filt[1], label=target_name[n], alpha=0.9)\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Flow rate pre-filtering')\n",
    "plt.show()\n",
    "\n",
    "    \n",
    "# Prob don't need this plot, but kept atm just for reference\n",
    "plt.figure(figsize=(9, 4))\n",
    "for n in range(len(target_ts)):\n",
    "    plt.plot(target_ts[n], label=target_name[n])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot example of correlation for different tau values\n",
    "\n",
    "tau_best, src_best = find_best_tau(target_ts=target_ts[0], rain_ts=rain_ts,\n",
    "                         rain_name=rain_name, plot=True,\n",
    "                         target_name=target_name[1],\n",
    "                         tau_array=np.linspace(1, 80, 80))\n",
    "\n",
    "tau_best, src_best = find_best_tau(target_ts=target_ts[1], rain_ts=rain_ts, plot=True,\n",
    "                         target_name=target_name[0],\n",
    "                         rain_name=rain_name,\n",
    "                         tau_array=np.linspace(1, 80, 80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Combine all pre-processed data\n",
    "all_data = np.concatenate((np.asarray(target_ts),\n",
    "                               np.asarray(rain_ts),\n",
    "                               np.asarray(depth_ts),\n",
    "                               np.asarray(temp_ts)))\n",
    "all_name = target_name + rain_name + depth_name + temp_name\n",
    "\n",
    "# Plot correlation matrices\n",
    "plot_correl_matrix(all_data, all_name,\n",
    "                   title='Correlation matrix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Better to genralise the earlier function probably\n",
    "\n",
    "def compile_model_input_variables_spring(model_rain_ind,\n",
    "                                  model_depth_ind,\n",
    "                                  model_temp_ind,\n",
    "                                  rain_pp,\n",
    "                                  depth_pp,\n",
    "                                  temp_pp,\n",
    "                                  num_timesteps):\n",
    "    \"\"\"\n",
    "    Extract the selected variables from the preprocessed data by their indices\n",
    "    and populate an input array to be used by the model.\n",
    "    The input array will have shape: (num features, num timesteps)\n",
    "    \n",
    "    \"\"\"    \n",
    "    num_inputs = len(model_rain_ind) + len(model_depth_ind) + \\\n",
    "                 len(model_temp_ind)\n",
    "    model_input_data = np.empty((num_inputs, num_timesteps))\n",
    "    count = 0\n",
    "    for i in model_rain_ind:\n",
    "        model_input_data[count, :] = rain_pp[i]\n",
    "        count += 1\n",
    "    for i in model_depth_ind:\n",
    "        model_input_data[count, :] = depth_pp[i]\n",
    "        count += 1\n",
    "    for i in model_temp_ind:\n",
    "        model_input_data[count, :] = temp_pp[i]\n",
    "        count += 1\n",
    "    \n",
    "    return model_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# LSTM with or without target data\n",
    "\n",
    "\n",
    "# Including the target data \n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "# Choose variables to include in model by their indices\n",
    "target_ind = 1\n",
    "model_rain_ind = []\n",
    "model_depth_ind = [0, 1, 2]\n",
    "model_temp_ind = []\n",
    "\n",
    "# Compile target data and inputs to arrays\n",
    "target_data = target_ts[target_ind]\n",
    "model_input_data = compile_model_input_variables_spring(model_rain_ind,\n",
    "                                                 model_depth_ind,\n",
    "                                                 model_temp_ind,\n",
    "                                                 rain_ts,\n",
    "                                                 depth_ts,\n",
    "                                                 temp_ts,\n",
    "                                                 target_ts[target_ind].size)\n",
    "\n",
    "# Create ML model class and run it\n",
    "Model_using_target = ML_model(target_data=target_data,\n",
    "                                input_data=model_input_data,\n",
    "                                use_target=True,\n",
    "                                look_back=30,\n",
    "                                chunk_step=3,\n",
    "                                train_ratio=0.67,\n",
    "                                num_epochs=50,\n",
    "                                batch_size=5,\n",
    "                                predict_steps=5,\n",
    "                                plot=False,\n",
    "                                verbose=0)\n",
    "Model_using_target.run(model_type='MLP')\n",
    "layer_1_mean = np.mean(Model_using_target.weights[0], axis=1)\n",
    "layer_1_mean = layer_1_mean.reshape((Model_using_target.look_back, model_input_data.shape[0] + 1))\n",
    "plt.figure()\n",
    "plt.imshow(layer_1_mean.T)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "Model_using_target.plot_predict_one_step(title=\"Including the target data (MLP)\")\n",
    "\n",
    "# Excluding the target data\n",
    "Model_excluding_target = ML_model(target_data=target_data,\n",
    "                                input_data=model_input_data,\n",
    "                                use_target=False,\n",
    "                                look_back=30,\n",
    "                                chunk_step=3,\n",
    "                                train_ratio=0.67,\n",
    "                                num_epochs=50,\n",
    "                                batch_size=5,\n",
    "                                predict_steps=5,\n",
    "                                plot=False,\n",
    "                                verbose=0)\n",
    "Model_excluding_target.run(model_type='LSTM')\n",
    "Model_excluding_target.plot_predict_one_step(title=\"Excluding the target data (LSTM)\")\n",
    "\n",
    "# Excluding the target data\n",
    "Model_excluding_target = ML_model(target_data=target_data,\n",
    "                                input_data=model_input_data,\n",
    "                                use_target=False,\n",
    "                                look_back=80,\n",
    "                                chunk_step=3,\n",
    "                                train_ratio=0.67,\n",
    "                                num_epochs=50,\n",
    "                                batch_size=5,\n",
    "                                predict_steps=5,\n",
    "                                plot=False,\n",
    "                                verbose=0)\n",
    "Model_excluding_target.run(model_type='MLP')\n",
    "layer_1_mean = np.mean(Model_excluding_target.weights[0], axis=1)\n",
    "layer_1_mean = layer_1_mean.reshape((Model_excluding_target.look_back, model_input_data.shape[0]))\n",
    "plt.figure()\n",
    "plt.imshow(layer_1_mean.T)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "Model_excluding_target.plot_predict_one_step(title=\"Excluding the target data (MLP)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# River Arno\n",
    "\n",
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing data\n",
    "\n",
    "# Importing all data into pandas dataframe\n",
    "df = pd.read_csv(files[5])\n",
    "\n",
    "# Find and plot gaps in the data\n",
    "all_data, col_names, is_nan, is_zero = find_data_gaps(df, plot=True, title=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows that there is a significant amount of missing data in rainfall. We choose to take data from ~ 2100 - 8300 days and ignore the rainfalls that do not have significant data within that time period. The input features thus include only the first 5 rainfalls and Temperature_Firenze. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find column indices for different datatypes\n",
    "datatypes, col_inds = find_datatypes(df)\n",
    "\n",
    "# Get time series data for all variable types\n",
    "start_ind = 2185\n",
    "stop_ind = 7165\n",
    "rain_ts, rain_name = get_time_series(col_inds[0], start_ind=start_ind, stop_ind=stop_ind, fill_zeros=False)\n",
    "temp_ts, temp_name = get_time_series(col_inds[2], start_ind=start_ind, stop_ind=stop_ind, fill_zeros=True)\n",
    "target_ts, target_name = get_time_series(col_inds[4], start_ind=start_ind, stop_ind=stop_ind, fill_zeros=True)\n",
    "# MG: Fill_zeros doesn't seem to be working here...\n",
    "\n",
    "# Remove zeros and non-zero anomalies from target\n",
    "# Note - probably not the most efficient method\n",
    "for m in range(len(target_ts)):\n",
    "    for n in range(len(target_ts[m])):\n",
    "        if n > 0 and target_ts[m][n] < 0.6:\n",
    "            target_ts[m][n] = target_ts[m][n-1]\n",
    "            \n",
    "# # Exlcuding irrelevant rainfall datasets\n",
    "rain_ts = rain_ts[0:4]\n",
    "rain_name = rain_name[0:4]\n",
    "\n",
    "plt.figure(figsize=(9, 4))\n",
    "for n in range(len(rain_ts)):\n",
    "    plt.plot(rain_ts[n], label=rain_name[n])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 4))\n",
    "for n in range(len(temp_ts)):\n",
    "    plt.plot(temp_ts[n], label=temp_name[n])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 4))\n",
    "for n in range(len(target_ts)):\n",
    "    plt.plot(target_ts[n], label=target_name[n])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows that the hydrometry timeseries displays sharp spikes followed by exponential-like decay. This behaviour is reminiscent of the Depth_to_Groundwater data explored for Aquifier Auser at the start of the notebook. We had found that pre-processing the rainfall data with an exponential filter had been effective at drawing correlations between the rainfall and targets, hence we attemp the same method here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_best, src_best = find_best_tau(target_ts=target_ts[0], rain_ts=rain_ts,\n",
    "                         rain_name=rain_name, plot=True,\n",
    "                         target_name=target_name[0],\n",
    "                         tau_array=np.linspace(1, 101, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating optimum tau for each rainfall/target combination that\n",
    "# maximises the Spearman's Rank correlation coefficient\n",
    "\n",
    "rain_tau = find_all_best_tau(rain_ts, rain_name, target_ts, target_name,\n",
    "                            tau_array=np.linspace(1, 101, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_filt = rain_ts.copy()\n",
    "rain_name_pp = rain_name.copy()\n",
    "\n",
    "# Rainfall data preprocessing\n",
    "rain_pp_all = []\n",
    "for n in range(len(rain_filt)):\n",
    "    rain_pp_ = []\n",
    "    for m in range(len(target_ts)):\n",
    "        # tau_ = rain_tau[m, n] # MG: I changed this because rain_tau looks wrong\n",
    "        tau_ = tau_best[n]\n",
    "        rain_pp_.append(exp_convolve(rain_filt[n], tau_))\n",
    "    rain_pp_all.append(rain_pp_)\n",
    "\n",
    "# Flip the dimensions of the list\n",
    "rain_pp_all = [list(x) for x in zip(*rain_pp_all)]\n",
    "\n",
    "# Temperature data preprocessing\n",
    "temp_pp = temp_ts.copy()\n",
    "temp_name_pp = temp_name.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better to genralise the earlier function probably\n",
    "\n",
    "def compile_model_input_variables_river(model_rain_ind,\n",
    "                                  model_temp_ind,\n",
    "                                  rain_pp,\n",
    "                                  temp_pp,\n",
    "                                  num_timesteps):\n",
    "    \"\"\"\n",
    "    Extract the selected variables from the preprocessed data by their indices\n",
    "    and populate an input array to be used by the model.\n",
    "    The input array will have shape: (num features, num timesteps)\n",
    "    \n",
    "    \"\"\"    \n",
    "    num_inputs = len(model_rain_ind) + len(model_temp_ind)\n",
    "    model_input_data = np.empty((num_inputs, num_timesteps))\n",
    "    count = 0\n",
    "    for i in model_rain_ind:\n",
    "        model_input_data[count, :] = rain_pp[i]\n",
    "        count += 1\n",
    "    for i in model_temp_ind:\n",
    "        model_input_data[count, :] = temp_pp[i]\n",
    "        count += 1\n",
    "    \n",
    "    return model_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rainfall data analysis\n",
    "\n",
    "r_i = 0\n",
    "t_i = 0\n",
    "rain_pp_test = exp_convolve(rain_ts[r_i], tau=rain_tau[t_i, r_i])\n",
    "#rain_pp_test = rain_pp_all[0][r_i]\n",
    "\n",
    "plot_preprocessing(rain_ts[r_i], rain_pp_test, rain_name[r_i], target_ts[t_i], target_name[t_i],\n",
    "                   crop_min=3200, crop_max=3800, title=None)\n",
    "\n",
    "\n",
    "# Looks like it would be better with tau = ~8, not sure why the correlation is maximised at ~45\n",
    "\n",
    "rain_pp_test = exp_convolve(rain_ts[r_i], tau=8)\n",
    "#rain_pp_test = rain_pp_all[0][r_i]\n",
    "\n",
    "plot_preprocessing(rain_ts[r_i], rain_pp_test, rain_name[r_i], target_ts[t_i], target_name[t_i],\n",
    "                   crop_min=3200, crop_max=3800, title=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# LSTM with or without target data\n",
    "\n",
    "\n",
    "# Including the target data \n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "# Choose variables to include in model by their indices\n",
    "target_ind = 0\n",
    "model_rain_ind = [0, 1, 2, 3]\n",
    "model_temp_ind = [0]\n",
    "\n",
    "# Compile target data and inputs to arrays\n",
    "target_data = target_ts[target_ind]\n",
    "model_input_data = compile_model_input_variables_river(model_rain_ind,\n",
    "                                                 model_temp_ind,\n",
    "                                                 rain_pp_all[target_ind],\n",
    "                                                 temp_pp,\n",
    "                                                 target_ts[target_ind].size)\n",
    "\n",
    "# Create ML model class and run it\n",
    "Model_using_target = ML_model(target_data=target_data,\n",
    "                                input_data=model_input_data,\n",
    "                                use_target=True,\n",
    "                                look_back=30,\n",
    "                                chunk_step=3,\n",
    "                                train_ratio=0.67,\n",
    "                                num_epochs=50,\n",
    "                                batch_size=5,\n",
    "                                predict_steps=5,\n",
    "                                plot=False,\n",
    "                                verbose=0)\n",
    "Model_using_target.run(model_type='MLP')\n",
    "layer_1_mean = np.mean(Model_using_target.weights[0], axis=1)\n",
    "layer_1_mean = layer_1_mean.reshape((Model_using_target.look_back, model_input_data.shape[0] + 1))\n",
    "plt.figure()\n",
    "plt.imshow(layer_1_mean.T)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "Model_using_target.plot_predict_one_step(title=\"Including the target data (MLP)\")\n",
    "\n",
    "# Excluding the target data\n",
    "Model_excluding_target = ML_model(target_data=target_data,\n",
    "                                input_data=model_input_data,\n",
    "                                use_target=False,\n",
    "                                look_back=30,\n",
    "                                chunk_step=3,\n",
    "                                train_ratio=0.67,\n",
    "                                num_epochs=50,\n",
    "                                batch_size=5,\n",
    "                                predict_steps=5,\n",
    "                                plot=False,\n",
    "                                verbose=0)\n",
    "Model_excluding_target.run(model_type='LSTM')\n",
    "Model_excluding_target.plot_predict_one_step(title=\"Excluding the target data (LSTM)\")\n",
    "\n",
    "# Excluding the target data\n",
    "Model_excluding_target = ML_model(target_data=target_data,\n",
    "                                input_data=model_input_data,\n",
    "                                use_target=False,\n",
    "                                look_back=80,\n",
    "                                chunk_step=3,\n",
    "                                train_ratio=0.67,\n",
    "                                num_epochs=70,\n",
    "                                batch_size=5,\n",
    "                                predict_steps=5,\n",
    "                                plot=False,\n",
    "                                verbose=0)\n",
    "Model_excluding_target.run(model_type='MLP')\n",
    "layer_1_mean = np.mean(Model_excluding_target.weights[0], axis=1)\n",
    "layer_1_mean = layer_1_mean.reshape((Model_excluding_target.look_back, model_input_data.shape[0]))\n",
    "plt.figure()\n",
    "plt.imshow(layer_1_mean.T)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "Model_excluding_target.plot_predict_one_step(title=\"Excluding the target data (MLP)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lake Bilancino\n",
    "\n",
    "## Data pre-processing\n",
    "\n",
    "### Data availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing data\n",
    "\n",
    "# Importing all data into pandas dataframe\n",
    "df = pd.read_csv(files[4])\n",
    "\n",
    "# Find and plot gaps in the data\n",
    "all_data, col_names, is_nan, is_zero = find_data_gaps(df, plot=True, title=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find column indices for different datatypes\n",
    "datatypes, col_inds = find_datatypes(df)\n",
    "\n",
    "# Get time series data for all variable types\n",
    "start_ind = 832\n",
    "rain_ts, rain_name = get_time_series(col_inds[0], start_ind=start_ind, fill_zeros=False)\n",
    "temp_ts, temp_name = get_time_series(col_inds[2], start_ind=start_ind, fill_zeros=True)\n",
    "target_ts1, target_name1 = get_time_series(col_inds[5], start_ind=start_ind, fill_zeros=True)\n",
    "target_ts2, target_name2 = get_time_series(col_inds[6], start_ind=start_ind, fill_zeros=True)\n",
    "target_ts = target_ts1 + target_ts2\n",
    "target_name = target_name1 + target_name2 \n",
    "\n",
    "plt.figure(figsize=(9, 4))\n",
    "for n in range(len(rain_ts)):\n",
    "    plt.plot(rain_ts[n], label=rain_name[n])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 4))\n",
    "for n in range(len(temp_ts)):\n",
    "    plt.plot(temp_ts[n], label=temp_name[n])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 4))\n",
    "plt.plot(target_ts[0], label=target_name[0])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 4))\n",
    "plt.plot(target_ts[1], label=target_name[1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rainfall analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rainfall data analysis\n",
    "\n",
    "r_i = 0\n",
    "t_i = 1\n",
    "rain_pp_test = exp_convolve(rain_ts[r_i], tau=150)\n",
    "#rain_pp_test = rain_pp_all[0][r_i]\n",
    "\n",
    "plot_preprocessing(rain_ts[r_i], rain_pp_test, rain_name[r_i], target_ts[t_i], target_name[t_i],\n",
    "                   crop_min=1600, crop_max=2000, title=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rainfall data analysis\n",
    "\n",
    "r_i = 1\n",
    "t_i = 0\n",
    "rain_pp_test = exp_convolve(rain_ts[r_i], tau=5)\n",
    "#rain_pp_test = rain_pp_all[0][r_i]\n",
    "\n",
    "plot_preprocessing(rain_ts[r_i], rain_pp_test, rain_name[r_i], target_ts[t_i], target_name[t_i],\n",
    "                   crop_min=1600, crop_max=2000, title=None)\n",
    "\n",
    "\n",
    "r_i = 1\n",
    "t_i = 0\n",
    "rain_pp_test = exp_convolve(rain_ts[r_i], tau=163)\n",
    "#rain_pp_test = rain_pp_all[0][r_i]\n",
    "\n",
    "plot_preprocessing(rain_ts[r_i], rain_pp_test, rain_name[r_i], target_ts[t_i], target_name[t_i],\n",
    "                   crop_min=1600, crop_max=2000, title=None)\n",
    "\n",
    "# Looks like tau = 5 is much better than the supposed 'max correlation' tau=163, not sure why?\n",
    "# There's a double peak in the 'find_best_tau' plot, and the first peak around tau = ~5 seems\n",
    "# more sensible than the peak at tau = ~160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_best, src_best = find_best_tau(target_ts=target_ts[0], rain_ts=rain_ts,\n",
    "                         rain_name=rain_name, plot=True,\n",
    "                         target_name=target_name[0],\n",
    "                         tau_array=np.linspace(2, 240, 80))\n",
    "\n",
    "tau_best, src_best = find_best_tau(target_ts=target_ts[1], rain_ts=rain_ts,\n",
    "                         rain_name=rain_name, plot=True,\n",
    "                         target_name=target_name[0],\n",
    "                         tau_array=np.linspace(2, 240, 80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating optimum tau for each rainfall/target combination that\n",
    "# maximises the Spearman's Rank correlation coefficient\n",
    "\n",
    "rain_tau = find_all_best_tau(rain_ts, rain_name, target_ts, target_name,\n",
    "                             tau_array=np.linspace(2, 200, 50))\n",
    "\n",
    "# Changed tau test range to avoid later peaks at ~tau=160 for first target\n",
    "# Not sure what the explanation is for correlation being higher at tau=160\n",
    "# but it seems more sensible to pick the peak at tau = ~5 as it seems visually better\n",
    "rain_tau = find_all_best_tau(rain_ts, rain_name, target_ts, target_name,\n",
    "                             tau_array=np.linspace(2, 200, 50),\n",
    "                             target_0_tau_array=np.linspace(1, 50, 50))\n",
    "# MG: Again seems like the tau's reported below are wrong..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_filt = rain_ts.copy()\n",
    "rain_name_pp = rain_name.copy()\n",
    "\n",
    "# Rainfall data preprocessing\n",
    "rain_pp_all = []\n",
    "for n in range(len(rain_filt)):\n",
    "    rain_pp_ = []\n",
    "    for m in range(len(target_ts)):\n",
    "        tau_ = rain_tau[m, n]\n",
    "        rain_pp_.append(exp_convolve(rain_filt[n], tau_))\n",
    "    rain_pp_all.append(rain_pp_)\n",
    "\n",
    "# Flip the dimensions of the list\n",
    "rain_pp_all = [list(x) for x in zip(*rain_pp_all)]\n",
    "\n",
    "# Temperature data preprocessing\n",
    "temp_pp = temp_ts.copy()\n",
    "temp_name_pp = temp_name.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better to genralise the earlier function probably\n",
    "\n",
    "def compile_model_input_variables_lake(model_rain_ind,\n",
    "                                  model_temp_ind,\n",
    "                                  rain_pp,\n",
    "                                  temp_pp,\n",
    "                                  num_timesteps):\n",
    "    \"\"\"\n",
    "    Extract the selected variables from the preprocessed data by their indices\n",
    "    and populate an input array to be used by the model.\n",
    "    The input array will have shape: (num features, num timesteps)\n",
    "    \n",
    "    \"\"\"    \n",
    "    num_inputs = len(model_rain_ind) + len(model_temp_ind)\n",
    "    model_input_data = np.empty((num_inputs, num_timesteps))\n",
    "    count = 0\n",
    "    for i in model_rain_ind:\n",
    "        model_input_data[count, :] = rain_pp[i]\n",
    "        count += 1\n",
    "    for i in model_temp_ind:\n",
    "        model_input_data[count, :] = temp_pp[i]\n",
    "        count += 1\n",
    "    \n",
    "    return model_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# LSTM with or without target data\n",
    "\n",
    "\n",
    "# Including the target data \n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "# Choose variables to include in model by their indices\n",
    "target_ind = 0\n",
    "model_rain_ind = [0, 1, 2, 3, 4]\n",
    "model_temp_ind = [0]\n",
    "\n",
    "# Compile target data and inputs to arrays\n",
    "target_data = target_ts[target_ind]\n",
    "model_input_data = compile_model_input_variables_lake(model_rain_ind,\n",
    "                                                 model_temp_ind,\n",
    "                                                 rain_pp_all[target_ind],\n",
    "                                                 temp_pp,\n",
    "                                                 target_ts[target_ind].size)\n",
    "\n",
    "# Create ML model class and run it\n",
    "Model_using_target = ML_model(target_data=target_data,\n",
    "                                input_data=model_input_data,\n",
    "                                use_target=True,\n",
    "                                look_back=30,\n",
    "                                chunk_step=3,\n",
    "                                train_ratio=0.67,\n",
    "                                num_epochs=50,\n",
    "                                batch_size=5,\n",
    "                                predict_steps=5,\n",
    "                                plot=False,\n",
    "                                verbose=0)\n",
    "Model_using_target.run(model_type='MLP')\n",
    "layer_1_mean = np.mean(Model_using_target.weights[0], axis=1)\n",
    "layer_1_mean = layer_1_mean.reshape((Model_using_target.look_back, model_input_data.shape[0] + 1))\n",
    "plt.figure()\n",
    "plt.imshow(layer_1_mean.T)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "Model_using_target.plot_predict_one_step(title=\"Including the target data (MLP)\")\n",
    "\n",
    "# Excluding the target data\n",
    "Model_excluding_target = ML_model(target_data=target_data,\n",
    "                                input_data=model_input_data,\n",
    "                                use_target=False,\n",
    "                                look_back=30,\n",
    "                                chunk_step=3,\n",
    "                                train_ratio=0.67,\n",
    "                                num_epochs=50,\n",
    "                                batch_size=5,\n",
    "                                predict_steps=5,\n",
    "                                plot=False,\n",
    "                                verbose=0)\n",
    "Model_excluding_target.run(model_type='LSTM')\n",
    "Model_excluding_target.plot_predict_one_step(title=\"Excluding the target data (LSTM)\")\n",
    "\n",
    "# Excluding the target data\n",
    "Model_excluding_target = ML_model(target_data=target_data,\n",
    "                                input_data=model_input_data,\n",
    "                                use_target=False,\n",
    "                                look_back=80,\n",
    "                                chunk_step=3,\n",
    "                                train_ratio=0.67,\n",
    "                                num_epochs=70,\n",
    "                                batch_size=5,\n",
    "                                predict_steps=5,\n",
    "                                plot=False,\n",
    "                                verbose=0)\n",
    "Model_excluding_target.run(model_type='MLP')\n",
    "layer_1_mean = np.mean(Model_excluding_target.weights[0], axis=1)\n",
    "layer_1_mean = layer_1_mean.reshape((Model_excluding_target.look_back, model_input_data.shape[0]))\n",
    "plt.figure()\n",
    "plt.imshow(layer_1_mean.T)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "Model_excluding_target.plot_predict_one_step(title=\"Excluding the target data (MLP)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# LSTM with or without target data\n",
    "\n",
    "\n",
    "# Including the target data \n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "# Choose variables to include in model by their indices\n",
    "target_ind = 1\n",
    "model_rain_ind = [0, 1, 2, 3, 4]\n",
    "model_temp_ind = [0]\n",
    "\n",
    "# Compile target data and inputs to arrays\n",
    "target_data = target_ts[target_ind]\n",
    "model_input_data = compile_model_input_variables_lake(model_rain_ind,\n",
    "                                                 model_temp_ind,\n",
    "                                                 rain_pp_all[target_ind],\n",
    "                                                 temp_pp,\n",
    "                                                 target_ts[target_ind].size)\n",
    "\n",
    "# Create ML model class and run it\n",
    "Model_using_target = ML_model(target_data=target_data,\n",
    "                                input_data=model_input_data,\n",
    "                                use_target=True,\n",
    "                                look_back=30,\n",
    "                                chunk_step=3,\n",
    "                                train_ratio=0.67,\n",
    "                                num_epochs=50,\n",
    "                                batch_size=5,\n",
    "                                predict_steps=5,\n",
    "                                plot=False,\n",
    "                                verbose=0)\n",
    "Model_using_target.run(model_type='MLP')\n",
    "layer_1_mean = np.mean(Model_using_target.weights[0], axis=1)\n",
    "layer_1_mean = layer_1_mean.reshape((Model_using_target.look_back, model_input_data.shape[0] + 1))\n",
    "plt.figure()\n",
    "plt.imshow(layer_1_mean.T)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "Model_using_target.plot_predict_one_step(title=\"Including the target data (MLP)\")\n",
    "\n",
    "# Excluding the target data\n",
    "Model_excluding_target = ML_model(target_data=target_data,\n",
    "                                input_data=model_input_data,\n",
    "                                use_target=False,\n",
    "                                look_back=30,\n",
    "                                chunk_step=3,\n",
    "                                train_ratio=0.67,\n",
    "                                num_epochs=50,\n",
    "                                batch_size=5,\n",
    "                                predict_steps=5,\n",
    "                                plot=False,\n",
    "                                verbose=0)\n",
    "Model_excluding_target.run(model_type='LSTM')\n",
    "Model_excluding_target.plot_predict_one_step(title=\"Excluding the target data (LSTM)\")\n",
    "\n",
    "# Excluding the target data\n",
    "Model_excluding_target = ML_model(target_data=target_data,\n",
    "                                input_data=model_input_data,\n",
    "                                use_target=False,\n",
    "                                look_back=80,\n",
    "                                chunk_step=3,\n",
    "                                train_ratio=0.67,\n",
    "                                num_epochs=70,\n",
    "                                batch_size=5,\n",
    "                                predict_steps=5,\n",
    "                                plot=False,\n",
    "                                verbose=0)\n",
    "Model_excluding_target.run(model_type='MLP')\n",
    "layer_1_mean = np.mean(Model_excluding_target.weights[0], axis=1)\n",
    "layer_1_mean = layer_1_mean.reshape((Model_excluding_target.look_back, model_input_data.shape[0]))\n",
    "plt.figure()\n",
    "plt.imshow(layer_1_mean.T)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "Model_excluding_target.plot_predict_one_step(title=\"Excluding the target data (MLP)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
