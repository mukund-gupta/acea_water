{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACEA WATER ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "from statsmodels.tsa.stattools import ccf\n",
    "from scipy import stats\n",
    "\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "plt.style.use('seaborn-muted')\n",
    "plt.rcParams['font.family'] = 'Arial'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing functions\n",
    "\n",
    "def preprocess(df, col_ind, start_ind=0):\n",
    "    \"\"\"Some basic preprocessing of data from selected column of dataframe.\n",
    "    This will probably need to be thought about more to deal with NaNs\n",
    "    more effectively\"\"\"\n",
    "    pd_series = df.iloc[start_ind:, col_ind]\n",
    "    pd_values = pd_series.to_numpy()\n",
    "    # max_value = np.max(np.abs(pd_values))\n",
    "    # pd_values = pd_values / max_value\n",
    "    name = df.columns[col_ind]\n",
    "    return pd_values, name\n",
    "\n",
    "def preprocess_int(df, col_ind, start_ind=3955):\n",
    "    \"\"\"Some basic preprocessing of data from selected column of dataframe.\n",
    "    This will probably need to be thought about more to deal with NaNs\n",
    "    more effectively\"\"\"\n",
    "    pd_series = df.iloc[start_ind:, col_ind]\n",
    "    pd_series = pd_series.interpolate(method='linear')\n",
    "    pd_series = pd_series.fillna(0)\n",
    "    pd_values = pd_series.to_numpy()\n",
    "    # max_value = np.max(np.abs(pd_values))\n",
    "    # pd_values = pd_values / max_value\n",
    "    name = df.columns[col_ind]\n",
    "    return pd_values, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical functions \n",
    "\n",
    "def spearman_lag(data1, data2, lag):\n",
    "    \"\"\"Calculate Spearman's rank correlation coefficient between 2 datasets,\n",
    "    with a lag applied to data2\"\"\"\n",
    "    data_length = data1.size\n",
    "    if lag > 0:\n",
    "        data2_lag = np.zeros(data_length)\n",
    "        data2_lag[lag:] = data2[:-lag] \n",
    "        data2_lag[:lag] = data2[0]\n",
    "    else:\n",
    "        data2_lag = data2\n",
    "    src, _ = stats.spearmanr(data1, data2_lag)\n",
    "    return src\n",
    "\n",
    "def cross_corr_lag(data1, data2, lag_array=None):\n",
    "    \"\"\"Calculate Spearman's rank correlation coefficient between 2 datasets,\n",
    "    for a range of different lags applied to data2\"\"\"\n",
    "    if lag_array is None:\n",
    "        lag_array = np.arange(data1.size)\n",
    "    crosscorr_lag = np.empty(len(lag_array))\n",
    "    for n in range(len(lag_array)):\n",
    "        crosscorr_lag[n] = spearman_lag(data1, data2, lag=lag_array[n])\n",
    "    return crosscorr_lag, lag_array\n",
    "\n",
    "def moving_average(x, w):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w\n",
    "\n",
    "def normalise_0_to_1(signal):\n",
    "    sig_min = np.min(signal)\n",
    "    sig_max = np.max(signal)\n",
    "    sig_norm = (signal - sig_min) / (sig_max - sig_min)\n",
    "    return sig_norm\n",
    "\n",
    "def find_datatypes(df):\n",
    "    names = df.columns\n",
    "    datatypes = ['Rainfall',\n",
    "                 'Depth_to_Groundwater',\n",
    "                 'Temperature',\n",
    "                 'Volume',\n",
    "                 'Hydrometry',\n",
    "                 'Flow_rate',\n",
    "                 'Lake_level']\n",
    "    col_inds = []\n",
    "    for n in range(len(datatypes)):\n",
    "        col_ind_type = []\n",
    "        for c in range(len(names)):\n",
    "            if datatypes[n] in names[c]:\n",
    "                col_ind_type.append(c)\n",
    "        col_inds.append(col_ind_type)\n",
    "    return datatypes, col_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting functions \n",
    "\n",
    "def plot_data_preprocessed(df, col_ind):\n",
    "    data_ts, _ = preprocess(df, col_ind)\n",
    "    data_ts_int, _ = preprocess_int(df, col_ind)\n",
    "    data_ts_size = data_ts.size\n",
    "    data_ts_int_size = data_ts_int.size\n",
    "    time_array = np.linspace(0, data_ts_size-1, data_ts_size)\n",
    "    time_array2 = np.linspace(0, data_ts_int_size-1, data_ts_int_size)\n",
    "    plt.figure()\n",
    "    plt.scatter(time_array, data_ts, s=0.2, alpha=0.6)\n",
    "    plt.scatter(time_array2, data_ts_int, s=0.2, alpha=0.6)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tau analysis functions \n",
    "\n",
    "def find_tau_correlation(target_ts, rain_ts, tau_array=None):\n",
    "    \"\"\"Calculate convolution of rainfall data with an exponential window\n",
    "    with time constant tau, for a range of values of tau.\n",
    "    Then determine how correlated these convolved signals are to the\n",
    "    target data by calculating Spearman's rank correlation coefficient\n",
    "    for each value of tau\"\"\"\n",
    "    if tau_array is None:\n",
    "        tau_array = np.linspace(2, 120, 60)\n",
    "    rain_ts = np.asarray(rain_ts)\n",
    "    target_ts = np.asarray(target_ts)\n",
    "    winlength = rain_ts.size\n",
    "    target_len = target_ts.size\n",
    "    t = np.linspace(0, winlength-1, winlength)\n",
    "    src = np.empty(len(tau_array))\n",
    "    for n in range(len(tau_array)):\n",
    "        exp_win = np.exp(-t/tau_array[n])\n",
    "        rain_conv = np.convolve(rain_ts, exp_win, 'full')[:target_len]\n",
    "        rain_conv = rain_conv / np.sum(exp_win)\n",
    "        # if n==40:\n",
    "        #     plt.figure()\n",
    "        #     plt.plot(rain_ts, label='rain_ts')\n",
    "        #     plt.plot(rain_conv, label='rain_conv')\n",
    "        #     plt.plot(target_ts, label='target_ts')\n",
    "        #     plt.legend()\n",
    "        #     plt.show()\n",
    "        src[n], _ = stats.spearmanr(target_ts, rain_conv)\n",
    "    return src, tau_array\n",
    "\n",
    "\n",
    "def find_best_tau(target_ts, rain_ts, plot=True, rain_name=None, tau_array=None):\n",
    "    \"\"\"\n",
    "    Calculate correlation of convolved rainfall signal with the\n",
    "    target signal for different time constants of exponential window\n",
    "    and select the tau value that gives the best correlation.\n",
    "    Optional plot of correlation for different tau values\n",
    "    \"\"\"\n",
    "    tau_best = []\n",
    "    if plot: plt.figure()\n",
    "    for n in range(len(rain_ts)):\n",
    "        src, tau_array = find_tau_correlation(\n",
    "                                normalise_0_to_1(target_ts),\n",
    "                                normalise_0_to_1(rain_ts[n]),\n",
    "                                tau_array=tau_array)\n",
    "        tau_best.append(tau_array[np.argmax(src)])\n",
    "        if plot: plt.plot(tau_array, src, label=rain_name[n])   \n",
    "    if plot:\n",
    "        plt.xlabel(\"tau for exponential window\")\n",
    "        plt.ylabel(\"Spearman's Rank Coefficient\")\n",
    "        title_text = 'Correlation with target for different rainfall data'\n",
    "        plt.title(title_text)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return tau_best\n",
    "\n",
    "\n",
    "def tau_and_lag_correl(target_ts, rain_ts, lag_array=None, tau_array=None,\n",
    "                       plot=True):\n",
    "    \"\"\"\n",
    "    Calculate correlation coefficients for different time lag and tau\n",
    "    values for rainfall. Optional plot.\n",
    "    \"\"\"\n",
    "    if lag_array is None:\n",
    "        lag_array = np.arange(20)\n",
    "    if tau_array is None:\n",
    "        tau_array = np.linspace(22, 90, 35).astype(np.int)\n",
    "    sp_rank_cc = []\n",
    "    for n in range(len(lag_array)):\n",
    "        lag = lag_array[n]\n",
    "        data_length = rain_ts.size\n",
    "        if lag > 0:\n",
    "            data_lag = np.zeros(data_length)\n",
    "            data_lag[lag:] = rain_ts[:-lag]\n",
    "            data_lag[:lag] = rain_ts[0]\n",
    "        else:\n",
    "            data_lag = rain_ts\n",
    "        src, _ = find_tau_correlation(target_ts, data_lag,\n",
    "                                      tau_array=tau_array)\n",
    "        sp_rank_cc.append(src)\n",
    "    sp_rank_cc = np.asarray(sp_rank_cc)\n",
    "    if plot:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        cax = ax.matshow(sp_rank_cc)\n",
    "        fig.colorbar(cax)\n",
    "        ax.set_xticks(np.arange(len(tau_array)))\n",
    "        ax.set_yticks(np.arange(len(lag_array)))\n",
    "        ax.set_xticklabels(tau_array)\n",
    "        ax.set_yticklabels(lag_array)\n",
    "        ax.xaxis.set_ticks_position('bottom')\n",
    "        plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n",
    "                 rotation_mode=\"anchor\")\n",
    "        ax.set_xlabel('Tau for exponential window')\n",
    "        ax.set_ylabel('Time lag [days]')\n",
    "        plt.title('Correlation of rainfall with target for different values of tau and time lag')\n",
    "        fig.show()\n",
    "    return sp_rank_cc, lag_array, tau_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM functions \n",
    "\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1, chunk_step=1, predict_steps=1):\n",
    "    numchunk = int(np.floor((dataset.shape[1] - look_back - 1) / chunk_step))\n",
    "    dataX = np.empty((numchunk, look_back, dataset.shape[0]))\n",
    "    dataY = np.empty((numchunk, predict_steps))\n",
    "    y_ind = []\n",
    "    # Create chunks of data with the specified look back\n",
    "    for i in range(numchunk):\n",
    "        start_ind = chunk_step*i\n",
    "        dataX[i, :, :] = dataset[:, start_ind:(start_ind + look_back)].T\n",
    "        dataY[i, :] = dataset[0,start_ind+look_back:start_ind+look_back+predict_steps] #MG\n",
    "        #dataY.append(dataset[0, start_ind + look_back])\n",
    "        y_ind.append(start_ind + look_back)\n",
    "    # Randomise order of chunks\n",
    "    rand_indices = np.random.permutation(numchunk)\n",
    "    x = np.array(dataX)\n",
    "    y = np.array(dataY)\n",
    "    y_ind = np.array(y_ind)\n",
    "    x = x[rand_indices, :]\n",
    "    y = y[rand_indices,:]\n",
    "    #y = np.reshape(y, (y.size, 1)) # MG\n",
    "    y_ind = y_ind[rand_indices]\n",
    "    return x, y, y_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing data\n",
    "\n",
    "# Importing all data\n",
    "foldpath = r\"acea-water-prediction\"\n",
    "files = list(Path(foldpath).rglob('*.csv'))\n",
    "df = pd.read_csv(files[0]) # Create dataframe\n",
    "\n",
    "# Get time series data for target variable and some other variables\n",
    "# Specified by column index of the pandas dataframe\n",
    "datatypes, col_inds = find_datatypes(df)\n",
    "col_ind_target = 13\n",
    "col_targets = col_inds[1]\n",
    "col_ind_others = [1, 15, 16]\n",
    "col_rain = col_inds[0]\n",
    "col_vol = col_inds[3]\n",
    "\n",
    "# Get target time series data\n",
    "target_ts, target_name = preprocess_int(df, col_ind_target)\n",
    "target_length = target_ts.size\n",
    "\n",
    "# Get time series data for other variables\n",
    "other_ts = []\n",
    "other_ts_int = []\n",
    "other_name = []\n",
    "for n in range(len(col_ind_others)):\n",
    "    ts_, name_ = preprocess(df, col_ind_others[n])\n",
    "    ts_int_, name_ = preprocess_int(df, col_ind_others[n])\n",
    "    other_ts.append(ts_)\n",
    "    other_ts_int.append(ts_int_)\n",
    "    other_name.append(name_)\n",
    "\n",
    "# Get time series data for all targets\n",
    "all_target_ts = []\n",
    "all_target_name = []\n",
    "for n in range(len(col_targets)):\n",
    "    ts_, name_ = preprocess_int(df, col_targets[n])\n",
    "    all_target_ts.append(ts_)\n",
    "    all_target_name.append(name_)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get time series data for all rain variables\n",
    "rain_ts = []\n",
    "rain_name = []\n",
    "for n in range(len(col_rain)):\n",
    "    ts_, name_ = preprocess_int(df, col_rain[n])\n",
    "    rain_ts.append(ts_)\n",
    "    rain_name.append(name_)\n",
    "    \n",
    "# Get time series data for all volume variables\n",
    "vol_ts = []\n",
    "vol_name = []\n",
    "for n in range(len(col_vol)):\n",
    "    ts_, name_ = preprocess_int(df, col_vol[n])\n",
    "    vol_ts.append(ts_)\n",
    "    vol_name.append(name_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nplt.figure()\\nfor n in range(len(col_targets)):\\n    plt.plot(normalise_0_to_1(all_target_ts[n]), label=all_target_name[n], lw=1, alpha=0.7)\\n# plt.plot(other_ts[0], label=other_name[0], lw=1, alpha=0.7)\\n# plt.plot(moving_average(other_ts[0], 30), label='Mov av, win=30', lw=1, alpha=0.7)\\nplt.plot(normalise_0_to_1(exp_model), label='exp model', lw=1, alpha=0.7)\\n# plt.plot(normalise_0_to_1(exp_model_av), label='exp model av', lw=1, alpha=0.7)\\n# plt.plot(moving_average(other_ts[0], 90), label='Mov av, win=90', lw=1, alpha=0.7)\\nplt.legend()\\nplt.title('Time series')\\nplt.show()\\n\\n\\nplt.figure()\\nfor n in range(len(col_rain)):\\n    plt.plot(rain_ts[n], label=rain_name[n], lw=1, alpha=0.7)\\nplt.legend()\\nplt.title('Time series')\\nplt.show()\\n\\n\\nplt.figure()\\nfor n in range(len(col_vol)):\\n    plt.plot(normalise_0_to_1(vol_ts[n]), label=vol_name[n], lw=1, alpha=0.7)\\nplt.plot(normalise_0_to_1(all_target_ts[0]), label=all_target_name[0], lw=1, alpha=0.7)\\nplt.legend()\\nplt.title('Time series')\\nplt.show()\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plotting preprocessed data\n",
    "\n",
    "# plot_data_preprocessed(df, col_ind_target) \n",
    "    \n",
    "\"\"\"\n",
    "# Calculate spearmans rank correlation for different lags\n",
    "# Note that the target data is appended to the list of other variables so\n",
    "# it is included in the analysis (effectively an autocorrelation)\n",
    "all_ts = [target_ts] + other_ts\n",
    "all_names = [target_name] + other_name\n",
    "num_datasets = len(all_ts)\n",
    "# loop through all datasets to calculate n-lag spearmans rank coefficients\n",
    "ccl = np.empty((num_datasets, target_length-1))\n",
    "for n in range(num_datasets):\n",
    "    ccl[n, :] = cross_corr_lag(target_ts, all_ts[n])\n",
    "lags = np.linspace(0, ccl.shape[1]-1, ccl.shape[1])\n",
    "\n",
    "# Plot results\n",
    "plt.figure()\n",
    "# fig, axes = plt.subplots(num_datasets, 1, sharex=True, sharey=True)\n",
    "for n in range(num_datasets):\n",
    "    plt.plot(ccl[n], label=all_names[n], lw=1, alpha=0.7)\n",
    "    # axes[n].set_ylim([-1, 1])\n",
    "plt.legend()\n",
    "plot_title = 'Correlation lag-N: ' + target_name\n",
    "plt.title(plot_title)\n",
    "plt.xlabel('')\n",
    "plt.show()\n",
    "    \"\"\"\n",
    "# plt.figure()\n",
    "# plt.plot(target_ts, label=target_name, lw=1, alpha=0.7)\n",
    "# plt.title('Time series')\n",
    "# plt.show()\n",
    "winlength = target_length\n",
    "tau = 20\n",
    "t = np.linspace(0, winlength-1, winlength)\n",
    "exp_window = np.exp(-t/tau)\n",
    "exp_model = np.convolve(other_ts[0], exp_window, 'full')\n",
    "exp_model_av = np.convolve(moving_average(other_ts[0], 7),\n",
    "                        exp_window, 'full')\n",
    "\n",
    "\"\"\"\n",
    "plt.figure()\n",
    "for n in range(len(col_targets)):\n",
    "    plt.plot(normalise_0_to_1(all_target_ts[n]), label=all_target_name[n], lw=1, alpha=0.7)\n",
    "# plt.plot(other_ts[0], label=other_name[0], lw=1, alpha=0.7)\n",
    "# plt.plot(moving_average(other_ts[0], 30), label='Mov av, win=30', lw=1, alpha=0.7)\n",
    "plt.plot(normalise_0_to_1(exp_model), label='exp model', lw=1, alpha=0.7)\n",
    "# plt.plot(normalise_0_to_1(exp_model_av), label='exp model av', lw=1, alpha=0.7)\n",
    "# plt.plot(moving_average(other_ts[0], 90), label='Mov av, win=90', lw=1, alpha=0.7)\n",
    "plt.legend()\n",
    "plt.title('Time series')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "for n in range(len(col_rain)):\n",
    "    plt.plot(rain_ts[n], label=rain_name[n], lw=1, alpha=0.7)\n",
    "plt.legend()\n",
    "plt.title('Time series')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "for n in range(len(col_vol)):\n",
    "    plt.plot(normalise_0_to_1(vol_ts[n]), label=vol_name[n], lw=1, alpha=0.7)\n",
    "plt.plot(normalise_0_to_1(all_target_ts[0]), label=all_target_name[0], lw=1, alpha=0.7)\n",
    "plt.legend()\n",
    "plt.title('Time series')\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "# vol_total = normalise_0_to_1(np.sum(vol_ts, 0))\n",
    "# vol_total_deviation = vol_total - np.convolve(vol_total, np.ones(500), 'same') / 500\n",
    "# vol_dev_smooth = np.convolve(vol_total_deviation, np.ones(10), 'same') / 10\n",
    "# target_total = np.sum(all_target_ts, 0)\n",
    "# # plt.plot(vol_total, label='total vol', lw=1, alpha=0.7)\n",
    "# # plt.plot(vol_total_deviation, label='vol deviation', lw=1, alpha=0.7)\n",
    "# plt.plot(normalise_0_to_1(vol_dev_smooth), label='vol dev smooth', lw=1, alpha=0.7)\n",
    "# plt.plot(normalise_0_to_1(exp_model), label='exp model', lw=1, alpha=0.7)\n",
    "# plt.plot(normalise_0_to_1(target_total[:-40]), label='total groundwater', lw=1, alpha=0.7)\n",
    "# plt.legend()\n",
    "# plt.title('Time series')\n",
    "# plt.show()\n",
    "# # cc = ccf(target_ts, other_ts)\n",
    "# # cc_lag = ccf(target_ts, other_ts_lag)\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(target_ts, label=target_name)\n",
    "# plt.plot(other_ts, label=other_name)\n",
    "# plt.plot(other_ts_lag, label='lag')\n",
    "# plt.plot(cc, label='cross-correlation')\n",
    "# plt.plot(cc_lag, label='cross-correlation_lag')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tau analysis\n",
    "\n",
    "\"\"\"\n",
    "Add time lag?\n",
    "Smoothing for deeper groundwater?\n",
    "\"\"\"\n",
    "# Analysis of how lag applied to rainfall affects correlation with target\n",
    "lag_array = np.arange(200)\n",
    "plt.figure()\n",
    "for n in range(len(rain_ts)):\n",
    "    crosscorr_lag, lag_array = cross_corr_lag(\n",
    "                                    normalise_0_to_1(all_target_ts[2]),\n",
    "                                    normalise_0_to_1(rain_ts[n]),\n",
    "                                    lag_array=lag_array)\n",
    "    plt.plot(lag_array, crosscorr_lag, label=rain_name[n])\n",
    "plt.legend()\n",
    "plt.xlabel('Lag applied to rainfall data [days]')\n",
    "plt.ylabel('Spearman''s rank correlation coefficient')\n",
    "plt.title('Correlation of rainfall with target for different lag amounts')\n",
    "plt.show()\n",
    "\n",
    "# Calculating optimum tau for each rainfall/target combination that\n",
    "# maximises the Spearman's Rank correlation coefficient\n",
    "all_tau_best = []\n",
    "for n in range(len(all_target_ts)):\n",
    "    if n == 0:\n",
    "        tau_array = np.linspace(205, 450, 50)\n",
    "    else:\n",
    "        tau_array = None\n",
    "    tau_best = find_best_tau(all_target_ts[n], rain_ts, plot=False,\n",
    "                             tau_array=tau_array)\n",
    "    all_tau_best.append(tau_best)\n",
    "all_tau_best = np.asarray(all_tau_best)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(all_tau_best[1:, :])\n",
    "for i in range(len(all_target_name[1:])):\n",
    "    for j in range(len(rain_name)):\n",
    "        text = ax.text(j, i, np.int(all_tau_best[i+1, j]),\n",
    "                       ha=\"center\", va=\"center\", color=\"w\")\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticks(np.arange(len(rain_name)))\n",
    "ax.set_yticks(np.arange(len(all_target_name[1:])))\n",
    "ax.set_xticklabels(rain_name)\n",
    "ax.set_yticklabels(all_target_name[1:])\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "plt.setp(ax.get_xticklabels(), rotation=70, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "plt.title('Optimum Tau value for exponential window')\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "    \n",
    "\n",
    "target_test_ind = 0\n",
    "rain_test_ind = 0\n",
    "# lag_array = np.arange(30)\n",
    "# tau_array = np.linspace(12, 90, 40).astype(np.int)\n",
    "lag_array = np.linspace(0, 60, 31).astype(np.int)\n",
    "tau_array = np.linspace(40, 2000, 50).astype(np.int)\n",
    "data1 = all_target_ts[target_test_ind]\n",
    "data2 = rain_ts[rain_test_ind]\n",
    "sp_rank_cc, lag_array, tau_array = tau_and_lag_correl(data1,\n",
    "                                                      data2,\n",
    "                                                      lag_array=lag_array,\n",
    "                                                      tau_array=tau_array,\n",
    "                                                      plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM analysis\n",
    "\n",
    "# Model parameters\n",
    "target_ind = 1\n",
    "look_back = 30\n",
    "chunk_step = 50\n",
    "train_ratio = 0.67\n",
    "num_epochs = 30\n",
    "batch_size = 5\n",
    "predict_steps = 3\n",
    "\n",
    "# Rain preprocessing\n",
    "tau_best = find_best_tau(all_target_ts[target_ind], rain_ts, plot=True,\n",
    "                         rain_name=rain_name)\n",
    "tau = tau_best[0]\n",
    "exp_win = np.exp(-t/tau)\n",
    "exp_win = exp_win / np.sum(exp_win)\n",
    "rain_len = rain_ts[0].size\n",
    "rain_conv = []\n",
    "for n in range(len(rain_ts)):\n",
    "    conv = np.convolve(rain_ts[n], exp_win, 'full')[:rain_len]\n",
    "    rain_conv.append(conv)\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "# Remove zeros from target\n",
    "# N.B. SHOULD REPLACE THIS WITH A DIFFERENT METHOD\n",
    "for n in range(len(all_target_ts[target_ind])):\n",
    "    if n > 0 and all_target_ts[target_ind][n] == 0:\n",
    "        all_target_ts[target_ind][n] = all_target_ts[target_ind][n-1]\n",
    "\n",
    "# Calculate differential of target & rainfall\n",
    "target_diff = np.diff(all_target_ts[target_ind])\n",
    "rain_conv_diff = []\n",
    "for n in range(len(rain_ts)):\n",
    "    rain_conv_diff.append(np.diff(rain_conv[n]))\n",
    "\n",
    "# normalize the datasets\n",
    "target_data = target_diff\n",
    "all_target_ts[target_ind] = all_target_ts[target_ind][1:]\n",
    "target_data = np.reshape(target_data, (target_data.size, 1))\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "target_scaled = scaler.fit_transform(target_data)\n",
    "target_scaled = np.squeeze(target_scaled)\n",
    "rain_scaled = normalise_0_to_1(rain_conv_diff[0])\n",
    "\n",
    "# Combine the dataset into single array\n",
    "dataset = np.stack((target_scaled, rain_scaled))\n",
    "\n",
    "# Plot presprocessed data\n",
    "plt.figure()\n",
    "plt.plot(dataset[0, :], label=all_target_name[target_ind])\n",
    "plt.plot(dataset[1, :], label=rain_name[0])\n",
    "plt.legend()\n",
    "plt.title('Preprocessed data')\n",
    "plt.show()\n",
    "\n",
    "# Split data into chunks with random order\n",
    "x, y, y_ind = create_dataset(dataset, look_back=look_back,\n",
    "                             chunk_step=chunk_step,predict_steps=predict_steps)\n",
    "numchunk = y.shape[0]\n",
    "\n",
    "# split into train and test sets\n",
    "train_size = int(numchunk * train_ratio)\n",
    "test_size = numchunk - train_size\n",
    "trainX, testX = x[0:train_size, :, :], x[train_size:numchunk, :, :]\n",
    "trainY, testY = y[0:train_size, :], y[train_size:numchunk, :]\n",
    "trainYind, testYind = y_ind[0:train_size], y_ind[train_size:numchunk]\n",
    "\n",
    "# Plot one example of chunk\n",
    "sample_num = 10\n",
    "plt.figure()\n",
    "time_ = np.arange(look_back)\n",
    "plt.plot(time_, trainX[sample_num, :, 0], label='Input: target')\n",
    "plt.plot(time_, trainX[sample_num, :, 1], label='Input: rain')\n",
    "plt.plot([look_back], trainY[sample_num, 0], 'xr', label='Output: target')\n",
    "plt.legend()\n",
    "plt.title(\"Example of one chunk from dataset\")\n",
    "plt.show()\n",
    "\n",
    "# create and fit the LSTM network\n",
    "num_features = x.shape[2]\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(look_back, num_features)))\n",
    "model.add(Dense(predict_steps))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=num_epochs, batch_size=batch_size, verbose=1)\n",
    "\n",
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform(trainY)\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform(testY)\n",
    "\n",
    "# Convert from diff to actual prediction\n",
    "for n in range(train_size):\n",
    "    prev_day = all_target_ts[target_ind][trainYind[n] - 1]\n",
    "    trainY[n] = prev_day + trainY[n]\n",
    "    trainPredict[n] = prev_day + trainPredict[n]\n",
    "for n in range(test_size):\n",
    "    prev_day = all_target_ts[target_ind][testYind[n] - 1]\n",
    "    testY[n] = prev_day + testY[n]\n",
    "    testPredict[n] = prev_day + testPredict[n]\n",
    "\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[:, 0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[:, 0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot one prediction example from test sample\n",
    "pred_sample = 4\n",
    "pred_ind = testYind[pred_sample]\n",
    "plt.figure()\n",
    "sample_t = np.linspace(pred_ind - look_back - 1, pred_ind - 1, look_back)\n",
    "sample_t = sample_t.astype(np.int)\n",
    "plt.plot(sample_t, all_target_ts[target_ind][sample_t[0]:sample_t[-1]])\n",
    "plt.plot(pred_ind, testPredict[pred_sample, 0], 'xr')\n",
    "plt.title(\"Example of one prediction from test data\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all predictions from test samples against original data\n",
    "plt.figure()\n",
    "plt.plot(np.linspace(0, all_target_ts[target_ind].size-1,\n",
    "                     all_target_ts[target_ind].size),\n",
    "         all_target_ts[target_ind],\n",
    "         label='Original data')\n",
    "plt.plot(testYind, testPredict[:, 0], 'xr', label='Test predictions')\n",
    "plt.plot(trainYind, trainPredict[:, 0], 'xg', label='Train predictions')\n",
    "plt.legend()\n",
    "plt.title(\"Predictions compared to original dataset\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all multistep predictions from test samples against original data\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.linspace(0, target_data.size-1, target_data.size), target_data,\n",
    "         label='Original data')\n",
    "for ii in range(len(testYind)):\n",
    "    plt.plot(np.arange(testYind[ii],testYind[ii] + predict_steps), testPredict[ii, :],color='k')\n",
    "plt.legend()\n",
    "plt.title(\"Predictions compared to original dataset\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
