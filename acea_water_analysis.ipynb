{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>ACEA Smart Water Analytics</center></h1>\n",
    "\n",
    "## 1. Introduction \n",
    "\n",
    "This notebook presents a statistical anlysis of water data provided by the Italian multiutility operator group ACEA. The dataset comprises 4 different kinds of water bodies, namely aquifier, river, lake and spring. For each of these bodies, the aim is to understand and predict the behavior of given target variables that relate to the state of the water source (e.g. flow rate, groundwater level, hydrometry, etc.). The dataset also includes information about the local environment (e.g. precipitation, temperature, etc.), which may have a relevant link to the target variables, and hence assist in their prediction. \n",
    "\n",
    "The general strategy adopted in this work was to:\n",
    "1. Clean the data, such that only reliable portions of the timeseries are taken into account\n",
    "2. Pre-process the data in a way that makes physical sense, with the aim of both enhancing our understanding of the problem and helping with the prediction.\n",
    "3. Develop machine learning models to predict the behviour of target variables, assess their performance over a range of scenarios, and report on the importance of each input feature in the prediction.\n",
    "\n",
    "This work is structured as follows: \n",
    "- Section 2 presents a comprehensive analysis based on a single aquifier dataset, demonstrating the approach and most of the techniques used throughout the notebook for other water sources.  \n",
    "- Sections 3, 4, and 5 provide a pared-down analysis for each other source type, namely lake, source and river, respectively. \n",
    "- Section 6 summarizes the main findings and provides suggestions for further analysis. \n",
    "\n",
    "\n",
    "The button below may be used to toggle the raw code on and off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guptam/opt/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'logging'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8b00241c9f52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'logging'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# %matplotlib notebook\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from pathlib import Path\n",
    "from statsmodels.tsa.stattools import ccf\n",
    "from scipy import stats\n",
    "\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import InputLayer\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "plt.style.use('seaborn-muted')\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing functions\n",
    "\n",
    "def preprocess(df, col_ind, start_ind=0):\n",
    "    \"\"\"\n",
    "    Some basic preprocessing of data from selected column of dataframe.\n",
    "    This will probably need to be thought about more to deal with NaNs\n",
    "    more effectively\n",
    "    \n",
    "    \"\"\"\n",
    "    pd_series = df.iloc[start_ind:, col_ind]\n",
    "    pd_values = pd_series.to_numpy()\n",
    "    # max_value = np.max(np.abs(pd_values))\n",
    "    # pd_values = pd_values / max_value\n",
    "    name = df.columns[col_ind]\n",
    "    \n",
    "    return pd_values, name\n",
    "\n",
    "\n",
    "def preprocess_int(df, col_ind, start_ind=0, stop_ind=-1, fill_zeros=True):\n",
    "    \"\"\"\n",
    "    Some basic preprocessing of data from selected column of dataframe.\n",
    "    This will probably need to be thought about more to deal with NaNs\n",
    "    more effectively\n",
    "    \n",
    "    \"\"\"\n",
    "    pd_series = df.iloc[start_ind:stop_ind, col_ind]\n",
    "    pd_series = pd_series.astype('float32')\n",
    "    if fill_zeros:\n",
    "#         print(np.sum(np.isnan(pd_series.to_numpy())))\n",
    "#         print('change_zero')\n",
    "#         pd_series.replace(to_replace=0, value=np.nan)\n",
    "        pd_series.mask(pd_series==0)\n",
    "#         print(np.sum(np.isnan(pd_series.to_numpy())))\n",
    "    pd_series = pd_series.interpolate(method='linear')\n",
    "    pd_series = pd_series.fillna(method='bfill')\n",
    "    pd_series = pd_series.fillna(method='ffill')\n",
    "    pd_values = pd_series.to_numpy()\n",
    "    # max_value = np.max(np.abs(pd_values))\n",
    "    # pd_values = pd_values / max_value\n",
    "    name = df.columns[col_ind]\n",
    "    \n",
    "    return pd_values, name\n",
    "\n",
    "\n",
    "def get_time_series(col_inds, start_ind=0, stop_ind=-1, fill_zeros=True):\n",
    "    \"\"\"\n",
    "    Get all of the time series from the specified column indices and their names\n",
    "    and populate lists for each\n",
    "    \n",
    "    \"\"\"\n",
    "    ts = []\n",
    "    name = []\n",
    "    \n",
    "    for n in range(len(col_inds)):\n",
    "        ts_, name_ = preprocess_int(df, col_inds[n], start_ind=start_ind, stop_ind=stop_ind, fill_zeros=fill_zeros)\n",
    "        ts.append(ts_)\n",
    "        name.append(name_)\n",
    "        \n",
    "    return ts, name\n",
    "\n",
    "\n",
    "def find_data_gaps(dataframe, plot=True, title=None):\n",
    "    \"\"\"\n",
    "    Finds NaNs and zeros in dataframe, with optional plot to show their locations\n",
    "    \n",
    "    \"\"\"\n",
    "    # Put dataframe into numpy array, ignoring date variable\n",
    "    df_nodate = df.drop(['Date'], axis=1)\n",
    "    all_data = df_nodate.to_numpy(na_value=np.nan)\n",
    "    col_names = df_nodate.columns\n",
    "    numcol = len(col_names)\n",
    "\n",
    "    # Find missing values\n",
    "    is_nan = np.isnan(all_data)\n",
    "    is_zero = (all_data == 0)\n",
    "    nan_array = np.where(is_nan, 1, np.nan)\n",
    "    zero_array = np.where(is_zero, 1, np.nan)\n",
    "\n",
    "    # Plot showing missing values and zeros\n",
    "    if plot:\n",
    "        if title is None: title = 'Location of NaNs and zeros in dataset'\n",
    "        fig = plt.figure(figsize=(9, 5))\n",
    "        ax = plt.subplot(111)\n",
    "        # divider = make_axes_locatable(ax)\n",
    "        # cax = divider.append_axes(\"top\", size=\"5%\", pad=0.08)\n",
    "        norm = mpl.colors.Normalize(vmin=-1, vmax=1)\n",
    "        sc_map = mpl.cm.ScalarMappable(norm=norm, cmap='PiYG')\n",
    "        ms = ax.imshow(nan_array.T, aspect='auto', cmap='Pastel1', interpolation='none')\n",
    "        ms2 = ax.imshow(zero_array.T, aspect='auto', cmap='Set3', interpolation='none')\n",
    "        ax.set_yticks(np.arange(numcol))\n",
    "        ax.set_yticklabels(col_names)\n",
    "        ax.set_xlabel('Time [days]')\n",
    "        cmap_nan = plt.cm.Pastel1\n",
    "        cmap_zero = plt.cm.Set3\n",
    "        custom_lines = [Line2D([0], [0], color=cmap_zero(0.), lw=5),\n",
    "                        Line2D([0], [0], color=cmap_nan(0.), lw=5)]\n",
    "        ax.legend(custom_lines, ['Zero', 'NaN'], loc='lower right')\n",
    "        plt.title(title)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return all_data, col_names, is_nan, is_zero\n",
    "\n",
    "\n",
    "def normalise_sum(signal):\n",
    "    \"\"\"\n",
    "    Normalises a signal so that the sum of all samples is unity.\n",
    "\n",
    "    :param signal: An array of values representing the signal to be normalised.\n",
    "    :returns: An array of values representing the normalised signal.\n",
    "\n",
    "    \"\"\"\n",
    "    signal = np.asarray(signal, dtype=float)\n",
    "    sum_ = np.sum(signal) if signal.any() else 1.\n",
    "\n",
    "    return signal / sum_\n",
    "\n",
    "\n",
    "def movingav(signal, winwidth, winfunc=None):\n",
    "    \"\"\"\n",
    "    Calculates the moving average of a signal using chosen window function.\n",
    "\n",
    "    :param signal: An array of values representing the signal to be smoothed.\n",
    "    :param winwidth: The width of the smoothing window (number of samples).\n",
    "    :param winfunc: The window function to use for smoothing. By default, a\n",
    "        rectangular window will be used.\n",
    "    :returns: An array of values representing the smoothed signal.\n",
    "    :raises ValueError: If window width is negative.\n",
    "    :raises ValueError: If window width exceeds length of input array.\n",
    "\n",
    "    \"\"\"\n",
    "    numsamples = len(signal)\n",
    "\n",
    "    hww = int(winwidth / 2.)\n",
    "    winwidth = 2 * hww + 1\n",
    "\n",
    "    if winwidth < 0:\n",
    "        raise ValueError(\"window width must not be negative\")\n",
    "\n",
    "    if winwidth >= numsamples:\n",
    "        raise ValueError(\"window width must not exceed length of input array\")\n",
    "\n",
    "    win = np.ones(winwidth) if winfunc is None else winfunc(winwidth)\n",
    "\n",
    "    win = normalise_sum(win)\n",
    "\n",
    "    halfwin2 = normalise_sum(win[hww:])\n",
    "    halfwin1 = normalise_sum(win[:hww+1])\n",
    "\n",
    "    valstart = np.dot(halfwin2, signal[0:hww+1])\n",
    "    valend = np.dot(halfwin1, signal[numsamples - hww - 1:])\n",
    "\n",
    "    signal = np.concatenate((np.ones(hww) * valstart, signal,\n",
    "                             np.ones(hww) * valend))\n",
    "\n",
    "    wpos = hww\n",
    "    wend = len(signal) - hww - 1\n",
    "\n",
    "    sig_smooth = np.empty(numsamples)\n",
    "\n",
    "    while wpos <= wend:\n",
    "        sig_smooth[wpos - hww] = np.dot(signal[wpos-hww: wpos+hww+1], win)\n",
    "        wpos += 1\n",
    "\n",
    "    return sig_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical functions \n",
    "\n",
    "def spearman_lag(data1, data2, lag):\n",
    "    \"\"\"Calculate Spearman's rank correlation coefficient between 2 datasets,\n",
    "    with a lag applied to data2\"\"\"\n",
    "    \n",
    "    data_length = data1.size\n",
    "    if lag > 0:\n",
    "        data2_lag = np.zeros(data_length)\n",
    "        data2_lag[lag:] = data2[:-lag] \n",
    "        data2_lag[:lag] = data2[0]\n",
    "    else:\n",
    "        data2_lag = data2\n",
    "    src, _ = stats.spearmanr(data1, data2_lag)\n",
    "    \n",
    "    return src\n",
    "\n",
    "\n",
    "def cross_corr_lag(data1, data2, lag_array=None):\n",
    "    \"\"\"Calculate Spearman's rank correlation coefficient between 2 datasets,\n",
    "    for a range of different lags applied to data2\"\"\"\n",
    "    if lag_array is None:\n",
    "        lag_array = np.arange(data1.size)\n",
    "    crosscorr_lag = np.empty(len(lag_array))\n",
    "    for n in range(len(lag_array)):\n",
    "        crosscorr_lag[n] = spearman_lag(data1, data2, lag=lag_array[n])\n",
    "        \n",
    "    return crosscorr_lag, lag_array\n",
    "\n",
    "\n",
    "def moving_average(x, w):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w\n",
    "\n",
    "\n",
    "def normalise_0_to_1(signal):\n",
    "#     signal = np.asarray(signal)\n",
    "    sig_min = np.min(signal)\n",
    "    sig_max = np.max(signal)\n",
    "    sig_norm = (signal - sig_min) / (sig_max - sig_min)\n",
    "    return sig_norm\n",
    "\n",
    "\n",
    "def exp_convolve(data, tau, winlength=None):\n",
    "    \"\"\"\n",
    "    Convolve input data with an exponential window function (time constant = tau)\n",
    "    \n",
    "    \"\"\"\n",
    "    data = np.asarray(data)\n",
    "    len_data = data.size\n",
    "    if winlength is None:\n",
    "        winlength = len_data\n",
    "    t = np.linspace(0, winlength-1, winlength)\n",
    "    exp_window = np.exp(-t / tau)\n",
    "    exp_window = exp_window / np.sum(exp_window)\n",
    "    data_conv = np.convolve(data, exp_window, 'full')[:len_data]\n",
    "\n",
    "    return data_conv\n",
    "\n",
    "\n",
    "def find_datatypes(df):\n",
    "    \"\"\"Find the indices of each pf the different datatypes in the dataframe\"\"\"\n",
    "    names = df.columns\n",
    "    datatypes = ['Rainfall',\n",
    "                 'Depth_to_Groundwater',\n",
    "                 'Temperature',\n",
    "                 'Volume',\n",
    "                 'Hydrometry',\n",
    "                 'Flow_Rate',\n",
    "                 'Lake_Level']\n",
    "    col_inds = []\n",
    "    \n",
    "    for n in range(len(datatypes)):\n",
    "        col_ind_type = []\n",
    "        for c in range(len(names)):\n",
    "            if datatypes[n] in names[c]:\n",
    "                col_ind_type.append(c)\n",
    "        col_inds.append(col_ind_type)\n",
    "        \n",
    "    return datatypes, col_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting functions \n",
    "\n",
    "def plot_data_preprocessed(df, col_ind):\n",
    "    data_ts, _ = preprocess(df, col_ind)\n",
    "    data_ts_int, _ = preprocess_int(df, col_ind)\n",
    "    data_ts_size = data_ts.size\n",
    "    data_ts_int_size = data_ts_int.size\n",
    "    time_array = np.linspace(0, data_ts_size-1, data_ts_size)\n",
    "    time_array2 = np.linspace(0, data_ts_int_size-1, data_ts_int_size)\n",
    "    plt.figure()\n",
    "    plt.scatter(time_array, data_ts, s=0.2, alpha=0.6)\n",
    "    plt.scatter(time_array2, data_ts_int, s=0.2, alpha=0.6)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_preprocessing(input_raw, input_pp, input_name, target_ts, target_name,\n",
    "                       crop_min=1600, crop_max=2000, title=None):\n",
    "    \"\"\"\n",
    "    Plot raw data and preprocessed data compared to target variable\n",
    "    \n",
    "    \"\"\"\n",
    "    cmap = plt.get_cmap('Dark2')\n",
    "    colors = [cmap(i) for i in np.linspace(0, 1, 8)]\n",
    "    len_data = input_raw.size\n",
    "    fig, ax = plt.subplots(3, 2, sharey=True, figsize=(9, 4))\n",
    "    y_max = 1.2\n",
    "    \n",
    "    ax[1, 0].set_ylim(0, y_max)\n",
    "    ax[0, 0].plot(normalise_0_to_1(input_raw), color=colors[0], alpha=0.8)\n",
    "    ax[1, 0].plot(normalise_0_to_1(input_pp), color=colors[1], alpha=0.8)\n",
    "    ax[2, 0].plot(normalise_0_to_1(target_ts), color=colors[2], alpha=0.8)\n",
    "    label1 = '{}, raw data'.format(input_name)\n",
    "    label2 = '{}, pre-processed'.format(input_name)\n",
    "    label3 = target_name\n",
    "    ax[0, 1].text(0.01, 0.95, label1, transform=ax[0, 1].transAxes, va='top', ha='left')\n",
    "    ax[1, 1].text(0.01, 0.95, label2, transform=ax[1, 1].transAxes, va='top', ha='left')\n",
    "    ax[2, 1].text(0.01, 0.95, label3, transform=ax[2, 1].transAxes, va='top', ha='left')\n",
    "    \n",
    "    xmin = crop_min\n",
    "    xmax = crop_max\n",
    "    for n in range(3):\n",
    "        ax[n, 0].plot([xmin, xmin], [0, y_max], 'k--', alpha=0.6)\n",
    "        ax[n, 0].plot([xmax, xmax], [0, y_max], 'k--', alpha=0.6)\n",
    "        ax[n, 0].set_xlim(0, len_data)\n",
    "        ax[n, 1].set_xlim(xmin, xmax)\n",
    "        con = mpl.patches.ConnectionPatch(xyA=[len_data, y_max/2], coordsA=ax[n, 0].transData,\n",
    "                                          xyB=[xmin, y_max/2], coordsB=ax[n, 1].transData,\n",
    "                                          arrowstyle='->')\n",
    "        fig.add_artist(con)\n",
    "        \n",
    "    days = np.arange(xmin, xmax) \n",
    "    ax[0, 1].plot(days, normalise_0_to_1(input_raw)[xmin: xmax], color=colors[0], alpha=0.8)\n",
    "    ax[1, 1].plot(days, normalise_0_to_1(input_pp)[xmin: xmax], color=colors[1], alpha=0.8)\n",
    "    ax[2, 1].plot(days, normalise_0_to_1(target_ts)[xmin: xmax], color=colors[2], alpha=0.8)\n",
    "    \n",
    "    for n in range(2):\n",
    "        ax[n, 0].axes.xaxis.set_ticklabels([])\n",
    "        ax[n, 1].axes.xaxis.set_ticklabels([])\n",
    "        ax[2, n].set_xlabel('Time [days]')\n",
    "        \n",
    "    if title is None:\n",
    "        title = input_name + ' raw and pre-processed data, compared to ' + target_name\n",
    "    fig.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def plot_correl_matrix(data, names, title=None):\n",
    "    \"\"\"\n",
    "    Plot cross-correlation matrix for input data, using Spearman's rank\n",
    "    data should have shape: (num variables, num samples)\n",
    "    names should be list of length (num variables)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Calculate cross-correlation\n",
    "    cross_corr, p_value = stats.spearmanr(data.T, nan_policy='omit')\n",
    "    \n",
    "    # Plot results\n",
    "    numdata = len(names)\n",
    "    if title is None:\n",
    "        title = 'Cross-correlation matrix'\n",
    "    fig = plt.figure(figsize=(9, 9))\n",
    "    ax = plt.subplot(111)\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.08)\n",
    "    norm = mpl.colors.Normalize(vmin=-1, vmax=1)\n",
    "    sc_map = mpl.cm.ScalarMappable(norm=norm, cmap='PiYG')\n",
    "    ms = ax.imshow(cross_corr, cmap='PiYG', interpolation='none', vmin=-1, vmax=1)\n",
    "    fig.suptitle(title)\n",
    "    ax.set_xticks(np.arange(numdata))\n",
    "    ax.set_yticks(np.arange(numdata))\n",
    "    ax.set_xticklabels(names)\n",
    "    ax.set_yticklabels(names)\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    plt.setp(ax.get_xticklabels(), rotation=60, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "    plt.colorbar(mappable=sc_map, cax=cax)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "      \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tau analysis functions \n",
    "\n",
    "def find_tau_correlation(target_ts, rain_ts, tau_array=None):\n",
    "    \"\"\"\n",
    "    Calculate convolution of rainfall data with an exponential window\n",
    "    with time constant tau, for a range of values of tau.\n",
    "    Then determine how correlated these convolved signals are to the\n",
    "    target data by calculating Spearman's rank correlation coefficient\n",
    "    for each value of tau\n",
    "    \n",
    "    \"\"\"\n",
    "    if tau_array is None:\n",
    "        tau_array = np.linspace(2, 120, 60)\n",
    "        \n",
    "    rain_ts = np.asarray(rain_ts)\n",
    "    target_ts = np.asarray(target_ts)\n",
    "    winlength = rain_ts.size\n",
    "    target_len = target_ts.size\n",
    "    \n",
    "    t = np.linspace(0, winlength-1, winlength)\n",
    "    src = np.empty(len(tau_array))\n",
    "    \n",
    "    for n in range(len(tau_array)):\n",
    "        exp_win = np.exp(-t/tau_array[n])\n",
    "        rain_conv = np.convolve(rain_ts, exp_win, 'full')[:target_len]\n",
    "        rain_conv = rain_conv / np.sum(exp_win)\n",
    "        # if n==40:\n",
    "        #     plt.figure()\n",
    "        #     plt.plot(rain_ts, label='rain_ts')\n",
    "        #     plt.plot(rain_conv, label='rain_conv')\n",
    "        #     plt.plot(target_ts, label='target_ts')\n",
    "        #     plt.legend()\n",
    "        #     plt.show()\n",
    "        src[n], _ = stats.spearmanr(target_ts, rain_conv)\n",
    "        \n",
    "    return src, tau_array\n",
    "\n",
    "\n",
    "def find_best_tau(target_ts, rain_ts, plot=True, rain_name=None, target_name=None, tau_array=None):\n",
    "    \"\"\"\n",
    "    Calculate correlation of convolved rainfall signal with the\n",
    "    target signal for different time constants of exponential window\n",
    "    and select the tau value that gives the best correlation.\n",
    "    Optional plot of correlation for different tau values.\n",
    "    \n",
    "    \"\"\"\n",
    "    tau_best = []\n",
    "    src_best = []\n",
    "    if plot: plt.figure()\n",
    "    for n in range(len(rain_ts)):\n",
    "        src, tau_array = find_tau_correlation(\n",
    "                                normalise_0_to_1(target_ts),\n",
    "                                normalise_0_to_1(rain_ts[n]),\n",
    "                                tau_array=tau_array)\n",
    "        tau_best.append(tau_array[np.argmax(src)])\n",
    "        src_best.append(src[np.argmax(src)])\n",
    "        if plot: plt.plot(tau_array, src, label=rain_name[n], lw=1.5, alpha=0.7)  \n",
    "            \n",
    "    if plot:\n",
    "        plt.xlabel(\"Time constant (tau) for convolution with exponential window\")\n",
    "        plt.ylabel(\"Spearman's Rank correlation coefficient\")\n",
    "        title_text = 'Correlation of convolved rainfall data with {} for different tau values'.format(target_name)\n",
    "        plt.title(title_text, wrap=True)\n",
    "        min_corr = np.min(src)\n",
    "        if min_corr < 0:\n",
    "            plt.ylim(min_corr, 1)\n",
    "        else:\n",
    "            plt.ylim(0, 1)\n",
    "        plt.legend(fontsize=9)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    return tau_best, src_best\n",
    "\n",
    "\n",
    "def find_all_best_tau(input_ts, input_name, target_ts, target_name, tau_array=None,\n",
    "                      target_0_tau_array=None, plot=True):\n",
    "    \"\"\"\n",
    "    Calculating optimum tau for each input/target combination that\n",
    "    maximises the Spearman's Rank correlation coefficient when the input is\n",
    "    convolved with an exponential window (time constant tau)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Find best tau values\n",
    "    all_tau_best = []\n",
    "    all_src_best = []\n",
    "    for n in range(len(target_ts)):\n",
    "        if n == 0 and target_0_tau_array is not None:\n",
    "            tau_ = target_0_tau_array\n",
    "        else:\n",
    "            tau_ = tau_array\n",
    "        tau_best, src_best = find_best_tau(target_ts[n], input_ts, plot=False,\n",
    "                                           tau_array=tau_)\n",
    "        all_tau_best.append(tau_best)\n",
    "        all_src_best.append(src_best)\n",
    "    all_tau_best = np.asarray(all_tau_best)\n",
    "    all_src_best = np.asarray(all_src_best)\n",
    "\n",
    "    # Plot results\n",
    "    if plot:\n",
    "        fig = plt.figure()\n",
    "        ax = plt.subplot(111)\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes(\"right\", size=0.3, pad=0.08)\n",
    "    #     cbar_min = np.min(all_tau_best)\n",
    "    #     cbar_max = np.max(all_tau_best[1:, :]) + 5\n",
    "        cbar_min = 0\n",
    "        cbar_max = 1\n",
    "        norm = mpl.colors.Normalize(vmin=cbar_min, vmax=cbar_max)\n",
    "        sc_map = mpl.cm.ScalarMappable(norm=norm, cmap='viridis')\n",
    "        ms = ax.imshow(all_src_best, norm=norm, cmap='viridis')\n",
    "        for i in range(len(target_name)):\n",
    "            for j in range(len(input_name)):\n",
    "                if (cbar_max - all_tau_best[i, j]) / (cbar_max - cbar_min) < 0.2:\n",
    "                    text_col = 'k'\n",
    "                else:\n",
    "                    text_col = 'w'\n",
    "                text = ax.text(j, i, np.int(all_tau_best[i, j]),\n",
    "                               ha=\"center\", va=\"center\", color=text_col,\n",
    "                               fontsize=8)\n",
    "        fig.suptitle('Correlation of convolved rainfall with target, for optimum tau values')\n",
    "        ax.set_xticks(np.arange(len(input_name)))\n",
    "        ax.set_yticks(np.arange(len(target_name)))\n",
    "        ax.set_xticklabels(input_name)\n",
    "        ax.set_yticklabels(target_name)\n",
    "        ax.xaxis.set_ticks_position('bottom')\n",
    "        plt.setp(ax.get_xticklabels(), rotation=60, ha=\"right\",\n",
    "                 rotation_mode=\"anchor\")\n",
    "    #     plt.colorbar(mappable=sc_map, cax=cax, extend='max')\n",
    "        plt.colorbar(mappable=sc_map, cax=cax)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return all_tau_best\n",
    "\n",
    "\n",
    "def tau_and_lag_correl(target_ts, rain_ts, lag_array=None, tau_array=None,\n",
    "                       plot=True):\n",
    "    \"\"\"\n",
    "    Calculate correlation coefficients for different time lag and tau\n",
    "    values for rainfall. Optional plot.\n",
    "\n",
    "    \"\"\"\n",
    "    # Set default lag and tau values to analyse\n",
    "    if lag_array is None:\n",
    "        lag_array = np.arange(20)\n",
    "    if tau_array is None:\n",
    "        tau_array = np.linspace(22, 90, 35).astype(np.int)\n",
    "        \n",
    "    # Calculate Spearman's Rank coefficient for each tau and lag combination\n",
    "    sp_rank_cc = []\n",
    "    for n in range(len(lag_array)):\n",
    "        lag = lag_array[n]\n",
    "        data_length = rain_ts.size\n",
    "        if lag > 0:\n",
    "            data_lag = np.zeros(data_length)\n",
    "            data_lag[lag:] = rain_ts[:-lag]\n",
    "            data_lag[:lag] = rain_ts[0]\n",
    "        else:\n",
    "            data_lag = rain_ts\n",
    "        src, _ = find_tau_correlation(target_ts, data_lag,\n",
    "                                      tau_array=tau_array)\n",
    "        sp_rank_cc.append(src)\n",
    "    sp_rank_cc = np.asarray(sp_rank_cc)\n",
    "    \n",
    "    # Plot matrix showing correlation for each tau and lag combination\n",
    "    if plot:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        im = ax.imshow(sp_rank_cc)\n",
    "        ax.set_xticks(np.arange(len(tau_array)))\n",
    "        ax.set_yticks(np.arange(len(lag_array)))\n",
    "        ax.set_xticklabels(tau_array)\n",
    "        ax.set_yticklabels(lag_array)\n",
    "        ax.xaxis.set_ticks_position('bottom')\n",
    "        plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n",
    "                 rotation_mode=\"anchor\")\n",
    "        plt.setp(ax.get_xticklabels()[::2], visible=False)\n",
    "        plt.setp(ax.get_yticklabels()[::2], visible=False)\n",
    "        ax.set_xlabel('Tau for exponential window')\n",
    "        ax.set_ylabel('Time lag [days]')\n",
    "        ax.set_title('Correlation of rainfall with target for different values of tau and time lag')\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes(\"right\", \"5%\", pad=\"5%\")\n",
    "        plt.colorbar(im, cax=cax)\n",
    "        fig.show()\n",
    "        \n",
    "    return sp_rank_cc, lag_array, tau_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volume pre-processing\n",
    "\n",
    "def preprocess_vol(vol_data, smooth_amt_1=800, smooth_amt_2=30):\n",
    "    \"\"\"Pre-process volume data by smoothing, calculating deviation from the smoothed data,\n",
    "    and then smoothing again\"\"\"\n",
    "    deviation = vol_data - movingav(vol_data, smooth_amt_1, winfunc=np.hanning)\n",
    "    smoothed = movingav(deviation, smooth_amt_2, winfunc=np.hanning)\n",
    "    return normalise_0_to_1(smoothed)\n",
    "    \n",
    "\n",
    "def find_best_vol_params(vol_ts, target_ts, smoothing_1=None, smoothing_2=None, plot=True):\n",
    "    \"\"\"Calculate the correlation of the preprocessed volume data with the target data for\n",
    "    different lengths of smoothing window, in order to find the most appropropriate settings\"\"\"\n",
    "    if smoothing_1 is None:\n",
    "        smoothing_1 = np.linspace(50, 2000, 40).astype(np.int)\n",
    "    if smoothing_2 is None:\n",
    "        smoothing_2 = np.linspace(2, 40, 20).astype(np.int)\n",
    "    num_smth_1 = smoothing_1.size\n",
    "    num_smth_2 = smoothing_2.size\n",
    "    coeffs = np.empty((num_smth_1, num_smth_2))\n",
    "    for i in range(num_smth_1):\n",
    "        for j in range(num_smth_2):\n",
    "            vol_processed = preprocess_vol(vol_ts, smooth_amt_1=smoothing_1[i],\n",
    "                                           smooth_amt_2=smoothing_2[j])\n",
    "            corr_coeff, _ = stats.spearmanr(vol_processed, target_ts)\n",
    "            coeffs[i, j] = corr_coeff\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        ax = plt.subplot(111)\n",
    "        im = ax.imshow(coeffs.T)\n",
    "#         norm = mpl.colors.Normalize(vmin=0, vmax=1)\n",
    "#         sc_map = mpl.cm.ScalarMappable(norm=norm, cmap='viridis')\n",
    "#         plt.colorbar(mappable=sc_map)\n",
    "        ax.figure.colorbar(im, ax=ax)\n",
    "        ax.set_xticks(np.arange(num_smth_1))\n",
    "        ax.set_yticks(np.arange(num_smth_2))\n",
    "        ax.set_xticklabels(smoothing_1)\n",
    "        ax.set_yticklabels(smoothing_2)\n",
    "        ax.xaxis.set_ticks_position('bottom')\n",
    "        plt.setp(ax.get_xticklabels(), rotation=60, ha=\"right\",\n",
    "                 rotation_mode=\"anchor\")\n",
    "        plt.show()\n",
    "        \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelling functions \n",
    "\n",
    "\n",
    "def run_model(target_data, input_data, model_type='LSTM', use_target=True, look_back=30,\n",
    "              chunk_step=50, train_ratio=0.67, num_epochs=30, batch_size=5, predict_steps=15,\n",
    "              plot=True, verbose=0, MLP_layers=None, print_score=False):\n",
    "    \"\"\"\n",
    "    Wrapper function for machine learning model\n",
    "    \n",
    "    model_type defines which machine learning model to use:\n",
    "        - If model_type is 'MLP', model will be a multi-layer perceptron model.\n",
    "        - Otherwise (e.g. for default value 'LSTM'), model will be an LSTM model.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Normalize the target dataset\n",
    "    target_data = np.reshape(target_data, (target_data.size, 1))\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    target_scaled = scaler.fit_transform(target_data)\n",
    "    target_scaled = np.squeeze(target_scaled)\n",
    "\n",
    "    # Normalise the input variables\n",
    "    num_input = model_input_data.shape[0]\n",
    "    input_data_scaled = np.empty(input_data.shape)\n",
    "    for n in range(num_input):\n",
    "        input_data_scaled[n, :] = normalise_0_to_1(input_data[n, :])\n",
    "\n",
    "    # Combine target and inputs into single array\n",
    "    num_timesteps = target_data.size\n",
    "    model_data = np.empty((num_input + 1, num_timesteps))\n",
    "    model_data[0, :] = target_scaled\n",
    "    model_data[1:, :] = input_data_scaled\n",
    "#     print('Model_data shape: {}'.format(model_data.shape))\n",
    "\n",
    "    # Plot preprocessed data\n",
    "    if plot:\n",
    "        plt.figure(figsize=(8, 3))\n",
    "        plt.plot(model_data[0, :], label=target_name[target_ind])\n",
    "        plt.plot(model_data[1, :], label='input')\n",
    "        plt.legend()\n",
    "        plt.title('Preprocessed data')\n",
    "        plt.show()\n",
    "\n",
    "    # Split data into chunks with random order\n",
    "    x, y, y_ind = create_dataset(model_data, use_target=use_target, look_back=look_back,\n",
    "                                 chunk_step=chunk_step, predict_steps=predict_steps)\n",
    "    numchunk = y.shape[0]\n",
    "#     print('x shape: {}'.format(x.shape))\n",
    "#     print('y shape: {}'.format(y.shape))\n",
    "\n",
    "    # split into train and test sets\n",
    "    train_size = int(numchunk * train_ratio)\n",
    "    test_size = numchunk - train_size\n",
    "    trainX, testX = x[0:train_size, :, :], x[train_size:numchunk, :, :]\n",
    "    trainY, testY = y[0:train_size, :], y[train_size:numchunk, :]\n",
    "    trainYind, testYind = y_ind[0:train_size], y_ind[train_size:numchunk]\n",
    "    \n",
    "    # Plot one example of chunk\n",
    "    if plot:\n",
    "        sample_num = 0\n",
    "        plt.figure(figsize=(8, 3))\n",
    "        time_ = np.arange(look_back)\n",
    "        plt.plot(time_, trainX[sample_num, :, 0], label='Input: target')\n",
    "        plt.plot(time_, trainX[sample_num, :, 1], label='Input: rain')\n",
    "        plt.plot([look_back], trainY[sample_num, 0], 'xr', label='Output: target')\n",
    "        plt.legend()\n",
    "        plt.title(\"Example of one chunk from dataset\")\n",
    "        plt.show()\n",
    "\n",
    "    # create and fit the LSTM network\n",
    "    num_features = x.shape[2]\n",
    "    \n",
    "    if model_type == 'MLP':\n",
    "        # flatten inputs for MLP model\n",
    "        n_input = x.shape[1] * x.shape[2]\n",
    "        trainX = trainX.reshape((trainX.shape[0], n_input))\n",
    "        testX = testX.reshape((testX.shape[0], n_input))\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    if model_type == 'MLP':\n",
    "        model.add(InputLayer(input_shape=(n_input, )))\n",
    "        if MLP_layers is not None:\n",
    "            for n in range(len(MLP_layers)):\n",
    "                model.add(Dense(MLP_layers[n], activation='relu'))\n",
    "        else:\n",
    "            model.add(Dense(500, activation='relu'))\n",
    "            model.add(Dense(100, activation='relu'))\n",
    "    else:\n",
    "        model.add(InputLayer(input_shape=(look_back, num_features)))\n",
    "        model.add(LSTM(4))\n",
    "    model.add(Dense(predict_steps))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.fit(trainX, trainY, epochs=num_epochs, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "    # make predictions\n",
    "    trainPredict = model.predict(trainX)\n",
    "    testPredict = model.predict(testX)\n",
    "\n",
    "    # invert predictions\n",
    "    trainPredict = scaler.inverse_transform(trainPredict)\n",
    "    trainY = scaler.inverse_transform(trainY)\n",
    "    testPredict = scaler.inverse_transform(testPredict)\n",
    "    testY = scaler.inverse_transform(testY)\n",
    "\n",
    "    # calculate RMSE and MSE\n",
    "    trainRMSE = mean_squared_error(trainY, trainPredict, multioutput='raw_values', squared=False)\n",
    "    testRMSE = mean_squared_error(testY, testPredict, multioutput='raw_values', squared=False)\n",
    "    trainMSE = mean_squared_error(trainY, trainPredict, multioutput='raw_values', squared=True)\n",
    "    testMSE = mean_squared_error(testY, testPredict, multioutput='raw_values', squared=True)\n",
    "    \n",
    "    # Print RMSE scores\n",
    "    if print_score:\n",
    "        print('Train Score: %.2f RMSE, %.2f MSE' % (trainRMSE[0], trainMSE[0]))\n",
    "        print('Test Score:  %.2f RMSE, %.2f MSE' % (testRMSE[0], testMSE[0]))\n",
    "    \n",
    "    return model, trainRMSE, testRMSE, trainMSE, testMSE, testYind, testPredict, \\\n",
    "            trainYind, trainPredict\n",
    "\n",
    "\n",
    "class ML_model(object):\n",
    "    \"\"\"\n",
    "    Stores a set of input data, target data, hyperparams and results for machine learning model.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, target_data, input_data, use_target=True, look_back=30,\n",
    "                 chunk_step=50, train_ratio=0.67, num_epochs=30, batch_size=5,\n",
    "                 predict_steps=15, plot=False, verbose=0):\n",
    "        \"\"\"\n",
    "        Initialise ML_model object\n",
    "        target_data: output variable for model, should have shape (num_timesteps)\n",
    "        input_data: input data for model, should have shape (numfeatures, num_timesteps)\n",
    "        use_target: if True, target variable will be used as an input, otherwise it will be excluded\n",
    "        look_back: number of timesteps behind output that the model will use\n",
    "        chunk_step: the spacing between chunks of input data (smaller number means more chunks)\n",
    "        train_ratio: proportion of total chunks that are used to train the model\n",
    "        num_epochs: Number of epochs in the model training\n",
    "        batch_size: batch size for the model training\n",
    "        predict_steps: number of timesteps that the model will predict ahead of input data\n",
    "        plot: if True, plots illustrating model will be shown\n",
    "        verbose: defines how much info on model training is output (can be 0, 1 or 2)\n",
    "        \n",
    "        \"\"\"\n",
    "        self.target_data = target_data\n",
    "        self.input_data = input_data\n",
    "        self.use_target = use_target\n",
    "        self.look_back = look_back\n",
    "        self.chunk_step = chunk_step\n",
    "        self.train_ratio = train_ratio\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.predict_steps = predict_steps\n",
    "        self.model_plot = plot\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def run(self, model_type='LSTM', print_score=False):\n",
    "        \"\"\"\n",
    "        Creates and runs a model, of the type specified by 'model_type':\n",
    "          - If model_type is 'MLP', model will be a multi-layer perceptron model.\n",
    "          - Otherwise (e.g. for default value 'LSTM'), model will be an LSTM model.        \n",
    "        Outputs are stored in class variables:\n",
    "        self.trainRMSE: RMSE score for predictions from training data\n",
    "        self.testRMSE: RMSE score for predictions from test data\n",
    "        self.testYind: timestep indices of the prediction from test chunks\n",
    "        self.trainYind: timestep indices of the prediction from training chunks\n",
    "        self.testPredict: Predictions from test data\n",
    "        self.trainPredict: Predictions from training data\n",
    "        \"\"\"\n",
    "        self.model_type = model_type\n",
    "        self.model, self.trainRMSE, self.testRMSE, self.trainMSE, \\\n",
    "        self.testMSE, self.testYind, self.testPredict, \\\n",
    "        self.trainYind, self.trainPredict = run_model(target_data=self.target_data,\n",
    "                                                      input_data=self.input_data,\n",
    "                                                      model_type=self.model_type,\n",
    "                                                      use_target=self.use_target,\n",
    "                                                      look_back=self.look_back,\n",
    "                                                      chunk_step=self.chunk_step,\n",
    "                                                      train_ratio=self.train_ratio,\n",
    "                                                      num_epochs=self.num_epochs,\n",
    "                                                      batch_size=self.batch_size,\n",
    "                                                      predict_steps=self.predict_steps,\n",
    "                                                      plot=self.model_plot,\n",
    "                                                      verbose=self.verbose,\n",
    "                                                      print_score=print_score)\n",
    "        \n",
    "        self.weights = []\n",
    "        for n in range(len(self.model.layers)):\n",
    "            self.weights.append(self.model.layers[n].get_weights()[0])\n",
    "            \n",
    "        return\n",
    "\n",
    "    \n",
    "    def plot_predict_one_step(self, crop_min=None, crop_max=None, title=None):\n",
    "        \"\"\"\n",
    "        Plot predictions from test and training samples against original data,\n",
    "        for first timestep prediction only (i.e. no lookahead)\n",
    "        x-axis (time) can be cropped to between 'crop_min' and 'crop_max' to show\n",
    "        a particular time range if desired.\n",
    "        \n",
    "        \"\"\"\n",
    "        if title == None:\n",
    "            title = \"Single-step prediction compared to original dataset\"\n",
    "        \n",
    "        \n",
    "        plt.figure(figsize=(9, 4))\n",
    "        plt.plot(np.linspace(0, self.target_data.size-1, self.target_data.size),\n",
    "                 self.target_data,\n",
    "                 label='Original data')\n",
    "        plt.plot(self.testYind, self.testPredict[:, 0], 'xr', markeredgewidth=0.8,\n",
    "                 markersize=4, markeredgecolor=[0.9, 0.2, 0.2, 0.8],\n",
    "                 label='Test predictions')\n",
    "        plt.plot(self.trainYind, self.trainPredict[:, 0], 'xg', markeredgewidth=0.8,\n",
    "                 markersize=4, markeredgecolor=[0.2, 0.7, 0.3, 0.8],\n",
    "                 label='Train predictions')\n",
    "        plt.legend()\n",
    "        plt.title(title)\n",
    "        if crop_min is not None and crop_max is not None:\n",
    "            plt.xlim(crop_min, crop_max)\n",
    "        plt.show()\n",
    "        \n",
    "        return\n",
    "\n",
    "    \n",
    "    def plot_predict_all_steps(self, crop_min=None, crop_max=None):\n",
    "        \"\"\"\n",
    "        Plot predictions from test and training samples against original data,\n",
    "        for all timesteps of prediction (i.e. with lookahead)\n",
    "        x-axis (time) can be cropped to between 'crop_min' and 'crop_max' to show\n",
    "        a particular time range if desired.\n",
    "        \n",
    "        \"\"\"        \n",
    "        plt.figure(figsize=(9, 4))\n",
    "        plt.plot(np.linspace(0, self.target_data.size-1, self.target_data.size),\n",
    "                 self.target_data,\n",
    "                 label='Original data')\n",
    "        for n in range(len(self.testYind)):\n",
    "            if n == 0:\n",
    "                plt.plot(np.arange(self.testYind[n], self.testYind[n] + self.predict_steps),\n",
    "                         self.testPredict[n, :], '-r', lw=1, color=[0.9, 0.2, 0.2, 0.8], label='Test predictions')\n",
    "            else:\n",
    "                plt.plot(np.arange(self.testYind[n], self.testYind[n] + self.predict_steps),\n",
    "                         self.testPredict[n, :], '-r', lw=1, color=[0.9, 0.2, 0.2, 0.8])\n",
    "        for n in range(len(self.trainYind)):\n",
    "            if n == 0:\n",
    "                plt.plot(np.arange(self.trainYind[n], self.trainYind[n] + self.predict_steps),\n",
    "                         self.trainPredict[n, :], '-g', lw=1, color=[0.2, 0.7, 0.3, 0.8], label='Train predictions')\n",
    "            else:\n",
    "                plt.plot(np.arange(self.trainYind[n], self.trainYind[n] + self.predict_steps),\n",
    "                         self.trainPredict[n, :], '-g', lw=1, color=[0.2, 0.7, 0.3, 0.8])\n",
    "        plt.legend()\n",
    "        plt.title(\"Multi-step prediction compared to original dataset\")\n",
    "        if crop_min is not None and crop_max is not None:\n",
    "            plt.xlim(crop_min, crop_max)\n",
    "            plt.text(0.01, 0.98, 'Note: graph shows only part of the predicted time series',\n",
    "                transform=plt.gca().transAxes, va='top', ha='left', color=[0.3, 0.3, 0.3])\n",
    "        plt.show()\n",
    "        \n",
    "        return\n",
    "    \n",
    "        \n",
    "    def plot_weights(self, feature_names):\n",
    "        \"\"\"\n",
    "        Plot weights matrix\n",
    "        \n",
    "        \"\"\"\n",
    "        layer_1_mean = np.mean(self.weights[0], axis=1)\n",
    "        if self.use_target:\n",
    "            layer_1_mean = layer_1_mean.reshape((self.look_back, self.input_data.shape[0] + 1))\n",
    "        else:\n",
    "            layer_1_mean = layer_1_mean.reshape((self.look_back, self.input_data.shape[0]))\n",
    "            \n",
    "        plt.figure()\n",
    "        ax = plt.subplot(111)\n",
    "        im = ax.imshow(layer_1_mean.T)\n",
    "        ax.set_yticks(np.arange(len(feature_names)))\n",
    "        ax.set_xticks(np.arange(self.look_back))\n",
    "        ax.set_yticklabels(feature_names)\n",
    "        ax.set_xticklabels(np.linspace(-self.look_back, -1, self.look_back).astype(np.int))\n",
    "        ax.xaxis.set_ticks_position('bottom')\n",
    "        plt.setp(ax.get_xticklabels()[::2], visible=False)\n",
    "        plt.setp(ax.get_yticklabels(), ha=\"right\")\n",
    "        ax.set_title(\"First model layer: average weights\")\n",
    "        ax.set_xlabel('Input sample timestep before prediction [days]')\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes(\"right\", \"5%\", pad=\"5%\")\n",
    "        plt.colorbar(im, cax=cax)\n",
    "        plt.show()\n",
    "        \n",
    "        return\n",
    "    \n",
    "\n",
    "def compile_model_input_variables(model_rain_ind,\n",
    "                                  model_vol_ind,\n",
    "                                  model_temp_ind,\n",
    "                                  model_hydro_ind,\n",
    "                                  rain_pp,\n",
    "                                  vol_pp,\n",
    "                                  temp_pp,\n",
    "                                  hydro_pp,\n",
    "                                  num_timesteps):\n",
    "    \"\"\"\n",
    "    Extract the selected variables from the preprocessed data by their indices\n",
    "    and populate an input array to be used by the model.\n",
    "    The input array will have shape: (num features, num timesteps)\n",
    "    \n",
    "    \"\"\"    \n",
    "    num_inputs = len(model_rain_ind) + len(model_vol_ind) + \\\n",
    "                 len(model_temp_ind) + len(model_hydro_ind)\n",
    "    model_input_data = np.empty((num_inputs, num_timesteps))\n",
    "    count = 0\n",
    "    for i in model_rain_ind:\n",
    "        model_input_data[count, :] = rain_pp[i]\n",
    "        count += 1\n",
    "    for i in model_vol_ind:\n",
    "        model_input_data[count, :] = vol_pp[i]\n",
    "        count += 1\n",
    "    for i in model_temp_ind:\n",
    "        model_input_data[count, :] = temp_pp[i]\n",
    "        count += 1\n",
    "    for i in model_hydro_ind:\n",
    "        print()\n",
    "        model_input_data[count, :] = hydro_pp[i]\n",
    "        count += 1\n",
    "    \n",
    "    return model_input_data\n",
    "\n",
    "\n",
    "def create_dataset(dataset, look_back=1, chunk_step=1, predict_steps=1, use_target=True):\n",
    "    \"\"\"\n",
    "    Convert data set into a dataset matrix by splitting the data into a number of 'chunks'\n",
    "    that are randomly ordered. Each chunk has an input (x) of length 'look_back', and an\n",
    "    output (y) of length 'predict_steps'. The indices of each chunk within the original array\n",
    "    are stored in 'y_ind'.\n",
    "    The target should be the first feature in the dataset.\n",
    "    If 'use_target' is True, the target will be used as an input (i.e. will be part of x),\n",
    "    otherwise it will be excluded from x and only used in y.\n",
    "    The shape of the input and output arrays is as follows:\n",
    "    dataset: (num features, num timesteps)\n",
    "    x:       (num chunks, look_back, num features)\n",
    "    y:       (num chunks, predict_steps)\n",
    "    y_ind:   (num chunks)\n",
    "    \n",
    "    \"\"\"\n",
    "    numchunk = int(np.floor((dataset.shape[1] - look_back - predict_steps - 1) / chunk_step))\n",
    "    if use_target:\n",
    "        dataX = np.empty((numchunk, look_back, dataset.shape[0]))\n",
    "    else:\n",
    "        if dataset.shape[0] == 1:\n",
    "            print('No input data in dataset, try adding features or setting use_target=True')\n",
    "        dataX = np.empty((numchunk, look_back, dataset.shape[0] - 1))\n",
    "    dataY = np.empty((numchunk, predict_steps))\n",
    "    y_ind = []\n",
    "    \n",
    "    # Create chunks of data with the specified look back\n",
    "    for i in range(numchunk):\n",
    "        start_ind = chunk_step*i\n",
    "        if use_target:\n",
    "            dataX[i, :, :] = dataset[:, start_ind:(start_ind + look_back)].T\n",
    "        else:\n",
    "            dataX[i, :, :] = dataset[1:, start_ind:(start_ind + look_back)].T\n",
    "        dataY[i, :] = dataset[0, start_ind+look_back:start_ind+look_back+predict_steps] #MG\n",
    "        #dataY.append(dataset[0, start_ind + look_back])\n",
    "        y_ind.append(start_ind + look_back)\n",
    "        \n",
    "    # Randomise order of chunks\n",
    "    rand_indices = np.random.permutation(numchunk)\n",
    "    x = np.array(dataX)\n",
    "    y = np.array(dataY)\n",
    "    y_ind = np.array(y_ind)\n",
    "    x = x[rand_indices, :]\n",
    "    y = y[rand_indices,:]\n",
    "    #y = np.reshape(y, (y.size, 1)) # MG\n",
    "    y_ind = y_ind[rand_indices]\n",
    "    \n",
    "    return x, y, y_ind\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Base analysis: Aquifier Auser\n",
    "## 2.1 Data pre-processing\n",
    "### 2.1.1 Handling missing data\n",
    "\n",
    "First, the data was read in and an analysis of missing data was carried out. The plot below shows that there is a significant amount of data missing over the total time period, either in the form of NaNs, or zeros. In the case of the rainfall data, the zeros are considered to be 'real data' since there are many days with zero rainfall, however for the other data it is assumed that zeros imply missing data.\n",
    "\n",
    "When deciding how to handle these gaps in the data, the aim was to find a compromise between maintaining as much of the original data as possible, and not introducing too much inaccuracy from filling in the gaps. For each variable, we consider a combination of three different options for handling these gaps:\n",
    "1. Crop the entire dataset over a certain time range to avoid the gaps. If many of the variables contain missing data for the same time period, then this option is sensible as this time period is unlikely to be useful for the model.\n",
    "2. Remove the variable completely. This may be necessary if there is missing data for large time periods, particularly if many other variables contain data in those time periods. For example, 'Volume_CSA' and 'Volume CSAL' both contain large gaps in the data where there is data for most other variables (from days ~3000-6000, so it makes sense to remove these from the analysis.\n",
    "3. Fill in the missing data for that variable by prediction (e.g. interpolation, backpropagation). Where the gaps in the data are relatively short and make up a small proportion of the total data, this option is likely to be preferable.\n",
    "\n",
    "Since there are NaNs in most of the data for the first ~3000 days, including all of the target variables, it was decided that this time period would not be used for the model at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Pre-processing data\n",
    "\n",
    "# Importing all data into pandas dataframe\n",
    "foldpath = r\"acea-water-prediction\"\n",
    "files = sorted(list(Path(foldpath).rglob('*.csv')))\n",
    "df = pd.read_csv(files[0])\n",
    "\n",
    "# Find and plot gaps in the data\n",
    "all_data, col_names, is_nan, is_zero = find_data_gaps(df, plot=True, title=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find column indices for different datatypes\n",
    "datatypes, col_inds = find_datatypes(df)\n",
    "\n",
    "# Get time series data for all variable types\n",
    "start_ind = 3955\n",
    "rain_ts, rain_name = get_time_series(col_inds[0], start_ind=start_ind, fill_zeros=False)\n",
    "target_ts, target_name = get_time_series(col_inds[1], start_ind=start_ind)\n",
    "temp_ts, temp_name = get_time_series(col_inds[2], start_ind=start_ind)\n",
    "vol_ts, vol_name = get_time_series(col_inds[3], start_ind=start_ind)\n",
    "hydro_ts, hydro_name = get_time_series(col_inds[4], start_ind=start_ind)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove zeros from target\n",
    "# N.B. SHOULD REPLACE THIS WITH A DIFFERENT METHOD\n",
    "for m in range(len(target_ts)):\n",
    "    for n in range(len(target_ts[m])):\n",
    "        if n > 0 and target_ts[m][n] == 0:\n",
    "            target_ts[m][n] = target_ts[m][n-1]\n",
    "\n",
    "# Extend target with 2 values missing from the end\n",
    "# to_append = target_ts[1][-1]*np.ones(2)\n",
    "# target_ts[1] = np.append(target_ts[1], target_ts[1][-1]*np.ones(2), 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Rainfall analysis\n",
    "\n",
    "Rainfall is expected to be one of the most useful variables in predicting groundwater levels, since it is one the key physical mechanisms that drives the inflow of water into aquifiers. From a visual inspection of the data, it is clear that many of the peaks in rainfall correspond approximately to peaks in the groundwater level. This applies both to the longer term trend, as well as on a daily time scale. However, while the peaks in rainfall are often immediately followed by days of zero rainfall, the peaks in the groundwater data seem to decay more slowly over time.\n",
    "\n",
    "This leads to the hypothesis that rainfall acts a short term impulse to the groudwater level, raising its level relatively quickly over a few days. When the rainfall quietens down, the groundwater level subsequently decays over a slower timescale of several weeks. \n",
    "\n",
    "The physical explanation for this seems to be logical - if the rate of the rainfall is significantly larger than the rate over which the water drains out of the groundwater subsystem, then the level will rise quickly and fall slowly (similar to the level of a slowly leaking cup when water is poured into it). \n",
    "\n",
    "Therefore, treating the rainfall as a series of 'impulse forcings', one can obtain the implied groundwater 'response' by convolving the rainfall timescales with an assumed impulse response $G(t)$. For simplicity, we choose $G(t)$ to be exponential decay, characterized by an e-folding timescale $\\tau$:\n",
    "\n",
    "$ G(t) = A e^{- t/\\tau} $, \n",
    " \n",
    "where the time scale $\\tau$ encapsulates the characterstic time for groundwater level to drain out of the aquifier following a burst of rainfall, and $A$ is a proportionality constant. In a statistical sense, this operation is equivalent to applying a smoothing filter to the rainfall data, using an exponentially decaying window in time.\n",
    "\n",
    "The plot below shows how applying this convolution (with $\\tau$ = 54 days) transforms the spiky rainfall dataset into a smoother function that bears much more resemblence to the groundwater level evolution.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Rainfall data analysis\n",
    "\n",
    "r_i = 0\n",
    "t_i = 1\n",
    "rain_pp = exp_convolve(rain_ts[r_i], tau=54)\n",
    "plot_preprocessing(rain_ts[r_i], rain_pp, rain_name[r_i], target_ts[t_i], target_name[t_i],\n",
    "                   crop_min=1600, crop_max=2000, title=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the optimal value of $\\tau$ can reveal important information about the drainage mechanism for a particular subsystem, as well as help predict its groundwater level. \n",
    "\n",
    "To do this, we apply the convolution for a range of $\\tau$ values and evaluate the correlation with the target as a function of $\\tau$. The plots below show that the correlation peaks at around 50 days for SAL, whereas it peaks at ~ 800-1000 days for LT2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot example of correlation for different tau values\n",
    "\n",
    "tau_best, src_best = find_best_tau(target_ts=target_ts[1], rain_ts=rain_ts,\n",
    "                         rain_name=rain_name, plot=True,\n",
    "                         target_name=target_name[1],\n",
    "                         tau_array=np.linspace(2, 160, 80))\n",
    "\n",
    "tau_best, src_best = find_best_tau(target_ts=target_ts[0], rain_ts=rain_ts, plot=True,\n",
    "                         target_name=target_name[0],\n",
    "                         rain_name=rain_name,\n",
    "                         tau_array=np.linspace(20, 1600, 80))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The large time scales associated with LT2 are likely due to the fact that this particular source is located significantly deeper than the rest (see plot below).\n",
    "\n",
    "This fits with our understanding that shallower subsystems should experience more short term reactions to rainfall, as it likely reaches the subsystem more quickly and also drain out faster. Conversely for deeper subsystems, we expect the levels to react more slowly, with smaller short term effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 4))\n",
    "for n in range(len(target_ts)):\n",
    "    plt.plot(target_ts[n], label=target_name[n])\n",
    "#plt.xlim(4100, 4300)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below summarizes the optimal values of $\\tau$ found for each rainfall/target combination. These time scales range between 46 - 120 days for the shallow subsystems, and about 640 - 1200 days for the deep one (LT2). \n",
    "\n",
    "The color shading shows the correlation coefficient in each case, ranging roughly between 0.3 to 0.9. \n",
    "\n",
    "Note that 'Rainfall_Piaggione' produces unphsyical results, which are likely due to missing values. It was hence not considered in subsequent analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating optimum tau for each rainfall/target combination that\n",
    "# maximises the Spearman's Rank correlation coefficient\n",
    "\n",
    "rain_tau = find_all_best_tau(rain_ts, rain_name, target_ts, target_name,\n",
    "                             target_0_tau_array=np.linspace(1, 2401, 80))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'Rainfall_Piaggione' from pre-processed data list due to missing values\n",
    "rain_filt = rain_ts.copy()\n",
    "rain_name_pp = rain_name.copy()\n",
    "del rain_filt[5]\n",
    "del rain_name_pp[5]\n",
    "\n",
    "# Create 3 different sets of pre-processed data\n",
    "#   - rain_pp_shallow is a list of size (num selected rain), where each element is a time series array. This\n",
    "#     array is just for the cross-correlation anaysis, so tau value is an average value of the best tau values for\n",
    "#     the shallow groundwater variables (all but LT2).\n",
    "#   - rain_pp_deep is a list of size (num selected rain), where each element is a time series array. This\n",
    "#     array is just for the cross-correlation anaysis, so tau value is the best tau values for the deep\n",
    "#     groundwater variable (LT2).\n",
    "#   - rain_pp_all is a list of size (num target variables, num selected hydro), where each element is a time\n",
    "#     series array. Each array has the preprocessed data with the optimum tau value for that particular\n",
    "#     combination of hydro/target. This will be used for the model.\n",
    "\n",
    "# Rainfall data preprocessing\n",
    "rain_pp_shallow = []\n",
    "rain_pp_deep = []\n",
    "rain_pp_all = []\n",
    "for n in range(len(rain_filt)):\n",
    "    tau_shallow = np.mean(rain_tau[1:, n])\n",
    "    tau_deep = np.mean(rain_tau[0, n])\n",
    "    rain_pp_shallow.append(exp_convolve(rain_filt[n], tau_shallow))\n",
    "    rain_pp_deep.append(exp_convolve(rain_filt[n], tau_deep))\n",
    "    rain_pp_ = []\n",
    "    for m in range(len(target_ts)):\n",
    "        tau_ = rain_tau[m, n]\n",
    "        rain_pp_.append(exp_convolve(rain_filt[n], tau_))\n",
    "    rain_pp_all.append(rain_pp_)\n",
    "\n",
    "# Flip the dimensions of the list\n",
    "rain_pp_all = [list(x) for x in zip(*rain_pp_all)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Volume data analysis \n",
    "Initial inspection of the 'volume' data showed the following:\n",
    "- There is a relatively strong weekly pattern, and typically step changes every month. These are presumably linked to human activity/demand, and similar patterns are not clearly visible in the 'depth to groundwater' target variables. This suggests that it may be helpful to apply some smoothing to the volume data prior to modelling, in order to reduce the effect of the weekly/monthly variation.\n",
    "- There are some very long timescale trends in the amplitude, for example both Volume_CC1 and Volume_CC2 both increase gradually over the whole time period of the data. This timescale of change (perhaps linked to human activity/demand changes) is not present in the depth to groundwater variables, so there may be some benefit in removing it from the volume data prior to modelling.\n",
    "\n",
    "In order to try to make the volume data more useful for modelling depth to groundwater, the following pre-processing was proposed based on the findings above:\n",
    "- Caclulate the deviation of the volume data from a smoothed version of the data. The smoothed version of the data should be created with a relatively long smoothing window so that it represents the long term trend of the volume data.\n",
    "- Apply some smoothing to the deviation calculated above, with a relatively short smoothing window, in order to reduce the effect of the weekly/monthly variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all volume data - for reference, not for final notebook\n",
    "\n",
    "# plt.figure()\n",
    "# for n in range(len(vol_ts)):\n",
    "#     plt.plot(vol_ts[n], label=vol_name[n])\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Volume pre-processing\n",
    "\n",
    "# find_best_vol_params(vol_ts[0], np.sum(target_ts, 0), smoothing_1=None, smoothing_2=None, plot=True)\n",
    "# find_best_vol_params(vol_ts[0], np.sum(target_ts, 0), smoothing_1=None, smoothing_2=None, plot=True)\n",
    "# find_best_vol_params(vol_ts[1], np.sum(target_ts, 0), smoothing_1=None, smoothing_2=None, plot=True)\n",
    "# find_best_vol_params(vol_ts[2], np.sum(target_ts, 0), smoothing_1=None, smoothing_2=None, plot=True)\n",
    "\n",
    "vol_pp = []\n",
    "for n in range(len(vol_ts)):\n",
    "    vol_pp.append(preprocess_vol(vol_ts[n]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparing original and pre-processed volume data\n",
    "\n",
    "v_i = 0\n",
    "t_i = 2\n",
    "plot_preprocessing(vol_ts[v_i], vol_pp[v_i], vol_name[v_i], target_ts[t_i], target_name[t_i],\n",
    "                   crop_min=2300, crop_max=2800, title=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4. Temperature data analysis\n",
    "All of the temperature variables appear to be reasonably well correlated to each other, showing similar seasonal variation.\n",
    "However, 'Temperature_Lucca_Orto_Botanico' has long periods of missing data, including the most recent ~1000 days, so it will not be used for the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot all temp data - for reference, not for final notebook\n",
    "# plt.figure(figsize=(9, 4))\n",
    "# for n in range(len(temp_ts)):\n",
    "#     plt.plot(temp_ts[n], label=temp_name[n])\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 4))\n",
    "plt.plot(normalise_0_to_1(-temp_ts[0]), label=temp_name[0])\n",
    "plt.plot(normalise_0_to_1(target_ts[1]), label=target_name[1])\n",
    "# plt.xlim(2000, 2500)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plt.figure(figsize=(9, 4))\n",
    "# plt.plot(normalise_0_to_1(-temp_ts[0]), label=temp_name[0])\n",
    "# plt.plot(normalise_0_to_1(target_ts[1]), label=target_name[1])\n",
    "# plt.xlim(2000, 2500)\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove 'Temperature_Lucca_Orto_Botanico' from pre-processed data list\n",
    "temp_pp = temp_ts.copy()\n",
    "temp_name_pp = temp_name.copy()\n",
    "del temp_pp[2]\n",
    "del temp_name_pp[2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5 Hydrometry data analysis\n",
    "The hydrometry data displays similar overall trend and with peaks in similar locations to the target variables. Peaks in the hydrometry data tend to have a relatively steep 'ramp up', which matches the target variables well, however the 'ramp down' is  and a slightly less steep ramp down.\n",
    "\n",
    "The same $\\tau$ convolution operation was applied to the hydrometry data as for the rainfall (see Section 2.1.2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot all hydrometry data - for reference, not for final notebook\n",
    "\n",
    "# plt.figure(figsize=(9, 4))\n",
    "# for n in range(len(hydro_ts)):\n",
    "#     plt.plot(hydro_ts[n], label=hydro_name[n])\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot showing comparison of raw hydrometry data with pre-processed (smoothed) data\n",
    "\n",
    "h_i = 0\n",
    "t_i = 1\n",
    "hydro_pp = exp_convolve(hydro_ts[h_i], tau=7, winlength=None)\n",
    "plot_preprocessing(hydro_ts[h_i], hydro_pp, hydro_name[h_i], target_ts[t_i], target_name[t_i],\n",
    "                   crop_min=1700, crop_max=2200, title=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot below summarizes the optimal $\\tau$ values for each hydrometry/target combination. These values range between 8 - 28 days for the shallow subsystems, and around 800 days for the deep one (LT2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calculating optimum tau for each hydrometry/target combination that\n",
    "# maximises the Spearman's Rank correlation coefficient\n",
    "\n",
    "hydro_tau = find_all_best_tau(hydro_ts, hydro_name, target_ts, target_name,\n",
    "                             target_0_tau_array=np.linspace(1, 2401, 80))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hydrometry data preprocessing\n",
    "\n",
    "# Remove Piaggione from data list due to missing values\n",
    "hydro_filt = hydro_ts.copy()\n",
    "hydro_name_pp = hydro_name.copy()\n",
    "if len(hydro_filt) == len(hydro_ts):\n",
    "    del hydro_filt[1]\n",
    "    del hydro_name_pp[1]\n",
    "\n",
    "# Create 3 different sets of pre-processed data\n",
    "#   - hydro_pp_shallow is a list of size (num selected hydro = 1), where each element is a time series array. This\n",
    "#     array is just for the cross-correlation anaysis, so tau value is an average value of the best tau values for\n",
    "#     the shallow groundwater variables (all but LT2).\n",
    "#   - hydro_pp_deep is a list of size (num selected hydro = 1), where each element is a time series array. This\n",
    "#     array is just for the cross-correlation anaysis, so tau value is the best tau values for the deep\n",
    "#     groundwater variable (LT2).\n",
    "#   - hydro_pp_all is a list of size (num target variables, num selected hydro), where each element is a time\n",
    "#     series array. Each array has the preprocessed data with the optimum tau value for that particular\n",
    "#     combination of hydro/target. This will be used for the model.\n",
    "\n",
    "hydro_pp_shallow = []\n",
    "hydro_pp_deep = []\n",
    "hydro_pp_all = []\n",
    "for n in range(len(hydro_filt)):\n",
    "    tau_shallow = np.mean(hydro_tau[1:, n])\n",
    "    tau_deep = np.mean(hydro_tau[0, n])\n",
    "    hydro_pp_shallow.append(exp_convolve(hydro_filt[n], tau_shallow))\n",
    "    hydro_pp_deep.append(exp_convolve(hydro_filt[n], tau_deep))\n",
    "    hydro_pp_ = []\n",
    "    for m in range(len(target_ts)):\n",
    "        tau_ = hydro_tau[m, n]\n",
    "        hydro_pp_.append(exp_convolve(hydro_filt[n], tau_))\n",
    "    hydro_pp_all.append(hydro_pp_)\n",
    "\n",
    "# Flip the dimensions of the list\n",
    "hydro_pp_all = [list(x) for x in zip(*hydro_pp_all)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.6 Lag analysis\n",
    "\n",
    "We were also interested in the correlation between groundwater level and rainfall, with different amounts of lag in the rainfall data. The line plot below shows results for the (deep) LT2 target, and reveals that the peak correlation arises with a lag of 50-100 days. This again shows how long it takes for rainfall to penetrate deep in the subsystem and affect the water level.\n",
    "\n",
    "As illustrated in the second (contour) plot below, there is an optimal combination of $\\tau$ and lag value that gives the best correlation for LT2. However, we find that adding lag did not have much effect for the shallower subsystems. So for simplicity and consistency, we pre-process all the rainfall data with $\\tau$ convolution only and no lag. The lag processing could be explored further to improve accuracy of the model, particularly for the LT2 target where the optimal lag is relatively long compared to the look-back of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Analysis of how lag applied to rainfall affects correlation with target\n",
    "\n",
    "lag_array = np.arange(200)\n",
    "plt.figure()\n",
    "for n in range(len(rain_filt)):\n",
    "    crosscorr_lag, lag_array = cross_corr_lag(\n",
    "                                    normalise_0_to_1(target_ts[0]),\n",
    "                                    normalise_0_to_1(rain_filt[n]),\n",
    "                                    lag_array=lag_array)\n",
    "    plt.plot(lag_array, crosscorr_lag, label=rain_name_pp[n])\n",
    "plt.legend()\n",
    "plt.xlabel('Lag applied to rainfall data [days]')\n",
    "plt.ylabel('Spearmans rank correlation coefficient')\n",
    "plt.title('Correlation of rainfall with target for different lag amounts')\n",
    "\n",
    "\n",
    "target_test_ind = 0\n",
    "rain_test_ind = 0\n",
    "# lag_array = np.arange(30)\n",
    "# tau_array = np.linspace(12, 90, 40).astype(np.int)\n",
    "lag_array = np.linspace(0, 60, 31).astype(np.int)\n",
    "tau_array = np.linspace(40, 2000, 50).astype(np.int)\n",
    "data1 = target_ts[target_test_ind]\n",
    "data2 = rain_ts[rain_test_ind]\n",
    "sp_rank_cc, lag_array, tau_array = tau_and_lag_correl(data1,\n",
    "                                                      data2,\n",
    "                                                      lag_array=lag_array,\n",
    "                                                      tau_array=tau_array,\n",
    "                                                      plot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Correlation analysis\n",
    "\n",
    "The plots below are correlation matrices between the pre-processed input features and the raw target data, calculated day-by-day. The top panel is designed for deep groundwater level (LT2), such that the pre-processing of the rainfall and hydrometry uses large $\\tau$ values (> 100 days). On the other hand, the bottom plot uses smaller $\\tau$ values (~ 50 days). \n",
    "\n",
    "Note that rainfall and hydrometry are pre-processed, whereas the other variables are essentially used raw.\n",
    "\n",
    "The matrices reveal the following information: \n",
    "- Most of the rainfall datasets are positively correlated \n",
    "- Most of the temperature datasets are positively correlated \n",
    "- Hydrometry and rainfall are positively correlated\n",
    "- The temperature and rainfall datasets are negatively correlated (when using low $\\tau$ values)\n",
    "- Depth to Groundwater is well correlated with rainfall\n",
    "- Depth to Groundwater is well correlated with temperature (for all targets except LT2)\n",
    "- Most of the volume data is not well correlated, neither to each other nor to targets. This is true except for Volume_POL, which has correlation with LT2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Since many of the variables seem to show a high correlation with each other, using all of them to train the model is likely to add unnecessary computation time, without improvement over a model trained with a smaller selection of variables. Therefore, some down-selection of variables was carried out prior to modelling, in order to make the machine learning model as computationally efficient as possible. This was done by calculating the cross-correlation of the pre-processed data, and evaluating the expected usefulness of each variable based on the following principles:\n",
    "# 1. If input variables have a strong correlation to the target variable, they are likely to be useful in training the model.\n",
    "# 2. If input variables have a strong correlation with other input variables, there may be limited benefit in including all of them in the model.\n",
    "# During the pre-processing of the rainfall and hydrometry data, it was found that for 'Depth_to_Groundwater_LT2', the optimum time constant (tau) for the convolution was much higher than the optimum value for the other groundwater variables. This was thought to be because the LT2 sub-system is significantly deeper underground than the other variables, and therefore it takes longer for the other variables to impact the level. In order to best evaluate the correlation of the variables, two different correlation matrices were calculated: one with variable pre-processing optimised for the shallower groundwater variables; and one with variable pre-processing optimised for the deepest groundwater variable LT2.\n",
    "\n",
    "# Plot correlation matrices\n",
    "\n",
    "# Combine all pre-processed data for the shallow groundwater variables\n",
    "data_pp_shallow = np.concatenate((np.asarray(target_ts),\n",
    "                                  np.asarray(rain_pp_shallow),\n",
    "                                  np.asarray(vol_pp),\n",
    "                                  np.asarray(temp_pp),\n",
    "                                  np.asarray(hydro_pp_shallow)))\n",
    "name_pp_shallow = target_name + rain_name_pp + vol_name + temp_name_pp + hydro_name_pp\n",
    "\n",
    "# Combine all pre-processed data for the deep groundwater variables\n",
    "data_pp_deep = np.concatenate((np.asarray(target_ts),\n",
    "                               np.asarray(rain_pp_deep),\n",
    "                               np.asarray(vol_pp),\n",
    "                               np.asarray(temp_pp),\n",
    "                               np.asarray(hydro_pp_deep)))\n",
    "name_pp_deep = target_name + rain_name_pp + vol_name + temp_name_pp + hydro_name_pp\n",
    "\n",
    "# Plot correlation matrices\n",
    "plot_correl_matrix(data_pp_deep, name_pp_deep,\n",
    "                   title='Correlation matrix, data pre-processed for deep LT2 groundwater variable')\n",
    "plot_correl_matrix(data_pp_shallow, name_pp_shallow,\n",
    "                   title='Correlation matrix, data pre-processed for shallow groundwater variables')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Machine Learning Prediction \n",
    "\n",
    "We use a machine learning model to predict the depth to ground water given past timeseries information. We explore the following scenarios:\n",
    "1. LSTM vs. Multi-Layer-Perceptron (MLP) \n",
    "2. Including versus excluding the target timeseries as an input to the predictive model\n",
    "3. Importance of the various features in the predictive skill\n",
    "4. Daily versus weekly predictions\n",
    "5. Predicting several timesteps (days or weeks) in advance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 LSTM versus MLP models\n",
    "\n",
    "Two different machine learning models were considered for this study: LSTM (Long short term memory) and MLP (Multi-layer Perceptron). The LSTM is widely used in time series prediction tasks, while the MLP is a simpler model often used for classification/regression problems.\n",
    "\n",
    "In order to select a model for this study, an initial experiment was carried out. This involved creating both an LSTM model and an MLP model for each of the target variables (using a cut-down selection of features), and calculating the RMSE of the test predictions in each case. The results of this are plotted in the bar chart below, showing that the two model types are  similarly effective at predicting the target variables.\n",
    "\n",
    "The MLP model was chosen in this study over the LSTM model for the following reasons:\n",
    "- The layer activation weights are easier to interpret, which gives us some insight into what features the model appears to be favoring, and therefore oppurtunity for a deeper physical understanding of the system.\n",
    "- The model requires significantly less computation time (at least for the parameters that were used in this study). This efficiency makes it more convenient to run the model multiple times, for example to assess the importance of different features or feature combinations (as in section 2.3.3).\n",
    "- The MLP model is inherently a less complex model than the LSTM, and it is generally good practise to use the simplest solution to a problem (Occam's razor).\n",
    "\n",
    "The LSTM is designed to be able to efficiently 'remember' important features from past timesteps, and is therefore suited to problems where useful data is found many timesteps before the prediction (e.g. NLP problems). It is possible that for this dataset, where the lookback is not large and the effects of external variables are relatively short term, the potential benefits of the LSTM are not utilised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose variables to include in model by their indices\n",
    "target_ind = 0\n",
    "model_rain_ind = [0, 2, 4, 6, 8]\n",
    "model_vol_ind = [0, 2]\n",
    "model_temp_ind = [0, 2]\n",
    "model_hydro_ind = [0]\n",
    "\n",
    "# Getting the names of the selected features\n",
    "a1 = [rain_name[i] for i in model_rain_ind]\n",
    "a2 = [vol_name[i] for i in model_vol_ind]\n",
    "a3 = [temp_name[i] for i in model_temp_ind]\n",
    "a4 = [hydro_name[i] for i in model_hydro_ind]\n",
    "input_names = ['All features included', target_name[target_ind]] + a1 + a2 + a3 + a4\n",
    "\n",
    "\n",
    "\n",
    "# Running LSTM a total of tot_idx times, each time removing one of the input features\n",
    "num_target = len(target_ts)\n",
    "all_scores_LSTM = np.empty(num_target)\n",
    "all_scores_MLP = np.empty(num_target)\n",
    "\n",
    "for i in range(num_target):\n",
    "    \n",
    "    target_data = target_ts[i]\n",
    "    model_input_data = compile_model_input_variables(model_rain_ind,\n",
    "                                                     model_vol_ind,\n",
    "                                                     model_temp_ind,\n",
    "                                                     model_hydro_ind,\n",
    "                                                     rain_pp_all[i],\n",
    "                                                     vol_pp,\n",
    "                                                     temp_pp,\n",
    "                                                     hydro_pp_all[i],\n",
    "                                                     target_ts[i].size)\n",
    "    \n",
    "    model_tmp = ML_model(target_data=target_data,\n",
    "                                    input_data=model_input_data,\n",
    "                                    use_target=False,\n",
    "                                    look_back=30,\n",
    "                                    chunk_step=10,\n",
    "                                    train_ratio=0.67,\n",
    "                                    num_epochs=40,\n",
    "                                    batch_size=5,\n",
    "                                    predict_steps=1,\n",
    "                                    plot=False,\n",
    "                                    verbose=0)\n",
    "    model_tmp.run(model_type='LSTM')\n",
    "    all_scores_LSTM[i] = (model_tmp.trainRMSE + model_tmp.testRMSE)/2\n",
    "    model_tmp.run(model_type='MLP')\n",
    "    all_scores_MLP[i] = (model_tmp.trainRMSE + model_tmp.testRMSE)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# Plotting bar chart\n",
    "plt.figure(figsize=(7, 3))\n",
    "ax = plt.subplot(111)\n",
    "x_pos = np.arange(0, num_target)\n",
    "plt.bar(x_pos-0.2, all_scores_MLP, color=[0.3, 0.3, 0.7], width = 0.39, label='MLP')\n",
    "plt.bar(x_pos+0.2, all_scores_LSTM, color=[0.5, 0.7, 0.2], width = 0.39, label='LSTM')\n",
    "plt.ylabel(\"RMSE score\")\n",
    "plt.title(\"Prediction error by model type\")\n",
    "plt.xticks(x_pos, target_name, rotation=40, ha=\"right\",\n",
    "                 rotation_mode=\"anchor\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Including versus excluding the target timeseries as a model input\n",
    "\n",
    "The line plots below show the timeseries prediction of the model, with and without the inclusion of past target data as an input, respectively. Note that other targets are excluded from the inputs in all cases. As expected, the model performs better when the target data is included, but it still has reasonable predictive skill without it.\n",
    "\n",
    "The color plots show the degree of activation of the weights associated with each input variable in the MLP model, as a function of the steps back considered. In this example, we feed the model t-30 days of past data to predict the target at time t. Those plots reveal the following:\n",
    "- When included, the target has the largest activation, consistent with it being useful in the prediction.\n",
    "- Rainfall and hydrometry have the largests weights when the target is excluded\n",
    "- The most recent days have significantly larger weights than past ones, suggesting that the model may not need to look so far into the past to produce a reasonable prediction. This idea should however be tested more thoroughly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# LSTM with or without target data\n",
    "\n",
    "\n",
    "# Including the target data \n",
    "\n",
    "# Choose variables to include in model by their indices\n",
    "target_ind = 1\n",
    "model_rain_ind = [1, 3, 4, 7, 8]\n",
    "model_vol_ind = [0, 1, 2]\n",
    "model_temp_ind = [0, 1, 2]\n",
    "model_hydro_ind = [0]\n",
    "\n",
    "# Getting the names of the selected features\n",
    "a1 = [rain_name[i] for i in model_rain_ind]\n",
    "a2 = [vol_name[i] for i in model_vol_ind]\n",
    "a3 = [temp_name[i] for i in model_temp_ind]\n",
    "a4 = [hydro_name[i] for i in model_hydro_ind]\n",
    "input_names = [target_name[target_ind]] + a1 + a2 + a3 + a4\n",
    "\n",
    "# Compile target data and inputs to arrays\n",
    "target_data = target_ts[target_ind]\n",
    "model_input_data = compile_model_input_variables(model_rain_ind,\n",
    "                                                 model_vol_ind,\n",
    "                                                 model_temp_ind,\n",
    "                                                 model_hydro_ind,\n",
    "                                                 rain_pp_all[target_ind],\n",
    "                                                 vol_pp,\n",
    "                                                 temp_pp,\n",
    "                                                 hydro_pp_all[target_ind],\n",
    "                                                 target_ts[target_ind].size)\n",
    "\n",
    "# Create ML model class and run it\n",
    "Model_using_target = ML_model(target_data=target_data,\n",
    "                                input_data=model_input_data,\n",
    "                                use_target=True,\n",
    "                                look_back=30,\n",
    "                                chunk_step=3,\n",
    "                                train_ratio=0.67,\n",
    "                                num_epochs=50,\n",
    "                                batch_size=5,\n",
    "                                predict_steps=5,\n",
    "                                plot=False,\n",
    "                                verbose=0)\n",
    "Model_using_target.run(model_type='MLP')\n",
    "\n",
    "# Excluding the target data\n",
    "Model_excluding_target = ML_model(target_data=target_data,\n",
    "                                input_data=model_input_data,\n",
    "                                use_target=False,\n",
    "                                look_back=30,\n",
    "                                chunk_step=3,\n",
    "                                train_ratio=0.67,\n",
    "                                num_epochs=70,\n",
    "                                batch_size=5,\n",
    "                                predict_steps=5,\n",
    "                                plot=False,\n",
    "                                verbose=0)\n",
    "Model_excluding_target.run(model_type='MLP')\n",
    "\n",
    "\n",
    "Model_using_target.plot_predict_one_step(title=\"Including the target data (MLP)\")\n",
    "print('Train Score: %.2f RMSE, %.2f MSE' % (Model_using_target.trainRMSE[0], Model_using_target.trainMSE[0]))\n",
    "print('Test Score:  %.2f RMSE, %.2f MSE' % (Model_using_target.testRMSE[0], Model_using_target.testMSE[0]))\n",
    "Model_using_target.plot_weights(input_names)\n",
    "\n",
    "Model_excluding_target.plot_predict_one_step(title=\"Excluding the target data (MLP)\")\n",
    "print('Train Score: %.2f RMSE, %.2f MSE' % (Model_excluding_target.trainRMSE[0], Model_excluding_target.trainMSE[0]))\n",
    "print('Test Score:  %.2f RMSE, %.2f MSE' % (Model_excluding_target.testRMSE[0], Model_excluding_target.testMSE[0]))\n",
    "Model_excluding_target.plot_weights(input_names[1:])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Assessing variable importance\n",
    "In order to quantify how important different variables are in terms of model accuracy, the following method was used:\n",
    "1. A multi-layer perceptron model was trained using only one input variable to predict the target one day ahead.\n",
    "2. The RMSE of the predictions made using this model were calculated.\n",
    "3. The above steps were repeated for each of the variables in the dataset, as well as for a model with all features included.\n",
    "4. The RMSE scores were compared to assess which variables were most effective at modelling the target.\n",
    "\n",
    "The bar plots below highlight the importance of the target, rainfall and hydrometry in the MLP's predictive skill. While the exact order depends on each target individually, these results are generally consistent with the correlation matrices shown in Section 2.2. In the case of LT2, it is additionally interesting to note that several rainfall and hydrometry datasets are more useful than the target itself in the prediction (although by a small margin).\n",
    "\n",
    "Another interesting result is that the RMSE of the model created using all of the features was not significantly different to the RMSE for models created with the best individual features. These results (and other experiments not shown here for brevity) strongly suggest that using a smaller number of features could still provide a similar level of predictive skill for lower computing cost. One possible explanation for this is the high levels of correlation between the target and some of the other most effective variables (with the exponential filtering applied), which points to the fact that many of the features are very similar, and adding more than one of these features adds little to the model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose variables to include in model by their indices\n",
    "target_ind = 0\n",
    "model_rain_ind = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "model_vol_ind = [0, 1, 2]\n",
    "model_temp_ind = [0, 1, 2]\n",
    "model_hydro_ind = [0]\n",
    "\n",
    "# Getting the names of the selected features\n",
    "a1 = [rain_name[i] for i in model_rain_ind]\n",
    "a2 = [vol_name[i] for i in model_vol_ind]\n",
    "a3 = [temp_name[i] for i in model_temp_ind]\n",
    "a4 = [hydro_name[i] for i in model_hydro_ind]\n",
    "input_names = ['All features included', target_name[target_ind]] + a1 + a2 + a3 + a4\n",
    "\n",
    "# Compile target data and inputs to arrays\n",
    "target_data = target_ts[target_ind]\n",
    "model_input_data_ref = compile_model_input_variables(model_rain_ind,\n",
    "                                                 model_vol_ind,\n",
    "                                                 model_temp_ind,\n",
    "                                                 model_hydro_ind,\n",
    "                                                 rain_pp_all[target_ind],\n",
    "                                                 vol_pp,\n",
    "                                                 temp_pp,\n",
    "                                                 hydro_pp_all[target_ind],\n",
    "                                                 target_ts[target_ind].size)\n",
    "\n",
    "\n",
    "# Running MLP a total of tot_idx times, each time using only one of the input features\n",
    "tot_idx = 2 + model_input_data_ref.shape[0]\n",
    "all_scores = np.empty(tot_idx)\n",
    "model_input_data = model_input_data_ref\n",
    "\n",
    "for i in range(tot_idx):\n",
    "    if i > 0:\n",
    "        model_input_data = np.expand_dims(model_input_data_ref[i-2, :], axis=0)\n",
    "        model_tmp = ML_model(target_data=target_data,\n",
    "                                        input_data=model_input_data,\n",
    "                                        use_target=False,\n",
    "                                        look_back=30,\n",
    "                                        chunk_step=20,\n",
    "                                        train_ratio=0.67,\n",
    "                                        num_epochs=30,\n",
    "                                        batch_size=5,\n",
    "                                        predict_steps=1,\n",
    "                                        plot=False,\n",
    "                                        verbose=0)\n",
    "    else:\n",
    "        model_input_data = model_input_data_ref         \n",
    "        model_tmp = ML_model(target_data=target_data,\n",
    "                                        input_data=model_input_data,\n",
    "                                        use_target=True,\n",
    "                                        look_back=30,\n",
    "                                        chunk_step=20,\n",
    "                                        train_ratio=0.67,\n",
    "                                        num_epochs=30,\n",
    "                                        batch_size=5,\n",
    "                                        predict_steps=1,\n",
    "                                        plot=False,\n",
    "                                        verbose=0)\n",
    "        model_input_data = np.expand_dims(model_input_data_ref[i-2, :], axis=0)\n",
    "        \n",
    "    model_tmp.run(model_type='MLP')\n",
    "    \n",
    "    all_scores[i] = (model_tmp.trainRMSE + model_tmp.testRMSE)/2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting ranking \n",
    "ranking = np.argsort(1*all_scores) # Sorting in ascending order\n",
    "sorted_scores = all_scores[ranking]\n",
    "sorted_names = [input_names[i] for i in ranking]\n",
    "\n",
    "# Plotting bar chart\n",
    "plt.figure(figsize=(8, 4))\n",
    "ax = plt.subplot(111)\n",
    "x_pos = np.arange(0,len(sorted_names))\n",
    "plt.bar(x_pos, sorted_scores, color=[0.3, 0.5, 0.6])\n",
    "plt.ylabel(\"RMSE score\")\n",
    "plt.title(\"Variable importance for predicting {}\".format(input_names[1]))\n",
    "ax.text(0.01, 0.98, 'Note: lower scores suggest higher importance',\n",
    "        transform=ax.transAxes, va='top', ha='left', color=[0.3, 0.3, 0.3])\n",
    "plt.xticks(x_pos, sorted_names, rotation=40, ha=\"right\",\n",
    "                 rotation_mode=\"anchor\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose variables to include in model by their indices\n",
    "target_ind = 1\n",
    "model_rain_ind = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "model_vol_ind = [0, 1, 2]\n",
    "model_temp_ind = [0, 1, 2]\n",
    "model_hydro_ind = [0]\n",
    "\n",
    "# Getting the names of the selected features\n",
    "a1 = [rain_name[i] for i in model_rain_ind]\n",
    "a2 = [vol_name[i] for i in model_vol_ind]\n",
    "a3 = [temp_name[i] for i in model_temp_ind]\n",
    "a4 = [hydro_name[i] for i in model_hydro_ind]\n",
    "input_names = ['All features included', target_name[target_ind]] + a1 + a2 + a3 + a4\n",
    "\n",
    "# Compile target data and inputs to arrays\n",
    "target_data = target_ts[target_ind]\n",
    "model_input_data_ref = compile_model_input_variables(model_rain_ind,\n",
    "                                                 model_vol_ind,\n",
    "                                                 model_temp_ind,\n",
    "                                                 model_hydro_ind,\n",
    "                                                 rain_pp_all[target_ind],\n",
    "                                                 vol_pp,\n",
    "                                                 temp_pp,\n",
    "                                                 hydro_pp_all[target_ind],\n",
    "                                                 target_ts[target_ind].size)\n",
    "\n",
    "\n",
    "# Running MLP a total of tot_idx times, each time using only one of the input features\n",
    "tot_idx = 2 + model_input_data_ref.shape[0]\n",
    "all_scores = np.empty(tot_idx)\n",
    "model_input_data = model_input_data_ref\n",
    "\n",
    "for i in range(tot_idx):\n",
    "    if i > 0:\n",
    "        model_input_data = np.expand_dims(model_input_data_ref[i-2, :], axis=0)\n",
    "        model_tmp = ML_model(target_data=target_data,\n",
    "                                        input_data=model_input_data,\n",
    "                                        use_target=False,\n",
    "                                        look_back=30,\n",
    "                                        chunk_step=20,\n",
    "                                        train_ratio=0.67,\n",
    "                                        num_epochs=30,\n",
    "                                        batch_size=5,\n",
    "                                        predict_steps=1,\n",
    "                                        plot=False,\n",
    "                                        verbose=0)\n",
    "    else:\n",
    "        model_input_data = model_input_data_ref         \n",
    "        model_tmp = ML_model(target_data=target_data,\n",
    "                                        input_data=model_input_data,\n",
    "                                        use_target=True,\n",
    "                                        look_back=30,\n",
    "                                        chunk_step=20,\n",
    "                                        train_ratio=0.67,\n",
    "                                        num_epochs=30,\n",
    "                                        batch_size=5,\n",
    "                                        predict_steps=1,\n",
    "                                        plot=False,\n",
    "                                        verbose=0)\n",
    "    model_tmp.run(model_type='MLP')\n",
    "    \n",
    "    all_scores[i] = (model_tmp.trainRMSE + model_tmp.testRMSE)/2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting ranking \n",
    "ranking = np.argsort(1*all_scores) # Sorting in ascending order\n",
    "sorted_scores = all_scores[ranking]\n",
    "sorted_names = [input_names[i] for i in ranking]\n",
    "\n",
    "# Plotting bar chart\n",
    "plt.figure(figsize=(8, 4))\n",
    "ax = plt.subplot(111)\n",
    "x_pos = np.arange(0,len(sorted_names))\n",
    "plt.bar(x_pos, sorted_scores, color=[0.3, 0.5, 0.6])\n",
    "plt.ylabel(\"RMSE score\")\n",
    "plt.title(\"Variable importance for predicting {}\".format(input_names[1]))\n",
    "ax.text(0.01, 0.98, 'Note: lower scores suggest higher importance',\n",
    "        transform=ax.transAxes, va='top', ha='left', color=[0.3, 0.3, 0.3])\n",
    "plt.xticks(x_pos, sorted_names, rotation=40, ha=\"right\",\n",
    "                 rotation_mode=\"anchor\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 Predictability horizon\n",
    "\n",
    "The results presented so far only considered predicting a single timestep. However, it may be important to predict several steps ahead (time t+n), given information only until time t. The plot below shows the results from such multi-step predictions, along with the original target timeseries. Results suggest that the MLP can reasonably capture the overall trend for several days in advance, but only when the latter doesn't have very large jumps. The multistep prediction does not do very well in forecasting day-to-day fluctuations, in particular predicting the steep rises that occur from time to time in the target.\n",
    "\n",
    "It is possible that the groundwater depth is sensitive to the rainfall that occurs on the same day or very recent days (particularly for the shallower sub-systems), which means that predictions further into the future are hampered by the lack of very recent rainfall data. The activation of the layer weights shown in section 2.3.2 suggested that the most recent few timesteps were the most valuable in the model, so this seems to be consistent with a loss of model accuracy in more advanced predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose variables to include in model by their indices\n",
    "target_ind = 1\n",
    "model_rain_ind = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "model_vol_ind = [0, 1, 2]\n",
    "model_temp_ind = [0, 1, 2]\n",
    "model_hydro_ind = [0]\n",
    "\n",
    "# Getting the names of the selected features\n",
    "a1 = [rain_name[i] for i in model_rain_ind]\n",
    "a2 = [vol_name[i] for i in model_vol_ind]\n",
    "a3 = [temp_name[i] for i in model_temp_ind]\n",
    "a4 = [hydro_name[i] for i in model_hydro_ind]\n",
    "input_names = a1 + a2 + a3 + a4\n",
    "\n",
    "# look_back = 80\n",
    "# chunk_step = 10\n",
    "# train_ratio = 0.67\n",
    "# num_epochs = 200\n",
    "# batch_size = 5\n",
    "\n",
    "\n",
    "# Model parameters\n",
    "predict_steps = 15\n",
    "\n",
    "# Run ML model\n",
    "model_day_horizon = []\n",
    "predict_horizon_RMSE = np.empty((len(target_ts), predict_steps))\n",
    "predict_horizon_MSE = np.empty((len(target_ts), predict_steps))\n",
    "for n in range(len(target_ts)):\n",
    "    target_data = target_ts[n]\n",
    "    model_input_data = compile_model_input_variables(model_rain_ind,\n",
    "                                                     model_vol_ind,\n",
    "                                                     model_temp_ind,\n",
    "                                                     model_hydro_ind,\n",
    "                                                     rain_pp_all[n],\n",
    "                                                     vol_pp,\n",
    "                                                     temp_pp,\n",
    "                                                     hydro_pp_all[n],\n",
    "                                                     target_ts[n].size)\n",
    "\n",
    "    model_day_horizon.append(ML_model(target_data=target_data,\n",
    "                                input_data=model_input_data,\n",
    "                                use_target=True,\n",
    "                                look_back=40,\n",
    "                                chunk_step=8,\n",
    "                                train_ratio=0.67,\n",
    "                                num_epochs=80,\n",
    "                                batch_size=5,\n",
    "                                predict_steps=predict_steps,\n",
    "                                plot=False,\n",
    "                                verbose=0))\n",
    "    model_day_horizon[n].run(model_type='MLP')\n",
    "    predict_horizon_RMSE[n, :] = model_day_horizon[n].testRMSE\n",
    "    predict_horizon_MSE[n, :] = model_day_horizon[n].testMSE\n",
    "    if n == 1:\n",
    "        model_day_horizon[n].plot_predict_all_steps(crop_min=2000, crop_max=2500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots below show the RMSE and MSE, as a function of the number of days in advance the model predicts. As expected, the error increases with the number of steps. We hope that by choosing an appropriate threshold for acceptable error, this analysis can give the reader an idea of the predictability horizon of this model.\n",
    "\n",
    "There is a general trend that the deeper the groundwater subsystem, the smaller the error, and for 'LT2' (the deepest by far), there is much less increase in error as the prediction timestep increases. There is much more 'day-to-day' scale variation in the shallower systems (e.g. 'PAG' and 'DIEC'), compared to deeper systems (e.g. 'LT2' and 'SAL'), so it is likely that this is what makes the deeper systems easier to predict. This result supports the theory discussed in section 2.1.2, that the time it takes for external factors (such as rainfall) to affect deeper systems is longer than for shallower systems and therefore the effects of external factors are less short term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot RMSE against prediction look ahead\n",
    "plt.figure(figsize=(8, 4))\n",
    "for n in range(len(target_ts)):\n",
    "    plt.plot(np.arange(1, predict_steps+1), predict_horizon_RMSE[n, :],\n",
    "            label=target_name[n])\n",
    "plt.legend(loc='upper left')\n",
    "plt.title(\"Predictability horizon (RMSE)\")\n",
    "plt.xlabel(\"Steps in advance [days]\")\n",
    "plt.ylabel(\"RMSE score\")\n",
    "plt.ylim([0, 1.25*np.max(predict_horizon_RMSE)])\n",
    "plt.show()\n",
    "\n",
    "# Plot MSE against prediction look ahead\n",
    "plt.figure(figsize=(8, 4))\n",
    "for n in range(len(target_ts)):\n",
    "    plt.plot(np.arange(1, predict_steps+1), predict_horizon_MSE[n, :],\n",
    "            label=target_name[n])\n",
    "plt.legend(loc='upper left')\n",
    "plt.title(\"Predictability horizon (MSE)\")\n",
    "plt.xlabel(\"Steps in advance [days]\")\n",
    "plt.ylabel(\"MSE score\")\n",
    "plt.ylim([0, 1.25*np.max(predict_horizon_MSE)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.5 Weekly analysis \n",
    "\n",
    "Finally, we perform the same analysis here as above (Section 2.3.3), but this time using weekly averaged data, rather than daily. It might indeed be useful for the ACEA group to predict the ground water level over the coming weeks, without the detail of day-to-day fluctuations.\n",
    "\n",
    "The results below show that by performing this weekly average, these short term fluctuations are smoothed out to a certain extent, however model prediction doesn't improve compared to the daily predictions. We also include a predicitability horizon plot with the weekly data, which provides information about how far in advance one can make predictions.\n",
    "\n",
    "It seems that this prediction potentially suffers from the same issue discussed for the daily predictions, where the external features (in particular, rainfall) have a relatively short term effect on the target, at least for the shallower groundwater variables. The information that seems to be most useful in the model is the rainfall/hydrometry in the previous timestep, and this is not available for predictions further in advance.\n",
    "\n",
    "Note that this analysis could easily be done for monthly-mean data as well if required, but is omitted here for brevity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_data_mean(field, average_fac):\n",
    "    \"\"\"\n",
    "    Calculate mean of data array along last axis. field is a vector,\n",
    "    which can be more than 1D. The last dimension of field will be averaged,\n",
    "    so could be shape: (num_timesteps)\n",
    "    or shape: (num features, num_timesteps)\n",
    "    The output will be an array with the same number of dimensions as\n",
    "    field, but with the size of the last axis reduced depending on the\n",
    "    averaging amount 'average_fac'.\n",
    "    \"\"\"\n",
    "    num_dim = field.ndim\n",
    "    num_timesteps = field.shape[-1]\n",
    "    num_av = np.int(np.floor(num_timesteps / average_fac))\n",
    "    \n",
    "    # Get shape for temporary reshaped array\n",
    "    if num_dim > 1:\n",
    "        feature_shape = list(field.shape[:-1])\n",
    "        tmp_shape = feature_shape + [num_av, average_fac]\n",
    "    else:\n",
    "        tmp_shape = [num_av, average_fac]\n",
    "        \n",
    "    # Remove earliest timesteps if num_timesteps is not divisible by average_fac\n",
    "    field_crop = np.delete(field, np.s_[: -num_av*average_fac], axis=-1)\n",
    "    \n",
    "    # Take mean of data\n",
    "    tmp = np.reshape(field_crop, tmp_shape)\n",
    "    data_mean = np.mean(tmp, axis=-1)\n",
    "    \n",
    "    return data_mean\n",
    "\n",
    "# Choose variables to include in model by their indices\n",
    "target_ind = 1\n",
    "model_rain_ind = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "model_vol_ind = [0, 1, 2]\n",
    "model_temp_ind = [0, 1, 2]\n",
    "model_hydro_ind = [0]\n",
    "\n",
    "# Getting the names of the selected features\n",
    "a1 = [rain_name[i] for i in model_rain_ind]\n",
    "a2 = [vol_name[i] for i in model_vol_ind]\n",
    "a3 = [temp_name[i] for i in model_temp_ind]\n",
    "a4 = [hydro_name[i] for i in model_hydro_ind]\n",
    "input_names = a1 + a2 + a3 + a4\n",
    "\n",
    "# Compile target data and inputs to arrays\n",
    "target_data = target_ts[target_ind]\n",
    "model_input_data = compile_model_input_variables(model_rain_ind,\n",
    "                                                 model_vol_ind,\n",
    "                                                 model_temp_ind,\n",
    "                                                 model_hydro_ind,\n",
    "                                                 rain_pp_all[target_ind],\n",
    "                                                 vol_pp,\n",
    "                                                 temp_pp,\n",
    "                                                 hydro_pp_all[target_ind],\n",
    "                                                 target_ts[target_ind].size)\n",
    "\n",
    "# Model parameters\n",
    "predict_steps = 15\n",
    "average_fac = 7\n",
    "\n",
    "# Run ML model\n",
    "model_weekly_horizon = []\n",
    "predict_horizon_RMSE = np.empty((len(target_ts), predict_steps))\n",
    "predict_horizon_MSE = np.empty((len(target_ts), predict_steps))\n",
    "for n in range(len(target_ts)):\n",
    "    target_data = target_ts[n]\n",
    "    model_input_data = compile_model_input_variables(model_rain_ind,\n",
    "                                                     model_vol_ind,\n",
    "                                                     model_temp_ind,\n",
    "                                                     model_hydro_ind,\n",
    "                                                     rain_pp_all[n],\n",
    "                                                     vol_pp,\n",
    "                                                     temp_pp,\n",
    "                                                     hydro_pp_all[n],\n",
    "                                                     target_ts[n].size)\n",
    "\n",
    "    # Calculate 1 week average\n",
    "    target_data_weekly = get_data_mean(target_ts[n], average_fac)\n",
    "    input_data_weekly = get_data_mean(model_input_data, average_fac)\n",
    "    \n",
    "    model_weekly_horizon.append(ML_model(target_data=target_data_weekly,\n",
    "                                input_data=input_data_weekly,\n",
    "                                use_target=True,\n",
    "                                look_back=40,\n",
    "                                chunk_step=8,\n",
    "                                train_ratio=0.67,\n",
    "                                num_epochs=80,\n",
    "                                batch_size=5,\n",
    "                                predict_steps=predict_steps,\n",
    "                                plot=False,\n",
    "                                verbose=0))\n",
    "    model_weekly_horizon[n].run(model_type='MLP')\n",
    "    predict_horizon_RMSE[n, :] = model_weekly_horizon[n].testRMSE\n",
    "    predict_horizon_MSE[n, :] = model_weekly_horizon[n].testMSE\n",
    "    if n == 1:\n",
    "        model_weekly_horizon[n].plot_predict_all_steps(crop_min=200, crop_max=450)\n",
    "\n",
    "# Plot RMSE against prediction look ahead\n",
    "plt.figure(figsize=(8, 4))\n",
    "for n in range(len(target_ts)):\n",
    "    plt.plot(np.arange(1, predict_steps+1), predict_horizon_RMSE[n, :],\n",
    "            label=target_name[n])\n",
    "plt.legend(loc='upper left')\n",
    "plt.title(\"Predictability horizon (RMSE)\")\n",
    "plt.xlabel(\"Steps in advance [weeks]\")\n",
    "plt.ylabel(\"RMSE score\")\n",
    "plt.ylim([0, 1.25*np.max(predict_horizon_RMSE)])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Summary of Aquifier Auser\n",
    "\n",
    "This analysis showed that rainfall and hydrometry data are the most useful variables in predicting depth to groundwater for aquifier Auser. This fits with our intuition that local rainfall directly supplies the underground water system. Intense rainfall provides a short term burst to the water level, which subsequently decays over a slow timescale ranging from 50 days for shallow subsystems and hundreds of days for deep ones. \n",
    "\n",
    "Day-to-day fluctuations in groundwater level are stongly influenced by recent rainfall, and are therefore harder to predict than the overall weekly to monthly trends. It would be interesting to understand these short term fluctuations better and assess to what extent they are predictable. Perhaps including rainfall forecasts in the machine learning model could also improve its short term prediction skill. \n",
    "\n",
    "Temperature data appears to be able to make reasonable predictions of target on its own, but only to the extent that it is correlated with the more relevant rainfall or hydrometry variables (cold days tend to be rainier than warm ones). Temperature is also much less relevant for the deep groundwater system, as it is only well correleted with rainfall over shorter timescales. \n",
    "\n",
    "The machine learning models provide good predictions for all target variables. The MLP model is comparably accurate, faster and simpler than the LSTM, so was chosen for the rest of the study. Past target data is as useful as rainfall and hydrometry, and recent data makes the most difference in the prediction. Including data from other targets may increase performance further, particularly if the subsystems are physically connected to each other, through underground water pathways.\n",
    "\n",
    "Predicting multiple steps at a time proves to be a challenging task for day-to-day forecasts, particularly when the fluctuations are abrupt. This also appears to be the case when considering weekly-averaged data, even though this is smoother in nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Water Spring Amiata\n",
    "## 3.1 Data pre-processing\n",
    "### 3.1.1 Handling missing data\n",
    "First, the data was read in and an analysis of missing data was carried out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Pre-processing data\n",
    "\n",
    "# Importing all data into pandas dataframe\n",
    "df = pd.read_csv(files[6])\n",
    "\n",
    "# Find and plot gaps in the data\n",
    "all_data, col_names, is_nan, is_zero = find_data_gaps(df, plot=True, title=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows that there is a significant amount of data missing over the total time period, either in the form of NaNs, or zeros. In the case of the rainfall data, the zeros are considered to be 'real data' since there are many days with zero rainfall, however for the other data it is assumed that zeros imply missing data.\n",
    "\n",
    "Since there are many NaNs in most of the features for the first ~5500 days, including all of the target variables, it was decided that this time period would not be used for the model at all.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find column indices for different datatypes\n",
    "datatypes, col_inds = find_datatypes(df)\n",
    "\n",
    "# Get time series data for all variable types\n",
    "start_ind = 5760\n",
    "rain_ts, rain_name = get_time_series(col_inds[0], start_ind=start_ind, fill_zeros=False)\n",
    "depth_ts, depth_name = get_time_series(col_inds[1], start_ind=start_ind)\n",
    "temp_ts, temp_name = get_time_series(col_inds[2], start_ind=start_ind)\n",
    "target_ts, target_name = get_time_series(col_inds[5], start_ind=start_ind) \n",
    "\n",
    "# plt.figure(figsize=(9, 4))\n",
    "# for n in range(len(rain_ts)):\n",
    "#     plt.plot(rain_ts[n], label=rain_name[n])\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(9, 4))\n",
    "# for n in range(len(target_ts)):\n",
    "#     plt.plot(target_ts[n], label=target_name[n])\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Flow Rate analysis\n",
    "There are some large step changes in some of the Flow Rate variables, where the rate becomes zero (or close to zero) for several days in a row. Without understanding more details about the data collection, it is difficult to know whether these deviations represent reality or whether they are missed/incorrect recordings. However, it is likely that including them in the modelling will make it significantly more challenging to make accurate predictions, since they appear to be anomalous and unrelated to other features in this dataset.\n",
    "It was decided that these values should be filled by interpolation, to allow the model to better represent the majority of the data.\n",
    "\n",
    "The potentially anomalous data was identified by the following method:\n",
    "- Calculate the moving average of the time series with a large time window\n",
    "- Calculate the standard deviation of the time series\n",
    "- Set upper and lower thresholds at 'moving average +/- 1.6*std'\n",
    "- Any values outside of these thresholds were deemed to be potentially anomalous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Flow rate target variables\"\"\"\n",
    "\n",
    "def interp_nan(arr):\n",
    "    ser = pd.Series(arr)\n",
    "    ser.interpolate(method='linear', inplace=True)\n",
    "    out = ser.to_numpy()\n",
    "    return out\n",
    "\n",
    "# Find potentially anomalous samples by deviation from moving average\n",
    "flow_ts_filt = []\n",
    "flow_name = []\n",
    "flow_mov_av = []\n",
    "flow_thresh_hi = []\n",
    "flow_thresh_lo = []\n",
    "moving_av_win = 450\n",
    "for n in range(len(target_ts)):\n",
    "    removed_zeros = np.where(target_ts[n]==0, np.nan, target_ts[n])\n",
    "    overall_av = np.nanmean(removed_zeros)\n",
    "    filled_zeros = np.where(np.isnan(removed_zeros), overall_av, removed_zeros)\n",
    "    mov_av = movingav(filled_zeros, moving_av_win, winfunc=None)\n",
    "    thresh_hi = mov_av + 1.6*np.nanstd(removed_zeros)\n",
    "    thresh_lo = mov_av - 1.6*np.nanstd(removed_zeros)\n",
    "    flow_thresh_hi.append(thresh_hi)\n",
    "    flow_thresh_lo.append(thresh_lo)\n",
    "    is_outlier = np.logical_and(target_ts[n] < thresh_hi, target_ts[n] > thresh_lo)\n",
    "    ts_filt = np.where(is_outlier, target_ts[n], np.nan)\n",
    "    flow_mov_av.append(mov_av)\n",
    "    flow_ts_filt.append(ts_filt)\n",
    "\n",
    "target_ts_raw = np.copy(target_ts)\n",
    "for n in range(len(target_ts)):\n",
    "    target_ts[n] = interp_nan(flow_ts_filt[n])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    \n",
    "# Plot example of filtering out potential anomalies\n",
    "plt.figure(figsize=(9, 4))\n",
    "plt.plot(flow_mov_av[1], lw=1, ls='dashed', color=[0, 0, 0, 0.5], label='Moving average')\n",
    "plt.plot(flow_thresh_hi[1], lw=1, color=[0, 0, 0, 0.5], label='Thresholds')\n",
    "plt.plot(flow_thresh_lo[1], lw=1, color=[0, 0, 0, 0.5])\n",
    "plt.plot(target_ts_raw[1], lw=1, color=[0.7, 0.2, 0.2, 0.7], label='Filtered out samples')\n",
    "plt.plot(target_ts[1], color=[0.2, 0.7, 0.2, 0.7], label='Interpolated', alpha=0.9)\n",
    "plt.plot(flow_ts_filt[1], label=target_name[n], alpha=0.9)\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Flow rate pre-filtering')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Rainfall\n",
    "Pre-processing of the rainfall data by exponential filtering was considered, as for Aquifier Auser, however it didn't appear to be beneficial in this case. The target flow rate variables have a very different pattern to the rainfall, showing peaks and short term variation at different times. There isn't an obvious physical explanation for the relationship, as there seems to be for rainfall/depth to groundwater, so the exponential filtering makes less logical sense and was not used in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot example of correlation for different tau values\n",
    "\n",
    "# tau_best, src_best = find_best_tau(target_ts=target_ts[0], rain_ts=rain_ts,\n",
    "#                          rain_name=rain_name, plot=True,\n",
    "#                          target_name=target_name[1],\n",
    "#                          tau_array=np.linspace(1, 80, 80))\n",
    "\n",
    "# tau_best, src_best = find_best_tau(target_ts=target_ts[1], rain_ts=rain_ts, plot=True,\n",
    "#                          target_name=target_name[0],\n",
    "#                          rain_name=rain_name,\n",
    "#                          tau_array=np.linspace(1, 80, 80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Correlation Analysis\n",
    "The plot below shows a correlation matrix between the input features and the target data, calculated day-by-day, revealing the following information: \n",
    "- All of the rainfall datasets are positively correlated with each other\n",
    "- All of the depth to groundwater variables are strongly positively correlated with each other, and show a reasonably strong negative correlation to the target.\n",
    "- All of the temperature variables are strongly positively correlated with each other, and show a small amount of correlation to the target.\n",
    "- Depth to Groundwater doesn't show a correlation with rainfall, although the rainfall has not been filtered with an exponential window as it was for Aquifier Auser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Combine all pre-processed data\n",
    "all_data = np.concatenate((np.asarray(target_ts),\n",
    "                               np.asarray(rain_ts),\n",
    "                               np.asarray(depth_ts),\n",
    "                               np.asarray(temp_ts)))\n",
    "all_name = target_name + rain_name + depth_name + temp_name\n",
    "\n",
    "# Plot correlation matrices\n",
    "plot_correl_matrix(all_data, all_name,\n",
    "                   title='Correlation matrix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Better to generalise the earlier function probably\n",
    "\n",
    "def compile_model_input_variables_spring(model_rain_ind,\n",
    "                                  model_depth_ind,\n",
    "                                  model_temp_ind,\n",
    "                                  rain_pp,\n",
    "                                  depth_pp,\n",
    "                                  temp_pp,\n",
    "                                  num_timesteps):\n",
    "    \"\"\"\n",
    "    Extract the selected variables from the preprocessed data by their indices\n",
    "    and populate an input array to be used by the model.\n",
    "    The input array will have shape: (num features, num timesteps)\n",
    "    \n",
    "    \"\"\"    \n",
    "    num_inputs = len(model_rain_ind) + len(model_depth_ind) + \\\n",
    "                 len(model_temp_ind)\n",
    "    model_input_data = np.empty((num_inputs, num_timesteps))\n",
    "    count = 0\n",
    "    for i in model_rain_ind:\n",
    "        model_input_data[count, :] = rain_pp[i]\n",
    "        count += 1\n",
    "    for i in model_depth_ind:\n",
    "        model_input_data[count, :] = depth_pp[i]\n",
    "        count += 1\n",
    "    for i in model_temp_ind:\n",
    "        model_input_data[count, :] = temp_pp[i]\n",
    "        count += 1\n",
    "    \n",
    "    return model_input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Machine Learning Prediction \n",
    "\n",
    "We use an MLP machine learning model to predict the target variables given past timeseries information, as for the Aquifier Auser in section 2.3. We explore the following scenarios:\n",
    "1. Including versus excluding the target timeseries as an input to the predictive model\n",
    "2. Importance of the various features in the predictive skill\n",
    "3. Predicting several timesteps in advance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Including versus excluding the target timeseries as a model input\n",
    "\n",
    "The line plots below show the timeseries prediction of the model for target 'Flow_Rate_Arbure', with and without the inclusion of past target data as an input, respectively. Note that other target variables are excluded from the inputs in all cases. As expected, the model performs better when the target data is included. The model appears to lose the ability to resolve day-to-day variability without the inclusion of the target, although the long-term average is predicted reasonably well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# LSTM with or without target data\n",
    "\n",
    "\n",
    "# Including the target data \n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "# Choose variables to include in model by their indices\n",
    "target_ind = 1\n",
    "model_rain_ind = [0, 1, 2, 3]\n",
    "model_depth_ind = [0, 1, 2]\n",
    "model_temp_ind = [0, 1, 2]\n",
    "\n",
    "# Compile target data and inputs to arrays\n",
    "target_data = target_ts[target_ind]\n",
    "model_input_data = compile_model_input_variables_spring(model_rain_ind,\n",
    "                                                 model_depth_ind,\n",
    "                                                 model_temp_ind,\n",
    "                                                 rain_ts,\n",
    "                                                 depth_ts,\n",
    "                                                 temp_ts,\n",
    "                                                 target_ts[target_ind].size)\n",
    "\n",
    "# Create ML model class and run it\n",
    "Model_using_target = ML_model(target_data=target_data,\n",
    "                                input_data=model_input_data,\n",
    "                                use_target=True,\n",
    "                                look_back=30,\n",
    "                                chunk_step=2,\n",
    "                                train_ratio=0.67,\n",
    "                                num_epochs=50,\n",
    "                                batch_size=5,\n",
    "                                predict_steps=1,\n",
    "                                plot=False,\n",
    "                                verbose=0)\n",
    "Model_using_target.run(model_type='MLP')\n",
    "Model_using_target.plot_predict_one_step(title=\"Including the target data (MLP)\")\n",
    "\n",
    "# Excluding the target data\n",
    "Model_excluding_target = ML_model(target_data=target_data,\n",
    "                                input_data=model_input_data,\n",
    "                                use_target=False,\n",
    "                                look_back=80,\n",
    "                                chunk_step=2,\n",
    "                                train_ratio=0.67,\n",
    "                                num_epochs=50,\n",
    "                                batch_size=5,\n",
    "                                predict_steps=1,\n",
    "                                plot=False,\n",
    "                                verbose=0)\n",
    "Model_excluding_target.run(model_type='MLP')\n",
    "Model_excluding_target.plot_predict_one_step(title=\"Excluding the target data (MLP)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Assessing variable importance\n",
    "\n",
    "A similar analysis to that described in section 2.3.3 was carried out for this dataset. The RMSE scores in the bar chart appear to be less in agreement with the correlation matrix than they were for Aquifier Auser. The rainfall variables show similar predictive ability to the depth to groundwater variables, although the correlation of rainfall with the target is much lower.\n",
    "There could be some time lag effects that are causing this difference, since the machine learning model is able to use past data, while the correlation matrix only compares features at the same timestep.\n",
    "The results of this analysis suggest that there is benefit in including multiple features in this model, as the accuracy of using all features (even excluding the target) is significantly better than for individual features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose variables to include in model by their indices\n",
    "target_ind = 1\n",
    "model_rain_ind = [0, 1, 2, 3]\n",
    "model_depth_ind = [0, 1, 2]\n",
    "model_temp_ind = [0, 1, 2]\n",
    "\n",
    "# Compile target data and inputs to arrays\n",
    "target_data = target_ts[target_ind]\n",
    "model_input_data_ref = compile_model_input_variables_spring(model_rain_ind,\n",
    "                                                 model_depth_ind,\n",
    "                                                 model_temp_ind,\n",
    "                                                 rain_ts,\n",
    "                                                 depth_ts,\n",
    "                                                 temp_ts,\n",
    "                                                 target_ts[target_ind].size)\n",
    "\n",
    "# Getting the names of the selected features\n",
    "a1 = [rain_name[i] for i in model_rain_ind]\n",
    "a2 = [depth_name[i] for i in model_vol_ind]\n",
    "a3 = [temp_name[i] for i in model_temp_ind]\n",
    "input_names = ['All features included', 'All features excl. target', target_name[target_ind]] + a1 + a2 + a3\n",
    "\n",
    "\n",
    "# Running MLP a total of tot_idx times, each time using only one of the input features\n",
    "tot_idx = 3 + model_input_data_ref.shape[0]\n",
    "all_scores = np.empty(tot_idx)\n",
    "model_input_data = model_input_data_ref\n",
    "\n",
    "for i in range(tot_idx):\n",
    "    if i > 1:\n",
    "        model_input_data = np.expand_dims(model_input_data_ref[i-3, :], axis=0)\n",
    "        model_tmp = ML_model(target_data=target_data,\n",
    "                                        input_data=model_input_data,\n",
    "                                        use_target=False,\n",
    "                                        look_back=30,\n",
    "                                        chunk_step=20,\n",
    "                                        train_ratio=0.67,\n",
    "                                        num_epochs=30,\n",
    "                                        batch_size=5,\n",
    "                                        predict_steps=1,\n",
    "                                        plot=False,\n",
    "                                        verbose=0)\n",
    "    else:\n",
    "        if i == 0: include_target = True\n",
    "        else: include_target=False\n",
    "        model_input_data = model_input_data_ref         \n",
    "        model_tmp = ML_model(target_data=target_data,\n",
    "                                        input_data=model_input_data,\n",
    "                                        use_target=include_target,\n",
    "                                        look_back=30,\n",
    "                                        chunk_step=20,\n",
    "                                        train_ratio=0.67,\n",
    "                                        num_epochs=30,\n",
    "                                        batch_size=5,\n",
    "                                        predict_steps=1,\n",
    "                                        plot=False,\n",
    "                                        verbose=0)\n",
    "        \n",
    "    model_tmp.run(model_type='MLP')\n",
    "    \n",
    "    all_scores[i] = (model_tmp.trainRMSE + model_tmp.testRMSE)/2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting ranking \n",
    "ranking = np.argsort(1*all_scores) # Sorting in ascending order\n",
    "sorted_scores = all_scores[ranking]\n",
    "sorted_names = [input_names[i] for i in ranking]\n",
    "\n",
    "# Plotting bar chart\n",
    "plt.figure(figsize=(8, 4))\n",
    "ax = plt.subplot(111)\n",
    "x_pos = np.arange(0,len(sorted_names))\n",
    "plt.bar(x_pos, sorted_scores, color=[0.3, 0.5, 0.6])\n",
    "plt.ylabel(\"RMSE score\")\n",
    "plt.title(\"Variable importance for predicting {}\".format(input_names[2]))\n",
    "ax.text(0.01, 0.98, 'Note: lower scores suggest higher importance',\n",
    "        transform=ax.transAxes, va='top', ha='left', color=[0.3, 0.3, 0.3])\n",
    "plt.xticks(x_pos, sorted_names, rotation=40, ha=\"right\",\n",
    "                 rotation_mode=\"anchor\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 Predictability horizon\n",
    "\n",
    "As for Aquifier Auser in section 2.3.4, an anlysis of the prediction of several timesteps ahead was carried out.\n",
    "The plot below shows the RMSE as a function of the number of days in advance the model predicts, and as expected, the error increases with the number of steps. We hope that by choosing an appropriate threshold for acceptable error, this analysis can give the reader an idea of the predictability horizon of this model.\n",
    "\n",
    "The error levels are very different for the different target variables, however the lower scores match the targets where the overall changes in flow rate are lower, which is to be expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose variables to include in model by their indices\n",
    "target_ind = 1\n",
    "model_rain_ind = [0, 1, 2, 3]\n",
    "model_depth_ind = [0, 1, 2]\n",
    "model_temp_ind = [0, 1, 2]\n",
    "\n",
    "# Compile target data and inputs to arrays\n",
    "target_data = target_ts[target_ind]\n",
    "model_input_data = compile_model_input_variables_spring(model_rain_ind,\n",
    "                                                 model_depth_ind,\n",
    "                                                 model_temp_ind,\n",
    "                                                 rain_ts,\n",
    "                                                 depth_ts,\n",
    "                                                 temp_ts,\n",
    "                                                 target_ts[target_ind].size)\n",
    "\n",
    "\n",
    "# Model parameters\n",
    "predict_steps = 15\n",
    "\n",
    "# Run ML model\n",
    "model_day_horizon = []\n",
    "predict_horizon_RMSE = np.empty((len(target_ts), predict_steps))\n",
    "predict_horizon_MSE = np.empty((len(target_ts), predict_steps))\n",
    "for n in range(len(target_ts)):\n",
    "    target_data = target_ts[n]\n",
    "    model_day_horizon.append(ML_model(target_data=target_ts[n],\n",
    "                                input_data=model_input_data,\n",
    "                                use_target=True,\n",
    "                                look_back=40,\n",
    "                                chunk_step=8,\n",
    "                                train_ratio=0.67,\n",
    "                                num_epochs=80,\n",
    "                                batch_size=5,\n",
    "                                predict_steps=predict_steps,\n",
    "                                plot=False,\n",
    "                                verbose=0))\n",
    "    model_day_horizon[n].run(model_type='MLP')\n",
    "    predict_horizon_RMSE[n, :] = model_day_horizon[n].testRMSE\n",
    "    predict_horizon_MSE[n, :] = model_day_horizon[n].testMSE\n",
    "#     if n == 1:\n",
    "#         model_day_horizon[n].plot_predict_all_steps()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot RMSE against prediction look ahead\n",
    "plt.figure(figsize=(8, 6))\n",
    "for n in range(len(target_ts)):\n",
    "    plt.plot(np.arange(1, predict_steps+1), predict_horizon_RMSE[n, :],\n",
    "            label=target_name[n])\n",
    "plt.legend(loc='upper left')\n",
    "plt.title(\"Predictability horizon (RMSE)\")\n",
    "plt.xlabel(\"Steps in advance [days]\")\n",
    "plt.ylabel(\"RMSE score\")\n",
    "plt.ylim([0, 1.25*np.max(predict_horizon_RMSE)])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. River Arno\n",
    "\n",
    "## 4.1 Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing data\n",
    "\n",
    "# Importing all data into pandas dataframe\n",
    "df = pd.read_csv(files[5])\n",
    "\n",
    "# Find and plot gaps in the data\n",
    "all_data, col_names, is_nan, is_zero = find_data_gaps(df, plot=True, title=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows that there is a significant amount of missing data in rainfall. We choose to take data from ~ 2100 - 7100 days and ignore the rainfalls that do not have significant data within that time period. The input features thus include only the first 5 rainfalls and Temperature_Firenze. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find column indices for different datatypes\n",
    "datatypes, col_inds = find_datatypes(df)\n",
    "\n",
    "# Get time series data for all variable types\n",
    "start_ind = 2185\n",
    "stop_ind = 7165\n",
    "rain_ts, rain_name = get_time_series(col_inds[0], start_ind=start_ind, stop_ind=stop_ind, fill_zeros=False)\n",
    "temp_ts, temp_name = get_time_series(col_inds[2], start_ind=start_ind, stop_ind=stop_ind, fill_zeros=True)\n",
    "target_ts, target_name = get_time_series(col_inds[4], start_ind=start_ind, stop_ind=stop_ind, fill_zeros=True)\n",
    "# MG: Fill_zeros doesn't seem to be working here...\n",
    "\n",
    "# Remove zeros and non-zero anomalies from target\n",
    "# Note - probably not the most efficient method\n",
    "for m in range(len(target_ts)):\n",
    "    for n in range(len(target_ts[m])):\n",
    "        if n > 0 and target_ts[m][n] < 0.6:\n",
    "            target_ts[m][n] = target_ts[m][n-1]\n",
    "            \n",
    "# Exlcuding irrelevant rainfall datasets\n",
    "rain_ts = rain_ts[0:4]\n",
    "rain_name = rain_name[0:4]\n",
    "\n",
    "# plt.figure(figsize=(9, 4))\n",
    "# for n in range(len(rain_ts)):\n",
    "#     plt.plot(rain_ts[n], label=rain_name[n])\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(9, 4))\n",
    "# for n in range(len(temp_ts)):\n",
    "#     plt.plot(temp_ts[n], label=temp_name[n])\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(9, 4))\n",
    "# for n in range(len(target_ts)):\n",
    "#     plt.plot(target_ts[n], label=target_name[n])\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interrogation of the data revealed that the hydrometry timeseries displays sharp spikes followed by exponential-like decay. This behaviour is reminiscent of the Depth_to_Groundwater data explored for Aquifier Auser at the start of the notebook. We had found that pre-processing the rainfall data with an exponential filter had been effective at drawing correlations between the rainfall and targets, hence we attempt the same method here.\n",
    "\n",
    "The plot below shows how the correlation of the rainfall with the target changes as the time constant (tau) of the exponential window is changed, with a clear pak visible at tau ~= 45."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_best, src_best = find_best_tau(target_ts=target_ts[0], rain_ts=rain_ts,\n",
    "                         rain_name=rain_name, plot=True,\n",
    "                         target_name=target_name[0],\n",
    "                         tau_array=np.linspace(1, 101, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating optimum tau for each rainfall/target combination that\n",
    "# maximises the Spearman's Rank correlation coefficient\n",
    "\n",
    "# rain_tau = find_all_best_tau(rain_ts, rain_name, target_ts, target_name,\n",
    "#                             tau_array=np.linspace(1, 101, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_filt = rain_ts.copy()\n",
    "rain_name_pp = rain_name.copy()\n",
    "\n",
    "# Rainfall data preprocessing\n",
    "rain_pp_all = []\n",
    "for n in range(len(rain_filt)):\n",
    "    rain_pp_ = []\n",
    "    for m in range(len(target_ts)):\n",
    "#         tau_ = tau_best[n]\n",
    "        tau_ = 8\n",
    "        rain_pp_.append(exp_convolve(rain_filt[n], tau_))\n",
    "    rain_pp_all.append(rain_pp_)\n",
    "\n",
    "# Flip the dimensions of the list\n",
    "rain_pp_all = [list(x) for x in zip(*rain_pp_all)]\n",
    "\n",
    "# Temperature data preprocessing\n",
    "temp_pp = temp_ts.copy()\n",
    "temp_name_pp = temp_name.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better to genralise the earlier function probably\n",
    "\n",
    "def compile_model_input_variables_river(model_rain_ind,\n",
    "                                  model_temp_ind,\n",
    "                                  rain_pp,\n",
    "                                  temp_pp,\n",
    "                                  num_timesteps):\n",
    "    \"\"\"\n",
    "    Extract the selected variables from the preprocessed data by their indices\n",
    "    and populate an input array to be used by the model.\n",
    "    The input array will have shape: (num features, num timesteps)\n",
    "    \n",
    "    \"\"\"    \n",
    "    num_inputs = len(model_rain_ind) + len(model_temp_ind)\n",
    "    model_input_data = np.empty((num_inputs, num_timesteps))\n",
    "    count = 0\n",
    "    for i in model_rain_ind:\n",
    "        model_input_data[count, :] = rain_pp[i]\n",
    "        count += 1\n",
    "    for i in model_temp_ind:\n",
    "        model_input_data[count, :] = temp_pp[i]\n",
    "        count += 1\n",
    "    \n",
    "    return model_input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot below compares the original rainfall data against the exponentially filtered data and the target data. There is a clear improvement in how closely the rainfall matches the target data after the processing. The upper of the two plots shows the processing with tau = 45, while the lower plot shows the same data processed with tau = 8. Although the correlation is maximised with tau = 45, there seems to be a better match visually for the smaller time constant with better resolution of the peaks and a better matching decay rate after the peaks. It is not clear why the correlation is maximised for the larger value, and perhaps some further work could be done to investigate this more closely.\n",
    "\n",
    "For the purposes of the machine learning model, the smaller tau of 8 is used to try to maintain the resolution, which may be useful in predicting daily variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Rainfall data analysis\n",
    "\n",
    "r_i = 0\n",
    "t_i = 0\n",
    "rain_pp_test = exp_convolve(rain_ts[r_i], tau=45)\n",
    "#rain_pp_test = rain_pp_all[0][r_i]\n",
    "\n",
    "plot_preprocessing(rain_ts[r_i], rain_pp_test, rain_name[r_i], target_ts[t_i], target_name[t_i],\n",
    "                   crop_min=3200, crop_max=3800, title='Rainfall preprocessing for tau = 45')\n",
    "\n",
    "\n",
    "# Looks like it would be better with tau = ~8, not sure why the correlation is maximised at ~45\n",
    "\n",
    "rain_pp_test = exp_convolve(rain_ts[r_i], tau=8)\n",
    "#rain_pp_test = rain_pp_all[0][r_i]\n",
    "\n",
    "plot_preprocessing(rain_ts[r_i], rain_pp_test, rain_name[r_i], target_ts[t_i], target_name[t_i],\n",
    "                   crop_min=3200, crop_max=3800, title='Rainfall preprocessing for tau = 8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Correlation Analysis\n",
    "The plot below shows a correlation matrix between the input features and the target data, calculated day-by-day, revealing the following information: \n",
    "- All of the rainfall datasets are positively correlated with each other and less strongly correlated to the target.\n",
    "- The temperature variable shows a negative correlation with both the target and the rainfall variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Combine all pre-processed data\n",
    "all_data = np.concatenate((np.asarray(target_ts),\n",
    "                               np.asarray(rain_ts),\n",
    "                               np.asarray(temp_ts)))\n",
    "all_name = target_name + rain_name + temp_name\n",
    "\n",
    "# Plot correlation matrices\n",
    "plot_correl_matrix(all_data, all_name,\n",
    "                   title='Correlation matrix')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Machine Learning Prediction \n",
    "\n",
    "We use an MLP machine learning model to predict the target variables given past timeseries information, as for the Aquifier Auser in section 2.3. We explore the following scenarios:\n",
    "1. Including versus excluding the target timeseries as an input to the predictive model\n",
    "2. Importance of the various features in the predictive skill\n",
    "3. Predicting several timesteps in advance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Including versus excluding the target timeseries as a model input\n",
    "\n",
    "The line plots below show the timeseries prediction of the model for target hydrometry, with and without the inclusion of past target data as an input, respectively. The model performs similarly, whether the target data is included or not. The RMSE scores for training are lower than for testing, which suggests that the model may be overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LSTM with or without target data\n",
    "\n",
    "\n",
    "# Including the target data \n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "# Choose variables to include in model by their indices\n",
    "target_ind = 0\n",
    "model_rain_ind = [0, 1, 2, 3]\n",
    "model_temp_ind = [0]\n",
    "\n",
    "# Compile target data and inputs to arrays\n",
    "target_data = target_ts[target_ind]\n",
    "model_input_data = compile_model_input_variables_river(model_rain_ind,\n",
    "                                                 model_temp_ind,\n",
    "                                                 rain_pp_all[target_ind],\n",
    "                                                 temp_pp,\n",
    "                                                 target_ts[target_ind].size)\n",
    "\n",
    "# Create ML model class and run it\n",
    "Model_using_target = ML_model(target_data=target_data,\n",
    "                                input_data=model_input_data,\n",
    "                                use_target=True,\n",
    "                                look_back=30,\n",
    "                                chunk_step=3,\n",
    "                                train_ratio=0.67,\n",
    "                                num_epochs=50,\n",
    "                                batch_size=5,\n",
    "                                predict_steps=5,\n",
    "                                plot=False,\n",
    "                                verbose=0)\n",
    "Model_using_target.run(model_type='MLP', print_score=True)\n",
    "Model_using_target.plot_predict_one_step(title=\"Including the target data (MLP)\")\n",
    "\n",
    "# Excluding the target data\n",
    "Model_excluding_target = ML_model(target_data=target_data,\n",
    "                                input_data=model_input_data,\n",
    "                                use_target=False,\n",
    "                                look_back=80,\n",
    "                                chunk_step=3,\n",
    "                                train_ratio=0.67,\n",
    "                                num_epochs=70,\n",
    "                                batch_size=5,\n",
    "                                predict_steps=5,\n",
    "                                plot=False,\n",
    "                                verbose=0)\n",
    "Model_excluding_target.run(model_type='MLP', print_score=True)\n",
    "Model_excluding_target.plot_predict_one_step(title=\"Excluding the target data (MLP)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 Assessing variable importance\n",
    "\n",
    "A similar analysis to that described in section 2.3.3 was carried out for this dataset. The RMSE scores in the bar chart appear to be less in agreement with the correlation matrix than they were for Aquifier Auser. The rainfall variables show good predictive ability, while the target is less effective on it's own. This may suggest that the rainfall is the cause of changes in the target - perhaps the most recent days of rainfall are more effective at predicting the target because there is a time lag of at least a day before the rain affects hydrometry.\n",
    "The results of this analysis suggest that there is benefit in including multiple features in this model, as the accuracy of using all features (even excluding the target) is better than for individual features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose variables to include in model by their indices\n",
    "target_ind = 0\n",
    "model_rain_ind = [0, 1, 2, 3]\n",
    "model_temp_ind = [0]\n",
    "\n",
    "# Compile target data and inputs to arrays\n",
    "target_data = target_ts[target_ind]\n",
    "model_input_data_ref = compile_model_input_variables_river(model_rain_ind,\n",
    "                                                 model_temp_ind,\n",
    "                                                 rain_pp_all[target_ind],\n",
    "                                                 temp_pp,\n",
    "                                                 target_ts[target_ind].size)\n",
    "\n",
    "# Getting the names of the selected features\n",
    "a1 = [rain_name[i] for i in model_rain_ind]\n",
    "a2 = [temp_name[i] for i in model_temp_ind]\n",
    "input_names = ['All features included', 'All features excl. target', target_name[target_ind]] + a1 + a2\n",
    "\n",
    "\n",
    "# Running MLP a total of tot_idx times, each time using only one of the input features\n",
    "tot_idx = 3 + model_input_data_ref.shape[0]\n",
    "all_scores = np.empty(tot_idx)\n",
    "model_input_data = model_input_data_ref\n",
    "\n",
    "for i in range(tot_idx):\n",
    "    if i > 1:\n",
    "        model_input_data = np.expand_dims(model_input_data_ref[i-3, :], axis=0)\n",
    "        model_tmp = ML_model(target_data=target_data,\n",
    "                                        input_data=model_input_data,\n",
    "                                        use_target=False,\n",
    "                                        look_back=30,\n",
    "                                        chunk_step=20,\n",
    "                                        train_ratio=0.67,\n",
    "                                        num_epochs=30,\n",
    "                                        batch_size=5,\n",
    "                                        predict_steps=1,\n",
    "                                        plot=False,\n",
    "                                        verbose=0)\n",
    "    else:\n",
    "        if i == 0: include_target = True\n",
    "        else: include_target=False\n",
    "        model_input_data = model_input_data_ref         \n",
    "        model_tmp = ML_model(target_data=target_data,\n",
    "                                        input_data=model_input_data,\n",
    "                                        use_target=include_target,\n",
    "                                        look_back=30,\n",
    "                                        chunk_step=20,\n",
    "                                        train_ratio=0.67,\n",
    "                                        num_epochs=30,\n",
    "                                        batch_size=5,\n",
    "                                        predict_steps=1,\n",
    "                                        plot=False,\n",
    "                                        verbose=0)\n",
    "        \n",
    "    model_tmp.run(model_type='MLP')\n",
    "    \n",
    "    all_scores[i] = (model_tmp.trainRMSE + model_tmp.testRMSE)/2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting ranking \n",
    "ranking = np.argsort(1*all_scores) # Sorting in ascending order\n",
    "sorted_scores = all_scores[ranking]\n",
    "sorted_names = [input_names[i] for i in ranking]\n",
    "\n",
    "# Plotting bar chart\n",
    "plt.figure(figsize=(8, 4))\n",
    "ax = plt.subplot(111)\n",
    "x_pos = np.arange(0,len(sorted_names))\n",
    "plt.bar(x_pos, sorted_scores, color=[0.3, 0.5, 0.6])\n",
    "plt.ylabel(\"RMSE score\")\n",
    "plt.title(\"Variable importance for predicting {}\".format(input_names[2]))\n",
    "ax.text(0.01, 0.98, 'Note: lower scores suggest higher importance',\n",
    "        transform=ax.transAxes, va='top', ha='left', color=[0.3, 0.3, 0.3])\n",
    "plt.xticks(x_pos, sorted_names, rotation=40, ha=\"right\",\n",
    "                 rotation_mode=\"anchor\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 Predictability horizon\n",
    "\n",
    "As for Aquifier Auser in section 2.3.4, an anlysis of the prediction of several timesteps ahead was carried out.\n",
    "The plot below shows the RMSE as a function of the number of days in advance the model predicts, and as expected, the error increases with the number of steps. We hope that by choosing an appropriate threshold for acceptable error, this analysis can give the reader an idea of the predictability horizon of this model. There is not a particularly steep increase in the RMSE as the number of steps increases compared to most of the other targets in the other datasets analysed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose variables to include in model by their indices\n",
    "target_ind = 0\n",
    "model_rain_ind = [0, 1, 2, 3]\n",
    "model_temp_ind = [0]\n",
    "\n",
    "# Compile target data and inputs to arrays\n",
    "target_data = target_ts[target_ind]\n",
    "model_input_data = compile_model_input_variables_river(model_rain_ind,\n",
    "                                                 model_temp_ind,\n",
    "                                                 rain_pp_all[target_ind],\n",
    "                                                 temp_pp,\n",
    "                                                 target_ts[target_ind].size)\n",
    "\n",
    "\n",
    "\n",
    "# Model parameters\n",
    "predict_steps = 15\n",
    "\n",
    "# Run ML model\n",
    "model_day_horizon = []\n",
    "predict_horizon_RMSE = np.empty((len(target_ts), predict_steps))\n",
    "predict_horizon_MSE = np.empty((len(target_ts), predict_steps))\n",
    "for n in range(len(target_ts)):\n",
    "    target_data = target_ts[n]\n",
    "    model_day_horizon.append(ML_model(target_data=target_ts[n],\n",
    "                                input_data=model_input_data,\n",
    "                                use_target=True,\n",
    "                                look_back=40,\n",
    "                                chunk_step=4,\n",
    "                                train_ratio=0.67,\n",
    "                                num_epochs=80,\n",
    "                                batch_size=5,\n",
    "                                predict_steps=predict_steps,\n",
    "                                plot=False,\n",
    "                                verbose=0))\n",
    "    model_day_horizon[n].run(model_type='MLP')\n",
    "    predict_horizon_RMSE[n, :] = model_day_horizon[n].testRMSE\n",
    "    predict_horizon_MSE[n, :] = model_day_horizon[n].testMSE\n",
    "#     if n == 1:\n",
    "#         model_day_horizon[n].plot_predict_all_steps()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot RMSE against prediction look ahead\n",
    "plt.figure(figsize=(8, 6))\n",
    "for n in range(len(target_ts)):\n",
    "    plt.plot(np.arange(1, predict_steps+1), predict_horizon_RMSE[n, :],\n",
    "            label=target_name[n])\n",
    "plt.legend(loc='upper left')\n",
    "plt.title(\"Predictability horizon (RMSE)\")\n",
    "plt.xlabel(\"Steps in advance [days]\")\n",
    "plt.ylabel(\"RMSE score\")\n",
    "plt.ylim([0, 1.25*np.max(predict_horizon_RMSE)])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Lake Bilancino\n",
    "\n",
    "## 5.1 Data pre-processing\n",
    "\n",
    "### 5.1.1 Data availability\n",
    "Since there is no data for most of the variables up to ~700 days, this portion of the data was removed from all of the variables for the modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing data\n",
    "\n",
    "# Importing all data into pandas dataframe\n",
    "df = pd.read_csv(files[4])\n",
    "\n",
    "# Find and plot gaps in the data\n",
    "all_data, col_names, is_nan, is_zero = find_data_gaps(df, plot=True, title=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find column indices for different datatypes\n",
    "datatypes, col_inds = find_datatypes(df)\n",
    "\n",
    "# Get time series data for all variable types\n",
    "start_ind = 832\n",
    "rain_ts, rain_name = get_time_series(col_inds[0], start_ind=start_ind, fill_zeros=False)\n",
    "temp_ts, temp_name = get_time_series(col_inds[2], start_ind=start_ind, fill_zeros=True)\n",
    "target_ts1, target_name1 = get_time_series(col_inds[5], start_ind=start_ind, fill_zeros=True)\n",
    "target_ts2, target_name2 = get_time_series(col_inds[6], start_ind=start_ind, fill_zeros=True)\n",
    "target_ts = target_ts1 + target_ts2\n",
    "target_name = target_name1 + target_name2 \n",
    "\n",
    "# plt.figure(figsize=(9, 4))\n",
    "# for n in range(len(rain_ts)):\n",
    "#     plt.plot(rain_ts[n], label=rain_name[n])\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(9, 4))\n",
    "# for n in range(len(temp_ts)):\n",
    "#     plt.plot(temp_ts[n], label=temp_name[n])\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(9, 4))\n",
    "# plt.plot(target_ts[0], label=target_name[0])\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(9, 4))\n",
    "# plt.plot(target_ts[1], label=target_name[1])\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 Rainfall analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot below compares the original rainfall data against the exponentially filtered data and the lake level target data. As for other datasets, there seems to be a clear improvement in the matching of the rainfall to the target with this processing applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rainfall data analysis\n",
    "\n",
    "r_i = 0\n",
    "t_i = 1\n",
    "rain_pp_test = exp_convolve(rain_ts[r_i], tau=150)\n",
    "#rain_pp_test = rain_pp_all[0][r_i]\n",
    "\n",
    "plot_preprocessing(rain_ts[r_i], rain_pp_test, rain_name[r_i], target_ts[t_i], target_name[t_i],\n",
    "                   crop_min=1600, crop_max=2000, title=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots show a similar analysis for the flow rate target data. The upper of the two plots shows the processing with tau = 163, while the lower plot shows the same data processed with tau = 5. Although the correlation is maximised with tau = 163, there seems to be a better match visually for the smaller time constant with better resolution of the peaks and a better matching decay rate after the peaks. It is not clear why the correlation is maximised for the larger value, and perhaps some further work could be done to investigate this more closely.\n",
    "\n",
    "It can be seen in the third plot that there are two peaks in the relationship between correlation and tau, with the smaller peak around tau = 5. Some further investigation into this may be interesting, as it could reveal some useful physical explanation for this relationship. For the purposes of the machine learning model in this study however, the smaller tau is used to try to maintain the resolution, which is expected to be more useful in predicting daily variation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rainfall data analysis\n",
    "\n",
    "r_i = 1\n",
    "t_i = 0\n",
    "rain_pp_test = exp_convolve(rain_ts[r_i], tau=163)\n",
    "#rain_pp_test = rain_pp_all[0][r_i]\n",
    "\n",
    "plot_preprocessing(rain_ts[r_i], rain_pp_test, rain_name[r_i], target_ts[t_i], target_name[t_i],\n",
    "                   crop_min=1600, crop_max=2000, title='Rainfall preprocessing for tau = 163')\n",
    "\n",
    "r_i = 1\n",
    "t_i = 0\n",
    "rain_pp_test = exp_convolve(rain_ts[r_i], tau=5)\n",
    "#rain_pp_test = rain_pp_all[0][r_i]\n",
    "\n",
    "plot_preprocessing(rain_ts[r_i], rain_pp_test, rain_name[r_i], target_ts[t_i], target_name[t_i],\n",
    "                   crop_min=1600, crop_max=2000, title='Rainfall preprocessing for tau = 5')\n",
    "\n",
    "\n",
    "\n",
    "# Looks like tau = 5 is much better than the supposed 'max correlation' tau=163, not sure why?\n",
    "# There's a double peak in the 'find_best_tau' plot, and the first peak around tau = ~5 seems\n",
    "# more sensible than the peak at tau = ~160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_best, src_best = find_best_tau(target_ts=target_ts[0], rain_ts=rain_ts,\n",
    "                         rain_name=rain_name, plot=True,\n",
    "                         target_name=target_name[0],\n",
    "                         tau_array=np.linspace(2, 240, 80))\n",
    "\n",
    "# tau_best, src_best = find_best_tau(target_ts=target_ts[1], rain_ts=rain_ts,\n",
    "#                          rain_name=rain_name, plot=True,\n",
    "#                          target_name=target_name[0],\n",
    "#                          tau_array=np.linspace(2, 240, 80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating optimum tau for each rainfall/target combination that\n",
    "# maximises the Spearman's Rank correlation coefficient\n",
    "\n",
    "rain_tau = find_all_best_tau(rain_ts, rain_name, target_ts, target_name,\n",
    "                             tau_array=np.linspace(2, 200, 50), plot=False)\n",
    "\n",
    "# Changed tau test range to avoid later peaks at ~tau=160 for first target\n",
    "# Not sure what the explanation is for correlation being higher at tau=160\n",
    "# but it seems more sensible to pick the peak at tau = ~5 as it seems visually better\n",
    "rain_tau = find_all_best_tau(rain_ts, rain_name, target_ts, target_name,\n",
    "                             tau_array=np.linspace(2, 200, 50),\n",
    "                             target_0_tau_array=np.linspace(1, 50, 50), plot=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_filt = rain_ts.copy()\n",
    "rain_name_pp = rain_name.copy()\n",
    "\n",
    "# Rainfall data preprocessing\n",
    "rain_pp_all = []\n",
    "for n in range(len(rain_filt)):\n",
    "    rain_pp_ = []\n",
    "    for m in range(len(target_ts)):\n",
    "        tau_ = rain_tau[m, n]\n",
    "        rain_pp_.append(exp_convolve(rain_filt[n], tau_))\n",
    "    rain_pp_all.append(rain_pp_)\n",
    "\n",
    "# Flip the dimensions of the list\n",
    "rain_pp_all = [list(x) for x in zip(*rain_pp_all)]\n",
    "\n",
    "# Temperature data preprocessing\n",
    "temp_pp = temp_ts.copy()\n",
    "temp_name_pp = temp_name.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better to genralise the earlier function probably\n",
    "\n",
    "def compile_model_input_variables_lake(model_rain_ind,\n",
    "                                  model_temp_ind,\n",
    "                                  rain_pp,\n",
    "                                  temp_pp,\n",
    "                                  num_timesteps):\n",
    "    \"\"\"\n",
    "    Extract the selected variables from the preprocessed data by their indices\n",
    "    and populate an input array to be used by the model.\n",
    "    The input array will have shape: (num features, num timesteps)\n",
    "    \n",
    "    \"\"\"    \n",
    "    num_inputs = len(model_rain_ind) + len(model_temp_ind)\n",
    "    model_input_data = np.empty((num_inputs, num_timesteps))\n",
    "    count = 0\n",
    "    for i in model_rain_ind:\n",
    "        model_input_data[count, :] = rain_pp[i]\n",
    "        count += 1\n",
    "    for i in model_temp_ind:\n",
    "        model_input_data[count, :] = temp_pp[i]\n",
    "        count += 1\n",
    "    \n",
    "    return model_input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Machine Learning Prediction \n",
    "\n",
    "We use an MLP machine learning model to predict the target variables given past timeseries information, as for the Aquifier Auser in section 2.3. We explore the following scenarios:\n",
    "1. Including versus excluding the target timeseries as an input to the predictive model\n",
    "2. Importance of the various features in the predictive skill\n",
    "3. Predicting several timesteps in advance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 Including versus excluding the target timeseries as a model input\n",
    "\n",
    "The line plots below show the timeseries prediction of the model for target flow rate, with and without the inclusion of past target data as an input, respectively. The model performs similarly, whether the target data is included or not. The RMSE scores for training are lower than for testing, which suggests that the model may be overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# With or without target data\n",
    "\n",
    "# Including the target data \n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "# Choose variables to include in model by their indices\n",
    "target_ind = 0\n",
    "model_rain_ind = [0, 1, 2, 3, 4]\n",
    "model_temp_ind = [0]\n",
    "\n",
    "# Compile target data and inputs to arrays\n",
    "target_data = target_ts[target_ind]\n",
    "model_input_data = compile_model_input_variables_lake(model_rain_ind,\n",
    "                                                 model_temp_ind,\n",
    "                                                 rain_pp_all[target_ind],\n",
    "                                                 temp_pp,\n",
    "                                                 target_ts[target_ind].size)\n",
    "\n",
    "# Create ML model class and run it\n",
    "Model_using_target = ML_model(target_data=target_data,\n",
    "                                input_data=model_input_data,\n",
    "                                use_target=True,\n",
    "                                look_back=30,\n",
    "                                chunk_step=3,\n",
    "                                train_ratio=0.67,\n",
    "                                num_epochs=50,\n",
    "                                batch_size=5,\n",
    "                                predict_steps=5,\n",
    "                                plot=False,\n",
    "                                verbose=0)\n",
    "Model_using_target.run(model_type='MLP', print_score=True)\n",
    "Model_using_target.plot_predict_one_step(title=\"Including the target data (MLP)\")\n",
    "\n",
    "# Excluding the target data\n",
    "Model_excluding_target = ML_model(target_data=target_data,\n",
    "                                input_data=model_input_data,\n",
    "                                use_target=False,\n",
    "                                look_back=80,\n",
    "                                chunk_step=3,\n",
    "                                train_ratio=0.67,\n",
    "                                num_epochs=70,\n",
    "                                batch_size=5,\n",
    "                                predict_steps=5,\n",
    "                                plot=False,\n",
    "                                verbose=0)\n",
    "Model_excluding_target.run(model_type='MLP', print_score=True)\n",
    "Model_excluding_target.plot_predict_one_step(title=\"Excluding the target data (MLP)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the same analysis repeated for the other tyoe of target variable, the lake level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# With or without target data\n",
    "\n",
    "\n",
    "# Including the target data \n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "# Choose variables to include in model by their indices\n",
    "target_ind = 1\n",
    "model_rain_ind = [0, 1, 2, 3, 4]\n",
    "model_temp_ind = [0]\n",
    "\n",
    "# Compile target data and inputs to arrays\n",
    "target_data = target_ts[target_ind]\n",
    "model_input_data = compile_model_input_variables_lake(model_rain_ind,\n",
    "                                                 model_temp_ind,\n",
    "                                                 rain_pp_all[target_ind],\n",
    "                                                 temp_pp,\n",
    "                                                 target_ts[target_ind].size)\n",
    "\n",
    "# Create ML model class and run it\n",
    "Model_using_target = ML_model(target_data=target_data,\n",
    "                                input_data=model_input_data,\n",
    "                                use_target=True,\n",
    "                                look_back=30,\n",
    "                                chunk_step=3,\n",
    "                                train_ratio=0.67,\n",
    "                                num_epochs=50,\n",
    "                                batch_size=5,\n",
    "                                predict_steps=5,\n",
    "                                plot=False,\n",
    "                                verbose=0)\n",
    "Model_using_target.run(model_type='MLP', print_score=True)\n",
    "Model_using_target.plot_predict_one_step(title=\"Including the target data (MLP)\")\n",
    "\n",
    "# Excluding the target data\n",
    "Model_excluding_target = ML_model(target_data=target_data,\n",
    "                                input_data=model_input_data,\n",
    "                                use_target=False,\n",
    "                                look_back=80,\n",
    "                                chunk_step=3,\n",
    "                                train_ratio=0.67,\n",
    "                                num_epochs=70,\n",
    "                                batch_size=5,\n",
    "                                predict_steps=5,\n",
    "                                plot=False,\n",
    "                                verbose=0)\n",
    "Model_excluding_target.run(model_type='MLP', print_score=True)\n",
    "Model_excluding_target.plot_predict_one_step(title=\"Excluding the target data (MLP)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2 Assessing variable importance\n",
    "\n",
    "A similar analysis to that described in section 2.3.3 was carried out for this dataset.\n",
    "For the lake level, there seems to be benefit in including all of the variables in the analysis, as the error scores are much lower than for individual features (even the target). This could suggest that there is some interaction between the variables, or that several variables are independently affecting the lake level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose variables to include in model by their indices\n",
    "target_ind = 0\n",
    "model_rain_ind = [0, 1, 2, 3, 4]\n",
    "model_temp_ind = [0]\n",
    "\n",
    "# Compile target data and inputs to arrays\n",
    "target_data = target_ts[target_ind]\n",
    "model_input_data_ref = compile_model_input_variables_lake(model_rain_ind,\n",
    "                                                 model_temp_ind,\n",
    "                                                 rain_pp_all[target_ind],\n",
    "                                                 temp_pp,\n",
    "                                                 target_ts[target_ind].size)\n",
    "\n",
    "# Getting the names of the selected features\n",
    "a1 = [rain_name[i] for i in model_rain_ind]\n",
    "a2 = [temp_name[i] for i in model_temp_ind]\n",
    "input_names = ['All features included', 'All features excl. target', target_name[target_ind]] + a1 + a2\n",
    "\n",
    "\n",
    "# Running MLP a total of tot_idx times, each time using only one of the input features\n",
    "tot_idx = 3 + model_input_data_ref.shape[0]\n",
    "all_scores = np.empty(tot_idx)\n",
    "model_input_data = model_input_data_ref\n",
    "\n",
    "for i in range(tot_idx):\n",
    "    if i > 1:\n",
    "        model_input_data = np.expand_dims(model_input_data_ref[i-3, :], axis=0)\n",
    "        model_tmp = ML_model(target_data=target_data,\n",
    "                                        input_data=model_input_data,\n",
    "                                        use_target=False,\n",
    "                                        look_back=30,\n",
    "                                        chunk_step=20,\n",
    "                                        train_ratio=0.67,\n",
    "                                        num_epochs=30,\n",
    "                                        batch_size=5,\n",
    "                                        predict_steps=1,\n",
    "                                        plot=False,\n",
    "                                        verbose=0)\n",
    "    else:\n",
    "        if i == 0: include_target = True\n",
    "        else: include_target=False\n",
    "        model_input_data = model_input_data_ref         \n",
    "        model_tmp = ML_model(target_data=target_data,\n",
    "                                        input_data=model_input_data,\n",
    "                                        use_target=include_target,\n",
    "                                        look_back=30,\n",
    "                                        chunk_step=20,\n",
    "                                        train_ratio=0.67,\n",
    "                                        num_epochs=30,\n",
    "                                        batch_size=5,\n",
    "                                        predict_steps=1,\n",
    "                                        plot=False,\n",
    "                                        verbose=0)\n",
    "        \n",
    "    model_tmp.run(model_type='MLP')\n",
    "    \n",
    "    all_scores[i] = (model_tmp.trainRMSE + model_tmp.testRMSE)/2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting ranking \n",
    "ranking = np.argsort(1*all_scores) # Sorting in ascending order\n",
    "sorted_scores = all_scores[ranking]\n",
    "sorted_names = [input_names[i] for i in ranking]\n",
    "\n",
    "# Plotting bar chart\n",
    "plt.figure(figsize=(8, 4))\n",
    "ax = plt.subplot(111)\n",
    "x_pos = np.arange(0,len(sorted_names))\n",
    "plt.bar(x_pos, sorted_scores, color=[0.3, 0.5, 0.6])\n",
    "plt.ylabel(\"RMSE score\")\n",
    "plt.title(\"Variable importance for predicting {}\".format(input_names[2]))\n",
    "ax.text(0.01, 0.98, 'Note: lower scores suggest higher importance',\n",
    "        transform=ax.transAxes, va='top', ha='left', color=[0.3, 0.3, 0.3])\n",
    "plt.xticks(x_pos, sorted_names, rotation=40, ha=\"right\",\n",
    "                 rotation_mode=\"anchor\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose variables to include in model by their indices\n",
    "target_ind = 1\n",
    "model_rain_ind = [0, 1, 2, 3, 4]\n",
    "model_temp_ind = [0]\n",
    "\n",
    "# Compile target data and inputs to arrays\n",
    "target_data = target_ts[target_ind]\n",
    "model_input_data_ref = compile_model_input_variables_lake(model_rain_ind,\n",
    "                                                 model_temp_ind,\n",
    "                                                 rain_pp_all[target_ind],\n",
    "                                                 temp_pp,\n",
    "                                                 target_ts[target_ind].size)\n",
    "\n",
    "# Getting the names of the selected features\n",
    "a1 = [rain_name[i] for i in model_rain_ind]\n",
    "a2 = [temp_name[i] for i in model_temp_ind]\n",
    "input_names = ['All features included', 'All features excl. target', target_name[target_ind]] + a1 + a2\n",
    "\n",
    "\n",
    "# Running MLP a total of tot_idx times, each time using only one of the input features\n",
    "tot_idx = 3 + model_input_data_ref.shape[0]\n",
    "all_scores = np.empty(tot_idx)\n",
    "model_input_data = model_input_data_ref\n",
    "\n",
    "for i in range(tot_idx):\n",
    "    if i > 1:\n",
    "        model_input_data = np.expand_dims(model_input_data_ref[i-3, :], axis=0)\n",
    "        model_tmp = ML_model(target_data=target_data,\n",
    "                                        input_data=model_input_data,\n",
    "                                        use_target=False,\n",
    "                                        look_back=30,\n",
    "                                        chunk_step=20,\n",
    "                                        train_ratio=0.67,\n",
    "                                        num_epochs=30,\n",
    "                                        batch_size=5,\n",
    "                                        predict_steps=1,\n",
    "                                        plot=False,\n",
    "                                        verbose=0)\n",
    "    else:\n",
    "        if i == 0: include_target = True\n",
    "        else: include_target=False\n",
    "        model_input_data = model_input_data_ref         \n",
    "        model_tmp = ML_model(target_data=target_data,\n",
    "                                        input_data=model_input_data,\n",
    "                                        use_target=include_target,\n",
    "                                        look_back=30,\n",
    "                                        chunk_step=20,\n",
    "                                        train_ratio=0.67,\n",
    "                                        num_epochs=30,\n",
    "                                        batch_size=5,\n",
    "                                        predict_steps=1,\n",
    "                                        plot=False,\n",
    "                                        verbose=0)\n",
    "        \n",
    "    model_tmp.run(model_type='MLP')\n",
    "    \n",
    "    all_scores[i] = (model_tmp.trainRMSE + model_tmp.testRMSE)/2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting ranking \n",
    "ranking = np.argsort(1*all_scores) # Sorting in ascending order\n",
    "sorted_scores = all_scores[ranking]\n",
    "sorted_names = [input_names[i] for i in ranking]\n",
    "\n",
    "# Plotting bar chart\n",
    "plt.figure(figsize=(8, 4))\n",
    "ax = plt.subplot(111)\n",
    "x_pos = np.arange(0,len(sorted_names))\n",
    "plt.bar(x_pos, sorted_scores, color=[0.3, 0.5, 0.6])\n",
    "plt.ylabel(\"RMSE score\")\n",
    "plt.title(\"Variable importance for predicting {}\".format(input_names[2]))\n",
    "ax.text(0.01, 0.98, 'Note: lower scores suggest higher importance',\n",
    "        transform=ax.transAxes, va='top', ha='left', color=[0.3, 0.3, 0.3])\n",
    "plt.xticks(x_pos, sorted_names, rotation=40, ha=\"right\",\n",
    "                 rotation_mode=\"anchor\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3 Predictability horizon\n",
    "\n",
    "As for Aquifier Auser in section 2.3.4, an anlysis of the prediction of several timesteps ahead was carried out.\n",
    "The plot below shows the RMSE as a function of the number of days in advance the model predicts, and as expected, the error increases with the number of steps. We hope that by choosing an appropriate threshold for acceptable error, this analysis can give the reader an idea of the predictability horizon of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose variables to include in model by their indices\n",
    "target_ind = 1\n",
    "model_rain_ind = [0, 1, 2, 3, 4]\n",
    "model_temp_ind = [0]\n",
    "\n",
    "# Compile target data and inputs to arrays\n",
    "target_data = target_ts[target_ind]\n",
    "model_input_data = compile_model_input_variables_lake(model_rain_ind,\n",
    "                                                 model_temp_ind,\n",
    "                                                 rain_pp_all[target_ind],\n",
    "                                                 temp_pp,\n",
    "                                                 target_ts[target_ind].size)\n",
    "\n",
    "\n",
    "# Model parameters\n",
    "predict_steps = 15\n",
    "\n",
    "# Run ML model\n",
    "model_day_horizon = []\n",
    "predict_horizon_RMSE = np.empty((len(target_ts), predict_steps))\n",
    "predict_horizon_MSE = np.empty((len(target_ts), predict_steps))\n",
    "for n in range(len(target_ts)):\n",
    "    target_data = target_ts[n]\n",
    "    model_day_horizon.append(ML_model(target_data=target_ts[n],\n",
    "                                input_data=model_input_data,\n",
    "                                use_target=True,\n",
    "                                look_back=40,\n",
    "                                chunk_step=8,\n",
    "                                train_ratio=0.67,\n",
    "                                num_epochs=80,\n",
    "                                batch_size=5,\n",
    "                                predict_steps=predict_steps,\n",
    "                                plot=False,\n",
    "                                verbose=0))\n",
    "    model_day_horizon[n].run(model_type='MLP')\n",
    "    predict_horizon_RMSE[n, :] = model_day_horizon[n].testRMSE\n",
    "    predict_horizon_MSE[n, :] = model_day_horizon[n].testMSE\n",
    "#     if n == 1:\n",
    "#         model_day_horizon[n].plot_predict_all_steps()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot RMSE against prediction look ahead\n",
    "plt.figure(figsize=(8, 6))\n",
    "for n in range(len(target_ts)):\n",
    "    plt.plot(np.arange(1, predict_steps+1), predict_horizon_RMSE[n, :],\n",
    "            label=target_name[n])\n",
    "plt.legend(loc='upper left')\n",
    "plt.title(\"Predictability horizon (RMSE)\")\n",
    "plt.xlabel(\"Steps in advance [days]\")\n",
    "plt.ylabel(\"RMSE score\")\n",
    "plt.ylim([0, 1.25*np.max(predict_horizon_RMSE)])\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusions \n",
    "\n",
    "This notebook provides a statistical and predictive analysis of water data provided by the ACEA group for the following 4 sources types: aquifier, river, lake and spring. Here we summarize the key physical insights, and refer the reader back to the rest of the notebook for detailed discussion about the machine learning model performance. \n",
    "\n",
    "The work reveals that for Aquifier Auser, rainfall is a key factor in explaining the depth to ground water, which fits with the intuitive understanding that rain water is a direct supply to these sources. The relationship between rainfall and target state variables is most clearly visible when applying a convolutional filter with an exponential decay window with a characteristic timescale $\\tau$. The latter physically represents the time for water to drain away from the source, which increases as a function of the depth of the groundwater system. \n",
    "\n",
    "The hydrometry of River Arno also displayed the exponential decay behavior observed for groundwater level in Aquifier Auser. Hence, the same pre-processing was applied to the rainfall data, giving optimal $\\tau$ values of ~ 45 days. We note however that the hydrometry data was much spikier than depth to ground water, and those spikes were difficult to capture when excluding past target data from the inputs. Perhaps our model would benefit from also including raw (unsmoothed) rainfall data in this case. \n",
    "\n",
    "The flow rate of Spring Amiata was not found to be well correlated with rainfall, for any of the time constants considered. Nevertheless, the rainfall was still a useful variable in the machine learnt prediction. Furthermore, the flow rate displayed step-like changes, which were possibly governed by human decisions based on water supply/demand. The depth to groundwater was well correlated with the spring’s flow rate, although the direction of correlation is not clear. A lag analysis, along with physical insight, could potentially reveal more about the mechanism and direction of the correlation. \n",
    "\n",
    "Lake Bilancino’s level was strongly influenced by rainfall, with a $\\tau$ value on the order of ~ 150 days. As with Spring Amiata however, the correlation between flow rate and rainfall was weak for all $\\tau$ values, suggesting that the exponential model is perhaps not the best suited here. Nevertheless, the machine learning model was able to make use of rainfall in the prediction, which means that the two variables are indeed connected. In this dataset, the flow rate also showed occasional large spikes, which proved difficult to predict, particularly when excluding past target data from the inputs. Perhaps our model would benefit from also including raw (unsmoothed) rainfall data in this case.\n",
    "\n",
    "The machine learning model produces reasonably good predictions, and it often doesn’t need all input variables to do so. Including past target data helps improves performance, particularly with respect to short term fluctuations. The predictive skill drops with the number of steps in advance we wish to infer, but this could likely be improved by including weather forecasts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
