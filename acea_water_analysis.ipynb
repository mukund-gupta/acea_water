{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACEA WATER ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from pathlib import Path\n",
    "from statsmodels.tsa.stattools import ccf\n",
    "from scipy import stats\n",
    "\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "plt.style.use('seaborn-muted')\n",
    "plt.rcParams['font.family'] = 'Arial'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing functions\n",
    "\n",
    "def preprocess(df, col_ind, start_ind=0):\n",
    "    \"\"\"\n",
    "    Some basic preprocessing of data from selected column of dataframe.\n",
    "    This will probably need to be thought about more to deal with NaNs\n",
    "    more effectively\n",
    "    \n",
    "    \"\"\"\n",
    "    pd_series = df.iloc[start_ind:, col_ind]\n",
    "    pd_values = pd_series.to_numpy()\n",
    "    # max_value = np.max(np.abs(pd_values))\n",
    "    # pd_values = pd_values / max_value\n",
    "    name = df.columns[col_ind]\n",
    "    \n",
    "    return pd_values, name\n",
    "\n",
    "\n",
    "def preprocess_int(df, col_ind, start_ind=3955):\n",
    "    \"\"\"\n",
    "    Some basic preprocessing of data from selected column of dataframe.\n",
    "    This will probably need to be thought about more to deal with NaNs\n",
    "    more effectively\n",
    "    \n",
    "    \"\"\"\n",
    "    pd_series = df.iloc[start_ind:, col_ind]\n",
    "    pd_series = pd_series.interpolate(method='linear')\n",
    "    pd_series = pd_series.fillna(0)\n",
    "    pd_values = pd_series.to_numpy()\n",
    "    # max_value = np.max(np.abs(pd_values))\n",
    "    # pd_values = pd_values / max_value\n",
    "    name = df.columns[col_ind]\n",
    "    \n",
    "    return pd_values, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical functions \n",
    "\n",
    "def spearman_lag(data1, data2, lag):\n",
    "    \"\"\"Calculate Spearman's rank correlation coefficient between 2 datasets,\n",
    "    with a lag applied to data2\"\"\"\n",
    "    \n",
    "    data_length = data1.size\n",
    "    if lag > 0:\n",
    "        data2_lag = np.zeros(data_length)\n",
    "        data2_lag[lag:] = data2[:-lag] \n",
    "        data2_lag[:lag] = data2[0]\n",
    "    else:\n",
    "        data2_lag = data2\n",
    "    src, _ = stats.spearmanr(data1, data2_lag)\n",
    "    \n",
    "    return src\n",
    "\n",
    "\n",
    "def cross_corr_lag(data1, data2, lag_array=None):\n",
    "    \"\"\"Calculate Spearman's rank correlation coefficient between 2 datasets,\n",
    "    for a range of different lags applied to data2\"\"\"\n",
    "    if lag_array is None:\n",
    "        lag_array = np.arange(data1.size)\n",
    "    crosscorr_lag = np.empty(len(lag_array))\n",
    "    for n in range(len(lag_array)):\n",
    "        crosscorr_lag[n] = spearman_lag(data1, data2, lag=lag_array[n])\n",
    "        \n",
    "    return crosscorr_lag, lag_array\n",
    "\n",
    "\n",
    "def moving_average(x, w):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w\n",
    "\n",
    "\n",
    "def normalise_0_to_1(signal):\n",
    "    sig_min = np.min(signal)\n",
    "    sig_max = np.max(signal)\n",
    "    sig_norm = (signal - sig_min) / (sig_max - sig_min)\n",
    "    return sig_norm\n",
    "\n",
    "\n",
    "def find_datatypes(df):\n",
    "    \"\"\"Find the indices of each pf the different datatypes in the dataframe\"\"\"\n",
    "    names = df.columns\n",
    "    datatypes = ['Rainfall',\n",
    "                 'Depth_to_Groundwater',\n",
    "                 'Temperature',\n",
    "                 'Volume',\n",
    "                 'Hydrometry',\n",
    "                 'Flow_rate',\n",
    "                 'Lake_level']\n",
    "    col_inds = []\n",
    "    \n",
    "    for n in range(len(datatypes)):\n",
    "        col_ind_type = []\n",
    "        for c in range(len(names)):\n",
    "            if datatypes[n] in names[c]:\n",
    "                col_ind_type.append(c)\n",
    "        col_inds.append(col_ind_type)\n",
    "        \n",
    "    return datatypes, col_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting functions \n",
    "\n",
    "def plot_data_preprocessed(df, col_ind):\n",
    "    data_ts, _ = preprocess(df, col_ind)\n",
    "    data_ts_int, _ = preprocess_int(df, col_ind)\n",
    "    data_ts_size = data_ts.size\n",
    "    data_ts_int_size = data_ts_int.size\n",
    "    time_array = np.linspace(0, data_ts_size-1, data_ts_size)\n",
    "    time_array2 = np.linspace(0, data_ts_int_size-1, data_ts_int_size)\n",
    "    plt.figure()\n",
    "    plt.scatter(time_array, data_ts, s=0.2, alpha=0.6)\n",
    "    plt.scatter(time_array2, data_ts_int, s=0.2, alpha=0.6)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tau analysis functions \n",
    "\n",
    "\n",
    "def tau_and_lag_correl(target_ts, rain_ts, lag_array=None, tau_array=None,\n",
    "                       plot=True):\n",
    "    \"\"\"\n",
    "    Calculate correlation coefficients for different time lag and tau\n",
    "    values for rainfall. Optional plot.\n",
    "\n",
    "    \"\"\"\n",
    "    # Set default lag and tau values to analyse\n",
    "    if lag_array is None:\n",
    "        lag_array = np.arange(20)\n",
    "    if tau_array is None:\n",
    "        tau_array = np.linspace(22, 90, 35).astype(np.int)\n",
    "        \n",
    "    # Calculate Spearman's Rank coefficient for each tau and lag combination\n",
    "    sp_rank_cc = []\n",
    "    for n in range(len(lag_array)):\n",
    "        lag = lag_array[n]\n",
    "        data_length = rain_ts.size\n",
    "        if lag > 0:\n",
    "            data_lag = np.zeros(data_length)\n",
    "            data_lag[lag:] = rain_ts[:-lag]\n",
    "            data_lag[:lag] = rain_ts[0]\n",
    "        else:\n",
    "            data_lag = rain_ts\n",
    "        src, _ = find_tau_correlation(target_ts, data_lag,\n",
    "                                      tau_array=tau_array)\n",
    "        sp_rank_cc.append(src)\n",
    "    sp_rank_cc = np.asarray(sp_rank_cc)\n",
    "    \n",
    "    # Plot matrix showing correlation for each tau and lag combination\n",
    "    if plot:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        cax = ax.matshow(sp_rank_cc)\n",
    "        fig.colorbar(cax)\n",
    "        ax.set_xticks(np.arange(len(tau_array)))\n",
    "        ax.set_yticks(np.arange(len(lag_array)))\n",
    "        ax.set_xticklabels(tau_array)\n",
    "        ax.set_yticklabels(lag_array)\n",
    "        ax.xaxis.set_ticks_position('bottom')\n",
    "        plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n",
    "                 rotation_mode=\"anchor\")\n",
    "        ax.set_xlabel('Tau for exponential window')\n",
    "        ax.set_ylabel('Time lag [days]')\n",
    "        plt.title('Correlation of rainfall with target for different values of tau and time lag')\n",
    "        fig.show()\n",
    "        \n",
    "    return sp_rank_cc, lag_array, tau_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM functions \n",
    "\n",
    "def create_dataset(dataset, look_back=1, chunk_step=1, predict_steps=1):\n",
    "    \"\"\"\n",
    "    Convert data set into a dataset matrix by splitting the data into a number of 'chunks'\n",
    "    that are randomly ordered. Each chunk has an input (x) of length 'look_back', and an\n",
    "    output (y) of length 'predict_steps'. The indices of each chunk within the original array\n",
    "    are stored in 'y_ind'.\n",
    "    The shape of the input and output arrays is as follows:\n",
    "    dataset: (num features, num timesteps)\n",
    "    x:       (num chunks, look_back, num features)\n",
    "    y:       (num chunks, predict_steps)\n",
    "    y_ind:   (num chunks)\n",
    "    \n",
    "    \"\"\"\n",
    "    numchunk = int(np.floor((dataset.shape[1] - look_back - 1) / chunk_step))\n",
    "    dataX = np.empty((numchunk, look_back, dataset.shape[0]))\n",
    "    dataY = np.empty((numchunk, predict_steps))\n",
    "    y_ind = []\n",
    "    \n",
    "    # Create chunks of data with the specified look back\n",
    "    for i in range(numchunk):\n",
    "        start_ind = chunk_step*i\n",
    "        dataX[i, :, :] = dataset[:, start_ind:(start_ind + look_back)].T\n",
    "        dataY[i, :] = dataset[0,start_ind+look_back:start_ind+look_back+predict_steps] #MG\n",
    "        #dataY.append(dataset[0, start_ind + look_back])\n",
    "        y_ind.append(start_ind + look_back)\n",
    "        \n",
    "    # Randomise order of chunks\n",
    "    rand_indices = np.random.permutation(numchunk)\n",
    "    x = np.array(dataX)\n",
    "    y = np.array(dataY)\n",
    "    y_ind = np.array(y_ind)\n",
    "    x = x[rand_indices, :]\n",
    "    y = y[rand_indices,:]\n",
    "    #y = np.reshape(y, (y.size, 1)) # MG\n",
    "    y_ind = y_ind[rand_indices]\n",
    "    \n",
    "    return x, y, y_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing data\n",
    "\n",
    "# Importing all data\n",
    "foldpath = r\"acea-water-prediction\"\n",
    "files = list(Path(foldpath).rglob('*.csv'))\n",
    "df = pd.read_csv(files[0]) # Create dataframe\n",
    "\n",
    "# Get time series data for target variable and some other variables\n",
    "# Specified by column index of the pandas dataframe\n",
    "datatypes, col_inds = find_datatypes(df)\n",
    "col_ind_target = 13\n",
    "col_targets = col_inds[1]\n",
    "col_ind_others = [1, 15, 16]\n",
    "col_rain = col_inds[0]\n",
    "col_vol = col_inds[3]\n",
    "\n",
    "# Get target time series data\n",
    "target_ts, target_name = preprocess_int(df, col_ind_target)\n",
    "target_length = target_ts.size\n",
    "\n",
    "# Get time series data for all targets\n",
    "all_target_ts = []\n",
    "all_target_name = []\n",
    "for n in range(len(col_targets)):\n",
    "    ts_, name_ = preprocess_int(df, col_targets[n])\n",
    "    all_target_ts.append(ts_)\n",
    "    all_target_name.append(name_)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get time series data for all rain variables\n",
    "rain_ts = []\n",
    "rain_name = []\n",
    "for n in range(len(col_rain)):\n",
    "    ts_, name_ = preprocess_int(df, col_rain[n])\n",
    "    rain_ts.append(ts_)\n",
    "    rain_name.append(name_)\n",
    "    \n",
    "# Get time series data for all volume variables\n",
    "vol_ts = []\n",
    "vol_name = []\n",
    "for n in range(len(col_vol)):\n",
    "    ts_, name_ = preprocess_int(df, col_vol[n])\n",
    "    vol_ts.append(ts_)\n",
    "    vol_name.append(name_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling gaps & anomalies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove zeros from target\n",
    "# N.B. SHOULD REPLACE THIS WITH A DIFFERENT METHOD\n",
    "for m in range(len(all_target_ts)):\n",
    "    for n in range(len(all_target_ts[m])):\n",
    "        if n > 0 and all_target_ts[m][n] == 0:\n",
    "            all_target_ts[m][n] = all_target_ts[m][n-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rainfall\n",
    "Rainfall was expected to be one of the most useful variables in predicting groundwater levels, since it is one the key physical mechanisms that drives the inflow of water into aquifiers. From a visual inspection of the data, it was clear that many of the peaks in rainfall correspond approximately to peaks in the groundwater level. This was seen to apply both to the longer term trend, as well as on a smaller day-by-day scale. However, while the peaks in rainfall are often immediately followed by days of zero rainfall, the peaks in the groundwater data seem to decay more slowly over time.\n",
    "\n",
    "This led to the hypothesis that the rainfall on a particular day adds to the groundwater level relatively quickly, and then this level decays more slowly over several weeks. The physical explanation for this seems to be logical - if the rate of the rainfall is significantly larger than the rate that the water drains out of the groundwater subsystem, then the level will rise quickly and fall slowly (similar to the level of a slowly leaking cup when water is poured into it). For shallower subsystems, we may expect to see more short term reactions to the rainfall, as it would reach the subsystem more quickly and drain out more quickly; and conversely for deeper subsystems, we would expect the levels to react more slowly, with smaller short term effects.\n",
    "\n",
    "To test this hypothesis, the rainfall data was convolved with an exponential window function, for a range of different time constants. The correlation of these convolved signals with the 'depth to groundwater' variables was calculated to find the time constant that gave the best correlation for each combination of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Rainfall data analysis\n",
    "\n",
    "def exp_convolve(data, tau, winlength=None):\n",
    "    \"\"\"\n",
    "    Convolve input data with an exponential window function (time constant = tau)\n",
    "    \n",
    "    \"\"\"\n",
    "    data = np.asarray(data)\n",
    "    len_data = data.size\n",
    "    if winlength is None:\n",
    "        winlength = len_data\n",
    "    t = np.linspace(0, winlength-1, winlength)\n",
    "    exp_window = np.exp(-t / tau)\n",
    "    exp_window = exp_window / np.sum(exp_window)\n",
    "    data_conv = np.convolve(data, exp_window, 'full')[:len_data]\n",
    "\n",
    "    return data_conv\n",
    "\n",
    "\n",
    "def plot_rainfall_conv_comparison(rain_ind, target_ind, tau, rain_name, target_name,\n",
    "                                  crop_min=1600, crop_max=2000):\n",
    "    \"\"\"\n",
    "    Plot example of rainfall raw data compared to rainfall convolved with exponential window\n",
    "    \n",
    "    \"\"\"\n",
    "    rain_example = rain_ts[rain_ind]\n",
    "    rain_conv_example = exp_convolve(rain_example, tau=tau)\n",
    "    cmap = plt.get_cmap('Dark2')\n",
    "    colors = [cmap(i) for i in np.linspace(0, 1, 8)]\n",
    "    len_data = rain_example.size\n",
    "    fig, ax = plt.subplots(3, 2, sharey=True, figsize=(9, 4))\n",
    "    y_max = 1.2\n",
    "    \n",
    "    ax[1, 0].set_ylim(0, y_max)\n",
    "    ax[0, 0].plot(normalise_0_to_1(rain_example), color=colors[0], alpha=0.8)\n",
    "    ax[1, 0].plot(normalise_0_to_1(rain_conv_example), color=colors[1], alpha=0.8)\n",
    "    ax[2, 0].plot(normalise_0_to_1(all_target_ts[target_ind]), color=colors[2], alpha=0.8)\n",
    "    ax[0, 1].text(0.01, 0.95, 'Rainfall, raw data', transform=ax[0, 1].transAxes, va='top', ha='left')\n",
    "    ax[1, 1].text(0.01, 0.95, 'Rainfall, convolved (tau={})'.format(np.int(tau)),\n",
    "                  transform=ax[1, 1].transAxes, va='top', ha='left')\n",
    "    ax[2, 1].text(0.01, 0.95, 'Depth to Groundwater', transform=ax[2, 1].transAxes, va='top', ha='left')\n",
    "    \n",
    "    xmin = crop_min\n",
    "    xmax = crop_max\n",
    "    for n in range(3):\n",
    "        ax[n, 0].plot([xmin, xmin], [0, y_max], 'k--', alpha=0.6)\n",
    "        ax[n, 0].plot([xmax, xmax], [0, y_max], 'k--', alpha=0.6)\n",
    "        ax[n, 0].set_xlim(0, len_data)\n",
    "        ax[n, 1].set_xlim(xmin, xmax)\n",
    "        con = mpl.patches.ConnectionPatch(xyA=[len_data, y_max/2], coordsA=ax[n, 0].transData,\n",
    "                                          xyB=[xmin, y_max/2], coordsB=ax[n, 1].transData,\n",
    "                                          arrowstyle='->')\n",
    "        fig.add_artist(con)\n",
    "        \n",
    "    days = np.arange(xmin, xmax) \n",
    "    ax[0, 1].plot(days, normalise_0_to_1(rain_example)[xmin: xmax], color=colors[0], alpha=0.8)\n",
    "    ax[1, 1].plot(days, normalise_0_to_1(rain_conv_example)[xmin: xmax], color=colors[1], alpha=0.8)\n",
    "    ax[2, 1].plot(days, normalise_0_to_1(all_target_ts[target_ind])[xmin: xmax], color=colors[2], alpha=0.8)\n",
    "    \n",
    "    for n in range(2):\n",
    "        ax[n, 0].axes.xaxis.set_ticklabels([])\n",
    "        ax[n, 1].axes.xaxis.set_ticklabels([])\n",
    "        ax[2, n].set_xlabel('Time [days]')\n",
    "        \n",
    "    plot_title = rain_name[rain_ind] + ' raw and convolved data, compared to ' + all_target_name[target_ind]\n",
    "    fig.suptitle(plot_title)\n",
    "    plt.tight_layout()\n",
    "    fig.show()\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "plot_rainfall_conv_comparison(rain_ind=0, target_ind=1, tau=54, rain_name=rain_name,\n",
    "                              target_name=all_target_name,\n",
    "                              crop_min=1600, crop_max=2000)\n",
    "\n",
    "# plot_rainfall_conv_comparison(rain_ind=0, target_ind=0, tau=1120, rain_name=rain_name,\n",
    "#                               target_name=all_target_name,\n",
    "#                               crop_min=500, crop_max=2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot example of correlation for different tau values\n",
    "\n",
    "def find_tau_correlation(target_ts, rain_ts, tau_array=None):\n",
    "    \"\"\"\n",
    "    Calculate convolution of rainfall data with an exponential window\n",
    "    with time constant tau, for a range of values of tau.\n",
    "    Then determine how correlated these convolved signals are to the\n",
    "    target data by calculating Spearman's rank correlation coefficient\n",
    "    for each value of tau\n",
    "    \n",
    "    \"\"\"\n",
    "    if tau_array is None:\n",
    "        tau_array = np.linspace(2, 120, 60)\n",
    "        \n",
    "    rain_ts = np.asarray(rain_ts)\n",
    "    target_ts = np.asarray(target_ts)\n",
    "    winlength = rain_ts.size\n",
    "    target_len = target_ts.size\n",
    "    \n",
    "    t = np.linspace(0, winlength-1, winlength)\n",
    "    src = np.empty(len(tau_array))\n",
    "    \n",
    "    for n in range(len(tau_array)):\n",
    "        exp_win = np.exp(-t/tau_array[n])\n",
    "        rain_conv = np.convolve(rain_ts, exp_win, 'full')[:target_len]\n",
    "        rain_conv = rain_conv / np.sum(exp_win)\n",
    "        # if n==40:\n",
    "        #     plt.figure()\n",
    "        #     plt.plot(rain_ts, label='rain_ts')\n",
    "        #     plt.plot(rain_conv, label='rain_conv')\n",
    "        #     plt.plot(target_ts, label='target_ts')\n",
    "        #     plt.legend()\n",
    "        #     plt.show()\n",
    "        src[n], _ = stats.spearmanr(target_ts, rain_conv)\n",
    "        \n",
    "    return src, tau_array\n",
    "\n",
    "\n",
    "def find_best_tau(target_ts, rain_ts, plot=True, rain_name=None, target_name=None, tau_array=None):\n",
    "    \"\"\"\n",
    "    Calculate correlation of convolved rainfall signal with the\n",
    "    target signal for different time constants of exponential window\n",
    "    and select the tau value that gives the best correlation.\n",
    "    Optional plot of correlation for different tau values.\n",
    "    \n",
    "    \"\"\"\n",
    "    tau_best = []\n",
    "    if plot: plt.figure()\n",
    "    for n in range(len(rain_ts)):\n",
    "        src, tau_array = find_tau_correlation(\n",
    "                                normalise_0_to_1(target_ts),\n",
    "                                normalise_0_to_1(rain_ts[n]),\n",
    "                                tau_array=tau_array)\n",
    "        tau_best.append(tau_array[np.argmax(src)])\n",
    "        if plot: plt.plot(tau_array, src, label=rain_name[n], lw=1.5, alpha=0.7)  \n",
    "            \n",
    "    if plot:\n",
    "        plt.xlabel(\"Time constant (tau) for convolution with exponential window\")\n",
    "        plt.ylabel(\"Spearman's Rank correlation coefficient\")\n",
    "        title_text = 'Correlation of convolved rainfall data with {} for different tau values'.format(target_name)\n",
    "        plt.title(title_text, wrap=True)\n",
    "        plt.ylim(0, 1)\n",
    "        plt.legend(fontsize=9)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    return tau_best\n",
    "\n",
    "\n",
    "tau_best = find_best_tau(target_ts=all_target_ts[1], rain_ts=rain_ts,\n",
    "                         rain_name=rain_name, plot=True,\n",
    "                         target_name=all_target_name[1],\n",
    "                         tau_array=np.linspace(2, 160, 80))\n",
    "\n",
    "tau_best = find_best_tau(target_ts=all_target_ts[0], rain_ts=rain_ts, plot=True,\n",
    "                         target_name=all_target_name[0],\n",
    "                         rain_name=rain_name,\n",
    "                         tau_array=np.linspace(20, 1600, 80))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating optimum tau for each rainfall/target combination that\n",
    "# maximises the Spearman's Rank correlation coefficient\n",
    "\n",
    "all_tau_best = []\n",
    "for n in range(len(all_target_ts)):\n",
    "    if n == 0:\n",
    "        tau_array = np.linspace(240, 2200, 50)\n",
    "    else:\n",
    "        tau_array = None\n",
    "    tau_best = find_best_tau(all_target_ts[n], rain_ts, plot=False,\n",
    "                             tau_array=tau_array)\n",
    "    all_tau_best.append(tau_best)\n",
    "all_tau_best = np.asarray(all_tau_best)\n",
    "\n",
    "# Plot results\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.08)\n",
    "cbar_min = np.min(all_tau_best)\n",
    "cbar_max = np.max(all_tau_best[1:, :]) + 5\n",
    "norm = mpl.colors.Normalize(vmin=cbar_min, vmax=cbar_max)\n",
    "sc_map = mpl.cm.ScalarMappable(norm=norm, cmap='viridis')\n",
    "ms = ax.imshow(all_tau_best, norm=norm, cmap='viridis')\n",
    "for i in range(len(all_target_name)):\n",
    "    for j in range(len(rain_name)):\n",
    "        if (cbar_max - all_tau_best[i, j]) / (cbar_max - cbar_min) < 0.2:\n",
    "            text_col = 'k'\n",
    "        else:\n",
    "            text_col = 'w'\n",
    "        text = ax.text(j, i, np.int(all_tau_best[i, j]),\n",
    "                       ha=\"center\", va=\"center\", color=text_col,\n",
    "                       fontsize=8)\n",
    "fig.suptitle('Optimum Tau value for exponential window')\n",
    "ax.set_xticks(np.arange(len(rain_name)))\n",
    "ax.set_yticks(np.arange(len(all_target_name)))\n",
    "ax.set_xticklabels(rain_name)\n",
    "ax.set_yticklabels(all_target_name)\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "plt.setp(ax.get_xticklabels(), rotation=60, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "plt.colorbar(mappable=sc_map, cax=cax, extend='max')\n",
    "plt.tight_layout()\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rainfall data preprocessing\n",
    "tau_best = find_best_tau(all_target_ts[target_ind], rain_ts, plot=False,\n",
    "                         rain_name=rain_name)\n",
    "tau = tau_best[0]\n",
    "exp_win = np.exp(-t/tau)\n",
    "exp_win = exp_win / np.sum(exp_win)\n",
    "rain_len = rain_ts[0].size\n",
    "rain_conv = []\n",
    "for n in range(len(rain_ts)):\n",
    "    conv = np.convolve(rain_ts[n], exp_win, 'full')[:rain_len]\n",
    "    rain_conv.append(conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plt.figure()\n",
    "# vol_total = normalise_0_to_1(np.sum(vol_ts, 0))\n",
    "# vol_total_deviation = vol_total - np.convolve(vol_total, np.ones(500), 'same') / 500\n",
    "# vol_dev_smooth = np.convolve(vol_total_deviation, np.ones(10), 'same') / 10\n",
    "# target_total = np.sum(all_target_ts, 0)\n",
    "# # plt.plot(vol_total, label='total vol', lw=1, alpha=0.7)\n",
    "# # plt.plot(vol_total_deviation, label='vol deviation', lw=1, alpha=0.7)\n",
    "# plt.plot(normalise_0_to_1(vol_dev_smooth), label='vol dev smooth', lw=1, alpha=0.7)\n",
    "# plt.plot(normalise_0_to_1(exp_model), label='exp model', lw=1, alpha=0.7)\n",
    "# plt.plot(normalise_0_to_1(target_total[:-40]), label='total groundwater', lw=1, alpha=0.7)\n",
    "# plt.legend()\n",
    "# plt.title('Time series')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Smoothing for deeper groundwater?\n",
    "\"\"\"\n",
    "# Analysis of how lag applied to rainfall affects correlation with target\n",
    "lag_array = np.arange(200)\n",
    "plt.figure()\n",
    "for n in range(len(rain_ts)):\n",
    "    crosscorr_lag, lag_array = cross_corr_lag(\n",
    "                                    normalise_0_to_1(all_target_ts[2]),\n",
    "                                    normalise_0_to_1(rain_ts[n]),\n",
    "                                    lag_array=lag_array)\n",
    "    plt.plot(lag_array, crosscorr_lag, label=rain_name[n])\n",
    "plt.legend()\n",
    "plt.xlabel('Lag applied to rainfall data [days]')\n",
    "plt.ylabel('Spearman''s rank correlation coefficient')\n",
    "plt.title('Correlation of rainfall with target for different lag amounts')\n",
    "plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "target_test_ind = 0\n",
    "rain_test_ind = 0\n",
    "# lag_array = np.arange(30)\n",
    "# tau_array = np.linspace(12, 90, 40).astype(np.int)\n",
    "lag_array = np.linspace(0, 60, 31).astype(np.int)\n",
    "tau_array = np.linspace(40, 2000, 50).astype(np.int)\n",
    "data1 = all_target_ts[target_test_ind]\n",
    "data2 = rain_ts[rain_test_ind]\n",
    "sp_rank_cc, lag_array, tau_array = tau_and_lag_correl(data1,\n",
    "                                                      data2,\n",
    "                                                      lag_array=lag_array,\n",
    "                                                      tau_array=tau_array,\n",
    "                                                      plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM analysis\n",
    "\n",
    "# Model parameters\n",
    "target_ind = 1\n",
    "look_back = 30\n",
    "chunk_step = 50\n",
    "train_ratio = 0.67\n",
    "num_epochs = 30\n",
    "batch_size = 5\n",
    "predict_steps = 3\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "# Calculate differential of target & rainfall\n",
    "target_diff = np.diff(all_target_ts[target_ind])\n",
    "rain_conv_diff = []\n",
    "for n in range(len(rain_ts)):\n",
    "    rain_conv_diff.append(np.diff(rain_conv[n]))\n",
    "\n",
    "# Normalize the datasets\n",
    "target_data = target_diff\n",
    "all_target_ts[target_ind] = all_target_ts[target_ind][1:]\n",
    "target_data = np.reshape(target_data, (target_data.size, 1))\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "target_scaled = scaler.fit_transform(target_data)\n",
    "target_scaled = np.squeeze(target_scaled)\n",
    "rain_scaled = normalise_0_to_1(rain_conv_diff[0])\n",
    "\n",
    "# Combine the dataset into single array\n",
    "dataset = np.stack((target_scaled, rain_scaled))\n",
    "\n",
    "# Plot presprocessed data\n",
    "plt.figure()\n",
    "plt.plot(dataset[0, :], label=all_target_name[target_ind])\n",
    "plt.plot(dataset[1, :], label=rain_name[0])\n",
    "plt.legend()\n",
    "plt.title('Preprocessed data')\n",
    "plt.show()\n",
    "\n",
    "# Split data into chunks with random order\n",
    "x, y, y_ind = create_dataset(dataset, look_back=look_back,\n",
    "                             chunk_step=chunk_step,predict_steps=predict_steps)\n",
    "numchunk = y.shape[0]\n",
    "\n",
    "# split into train and test sets\n",
    "train_size = int(numchunk * train_ratio)\n",
    "test_size = numchunk - train_size\n",
    "trainX, testX = x[0:train_size, :, :], x[train_size:numchunk, :, :]\n",
    "trainY, testY = y[0:train_size, :], y[train_size:numchunk, :]\n",
    "trainYind, testYind = y_ind[0:train_size], y_ind[train_size:numchunk]\n",
    "\n",
    "# Plot one example of chunk\n",
    "sample_num = 10\n",
    "plt.figure()\n",
    "time_ = np.arange(look_back)\n",
    "plt.plot(time_, trainX[sample_num, :, 0], label='Input: target')\n",
    "plt.plot(time_, trainX[sample_num, :, 1], label='Input: rain')\n",
    "plt.plot([look_back], trainY[sample_num, 0], 'xr', label='Output: target')\n",
    "plt.legend()\n",
    "plt.title(\"Example of one chunk from dataset\")\n",
    "plt.show()\n",
    "\n",
    "# create and fit the LSTM network\n",
    "num_features = x.shape[2]\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(look_back, num_features)))\n",
    "model.add(Dense(predict_steps))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=num_epochs, batch_size=batch_size, verbose=1)\n",
    "\n",
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform(trainY)\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform(testY)\n",
    "\n",
    "# Convert from diff to actual prediction\n",
    "for n in range(train_size):\n",
    "    prev_day = all_target_ts[target_ind][trainYind[n] - 1]\n",
    "    trainY[n] = prev_day + trainY[n]\n",
    "    trainPredict[n] = prev_day + trainPredict[n]\n",
    "for n in range(test_size):\n",
    "    prev_day = all_target_ts[target_ind][testYind[n] - 1]\n",
    "    testY[n] = prev_day + testY[n]\n",
    "    testPredict[n] = prev_day + testPredict[n]\n",
    "\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[:, 0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[:, 0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot one prediction example from test sample\n",
    "pred_sample = 4\n",
    "pred_ind = testYind[pred_sample]\n",
    "plt.figure()\n",
    "sample_t = np.linspace(pred_ind - look_back - 1, pred_ind - 1, look_back)\n",
    "sample_t = sample_t.astype(np.int)\n",
    "plt.plot(sample_t, all_target_ts[target_ind][sample_t[0]:sample_t[-1]])\n",
    "plt.plot(pred_ind, testPredict[pred_sample, 0], 'xr')\n",
    "plt.title(\"Example of one prediction from test data\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all predictions from test samples against original data\n",
    "plt.figure()\n",
    "plt.plot(np.linspace(0, all_target_ts[target_ind].size-1,\n",
    "                     all_target_ts[target_ind].size),\n",
    "         all_target_ts[target_ind],\n",
    "         label='Original data')\n",
    "plt.plot(testYind, testPredict[:, 0], 'xr', label='Test predictions')\n",
    "plt.plot(trainYind, trainPredict[:, 0], 'xg', label='Train predictions')\n",
    "plt.legend()\n",
    "plt.title(\"Predictions compared to original dataset\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all multistep predictions from test samples against original data\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.linspace(0, target_data.size-1, target_data.size), target_data,\n",
    "         label='Original data')\n",
    "for ii in range(len(testYind)):\n",
    "    plt.plot(np.arange(testYind[ii],testYind[ii] + predict_steps), testPredict[ii, :],color='k')\n",
    "plt.legend()\n",
    "plt.title(\"Predictions compared to original dataset\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
