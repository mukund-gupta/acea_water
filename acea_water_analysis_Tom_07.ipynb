{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACEA WATER ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# %matplotlib notebook\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from pathlib import Path\n",
    "from statsmodels.tsa.stattools import ccf\n",
    "from scipy import stats\n",
    "\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "plt.style.use('seaborn-muted')\n",
    "plt.rcParams['font.family'] = 'Arial'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing functions\n",
    "\n",
    "def preprocess(df, col_ind, start_ind=0):\n",
    "    \"\"\"\n",
    "    Some basic preprocessing of data from selected column of dataframe.\n",
    "    This will probably need to be thought about more to deal with NaNs\n",
    "    more effectively\n",
    "    \n",
    "    \"\"\"\n",
    "    pd_series = df.iloc[start_ind:, col_ind]\n",
    "    pd_values = pd_series.to_numpy()\n",
    "    # max_value = np.max(np.abs(pd_values))\n",
    "    # pd_values = pd_values / max_value\n",
    "    name = df.columns[col_ind]\n",
    "    \n",
    "    return pd_values, name\n",
    "\n",
    "\n",
    "def preprocess_int(df, col_ind, start_ind=0, fill_zeros=True):\n",
    "    \"\"\"\n",
    "    Some basic preprocessing of data from selected column of dataframe.\n",
    "    This will probably need to be thought about more to deal with NaNs\n",
    "    more effectively\n",
    "    \n",
    "    \"\"\"\n",
    "    pd_series = df.iloc[start_ind:, col_ind]\n",
    "    if fill_zeros:\n",
    "        pd_series.replace(0, np.nan)\n",
    "    pd_series = pd_series.interpolate(method='linear')\n",
    "    pd_series = pd_series.fillna(method='bfill')\n",
    "    pd_series = pd_series.fillna(method='ffill')\n",
    "    pd_values = pd_series.to_numpy()\n",
    "    # max_value = np.max(np.abs(pd_values))\n",
    "    # pd_values = pd_values / max_value\n",
    "    name = df.columns[col_ind]\n",
    "    \n",
    "    return pd_values, name\n",
    "\n",
    "\n",
    "def get_time_series(col_inds, start_ind=0, fill_zeros=True):\n",
    "    \"\"\"\n",
    "    Get all of the time series from the specified column indices and their names\n",
    "    and populate lists for each\n",
    "    \n",
    "    \"\"\"\n",
    "    ts = []\n",
    "    name = []\n",
    "    \n",
    "    for n in range(len(col_inds)):\n",
    "        ts_, name_ = preprocess_int(df, col_inds[n], start_ind=start_ind, fill_zeros=fill_zeros)\n",
    "        ts.append(ts_)\n",
    "        name.append(name_)\n",
    "        \n",
    "    return ts, name\n",
    "\n",
    "\n",
    "def find_data_gaps(dataframe, plot=True, title=None):\n",
    "    \"\"\"\n",
    "    Finds NaNs and zeros in dataframe, with optional plot to show their locations\n",
    "    \n",
    "    \"\"\"\n",
    "    # Put dataframe into numpy array, ignoring date variable\n",
    "    df_nodate = df.drop(['Date'], axis=1)\n",
    "    all_data = df_nodate.to_numpy(na_value=np.nan)\n",
    "    col_names = df_nodate.columns\n",
    "    numcol = len(col_names)\n",
    "\n",
    "    # Find missing values\n",
    "    is_nan = np.isnan(all_data)\n",
    "    is_zero = (all_data == 0)\n",
    "    nan_array = np.where(is_nan, 1, np.nan)\n",
    "    zero_array = np.where(is_zero, 1, np.nan)\n",
    "\n",
    "    # Plot showing missing values and zeros\n",
    "    if plot:\n",
    "        if title is None: title = 'Location of NaNs and zeros in dataset'\n",
    "        fig = plt.figure(figsize=(9, 5))\n",
    "        ax = plt.subplot(111)\n",
    "        # divider = make_axes_locatable(ax)\n",
    "        # cax = divider.append_axes(\"top\", size=\"5%\", pad=0.08)\n",
    "        norm = mpl.colors.Normalize(vmin=-1, vmax=1)\n",
    "        sc_map = mpl.cm.ScalarMappable(norm=norm, cmap='PiYG')\n",
    "        ms = ax.imshow(nan_array.T, aspect='auto', cmap='Pastel1', interpolation='none')\n",
    "        ms2 = ax.imshow(zero_array.T, aspect='auto', cmap='Set3', interpolation='none')\n",
    "        ax.set_yticks(np.arange(numcol))\n",
    "        ax.set_yticklabels(col_names)\n",
    "        ax.set_xlabel('Time [days]')\n",
    "        cmap_nan = plt.cm.Pastel1\n",
    "        cmap_zero = plt.cm.Set3\n",
    "        custom_lines = [Line2D([0], [0], color=cmap_zero(0.), lw=5),\n",
    "                        Line2D([0], [0], color=cmap_nan(0.), lw=5)]\n",
    "        ax.legend(custom_lines, ['Zero', 'NaN'], loc='lower right')\n",
    "        plt.title(title)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return all_data, col_names, is_nan, is_zero\n",
    "\n",
    "\n",
    "def normalise_sum(signal):\n",
    "    \"\"\"\n",
    "    Normalises a signal so that the sum of all samples is unity.\n",
    "\n",
    "    :param signal: An array of values representing the signal to be normalised.\n",
    "    :returns: An array of values representing the normalised signal.\n",
    "\n",
    "    \"\"\"\n",
    "    signal = np.asarray(signal, dtype=float)\n",
    "    sum_ = np.sum(signal) if signal.any() else 1.\n",
    "\n",
    "    return signal / sum_\n",
    "\n",
    "\n",
    "def movingav(signal, winwidth, winfunc=None):\n",
    "    \"\"\"\n",
    "    Calculates the moving average of a signal using chosen window function.\n",
    "\n",
    "    :param signal: An array of values representing the signal to be smoothed.\n",
    "    :param winwidth: The width of the smoothing window (number of samples).\n",
    "    :param winfunc: The window function to use for smoothing. By default, a\n",
    "        rectangular window will be used.\n",
    "    :returns: An array of values representing the smoothed signal.\n",
    "    :raises ValueError: If window width is negative.\n",
    "    :raises ValueError: If window width exceeds length of input array.\n",
    "\n",
    "    \"\"\"\n",
    "    numsamples = len(signal)\n",
    "\n",
    "    hww = int(winwidth / 2.)\n",
    "    winwidth = 2 * hww + 1\n",
    "\n",
    "    if winwidth < 0:\n",
    "        raise ValueError(\"window width must not be negative\")\n",
    "\n",
    "    if winwidth >= numsamples:\n",
    "        raise ValueError(\"window width must not exceed length of input array\")\n",
    "\n",
    "    win = np.ones(winwidth) if winfunc is None else winfunc(winwidth)\n",
    "\n",
    "    win = normalise_sum(win)\n",
    "\n",
    "    halfwin2 = normalise_sum(win[hww:])\n",
    "    halfwin1 = normalise_sum(win[:hww+1])\n",
    "\n",
    "    valstart = np.dot(halfwin2, signal[0:hww+1])\n",
    "    valend = np.dot(halfwin1, signal[numsamples - hww - 1:])\n",
    "\n",
    "    signal = np.concatenate((np.ones(hww) * valstart, signal,\n",
    "                             np.ones(hww) * valend))\n",
    "\n",
    "    wpos = hww\n",
    "    wend = len(signal) - hww - 1\n",
    "\n",
    "    sig_smooth = np.empty(numsamples)\n",
    "\n",
    "    while wpos <= wend:\n",
    "        sig_smooth[wpos - hww] = np.dot(signal[wpos-hww: wpos+hww+1], win)\n",
    "        wpos += 1\n",
    "\n",
    "    return sig_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical functions \n",
    "\n",
    "def spearman_lag(data1, data2, lag):\n",
    "    \"\"\"Calculate Spearman's rank correlation coefficient between 2 datasets,\n",
    "    with a lag applied to data2\"\"\"\n",
    "    \n",
    "    data_length = data1.size\n",
    "    if lag > 0:\n",
    "        data2_lag = np.zeros(data_length)\n",
    "        data2_lag[lag:] = data2[:-lag] \n",
    "        data2_lag[:lag] = data2[0]\n",
    "    else:\n",
    "        data2_lag = data2\n",
    "    src, _ = stats.spearmanr(data1, data2_lag)\n",
    "    \n",
    "    return src\n",
    "\n",
    "\n",
    "def cross_corr_lag(data1, data2, lag_array=None):\n",
    "    \"\"\"Calculate Spearman's rank correlation coefficient between 2 datasets,\n",
    "    for a range of different lags applied to data2\"\"\"\n",
    "    if lag_array is None:\n",
    "        lag_array = np.arange(data1.size)\n",
    "    crosscorr_lag = np.empty(len(lag_array))\n",
    "    for n in range(len(lag_array)):\n",
    "        crosscorr_lag[n] = spearman_lag(data1, data2, lag=lag_array[n])\n",
    "        \n",
    "    return crosscorr_lag, lag_array\n",
    "\n",
    "\n",
    "def moving_average(x, w):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w\n",
    "\n",
    "\n",
    "def normalise_0_to_1(signal):\n",
    "    sig_min = np.min(signal)\n",
    "    sig_max = np.max(signal)\n",
    "    sig_norm = (signal - sig_min) / (sig_max - sig_min)\n",
    "    return sig_norm\n",
    "\n",
    "\n",
    "def exp_convolve(data, tau, winlength=None):\n",
    "    \"\"\"\n",
    "    Convolve input data with an exponential window function (time constant = tau)\n",
    "    \n",
    "    \"\"\"\n",
    "    data = np.asarray(data)\n",
    "    len_data = data.size\n",
    "    if winlength is None:\n",
    "        winlength = len_data\n",
    "    t = np.linspace(0, winlength-1, winlength)\n",
    "    exp_window = np.exp(-t / tau)\n",
    "    exp_window = exp_window / np.sum(exp_window)\n",
    "    data_conv = np.convolve(data, exp_window, 'full')[:len_data]\n",
    "\n",
    "    return data_conv\n",
    "\n",
    "\n",
    "def find_datatypes(df):\n",
    "    \"\"\"Find the indices of each pf the different datatypes in the dataframe\"\"\"\n",
    "    names = df.columns\n",
    "    datatypes = ['Rainfall',\n",
    "                 'Depth_to_Groundwater',\n",
    "                 'Temperature',\n",
    "                 'Volume',\n",
    "                 'Hydrometry',\n",
    "                 'Flow_rate',\n",
    "                 'Lake_level']\n",
    "    col_inds = []\n",
    "    \n",
    "    for n in range(len(datatypes)):\n",
    "        col_ind_type = []\n",
    "        for c in range(len(names)):\n",
    "            if datatypes[n] in names[c]:\n",
    "                col_ind_type.append(c)\n",
    "        col_inds.append(col_ind_type)\n",
    "        \n",
    "    return datatypes, col_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting functions \n",
    "\n",
    "def plot_data_preprocessed(df, col_ind):\n",
    "    data_ts, _ = preprocess(df, col_ind)\n",
    "    data_ts_int, _ = preprocess_int(df, col_ind)\n",
    "    data_ts_size = data_ts.size\n",
    "    data_ts_int_size = data_ts_int.size\n",
    "    time_array = np.linspace(0, data_ts_size-1, data_ts_size)\n",
    "    time_array2 = np.linspace(0, data_ts_int_size-1, data_ts_int_size)\n",
    "    plt.figure()\n",
    "    plt.scatter(time_array, data_ts, s=0.2, alpha=0.6)\n",
    "    plt.scatter(time_array2, data_ts_int, s=0.2, alpha=0.6)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_preprocessing(input_raw, input_pp, input_name, target_ts, target_name,\n",
    "                       crop_min=1600, crop_max=2000, title=None):\n",
    "    \"\"\"\n",
    "    Plot raw data and preprocessed data compared to target variable\n",
    "    \n",
    "    \"\"\"\n",
    "    cmap = plt.get_cmap('Dark2')\n",
    "    colors = [cmap(i) for i in np.linspace(0, 1, 8)]\n",
    "    len_data = input_raw.size\n",
    "    fig, ax = plt.subplots(3, 2, sharey=True, figsize=(9, 4))\n",
    "    y_max = 1.2\n",
    "    \n",
    "    ax[1, 0].set_ylim(0, y_max)\n",
    "    ax[0, 0].plot(normalise_0_to_1(input_raw), color=colors[0], alpha=0.8)\n",
    "    ax[1, 0].plot(normalise_0_to_1(input_pp), color=colors[1], alpha=0.8)\n",
    "    ax[2, 0].plot(normalise_0_to_1(target_ts), color=colors[2], alpha=0.8)\n",
    "    label1 = '{}, raw data'.format(input_name)\n",
    "    label2 = '{}, pre-processed'.format(input_name)\n",
    "    label3 = target_name\n",
    "    ax[0, 1].text(0.01, 0.95, label1, transform=ax[0, 1].transAxes, va='top', ha='left')\n",
    "    ax[1, 1].text(0.01, 0.95, label2, transform=ax[1, 1].transAxes, va='top', ha='left')\n",
    "    ax[2, 1].text(0.01, 0.95, label3, transform=ax[2, 1].transAxes, va='top', ha='left')\n",
    "    \n",
    "    xmin = crop_min\n",
    "    xmax = crop_max\n",
    "    for n in range(3):\n",
    "        ax[n, 0].plot([xmin, xmin], [0, y_max], 'k--', alpha=0.6)\n",
    "        ax[n, 0].plot([xmax, xmax], [0, y_max], 'k--', alpha=0.6)\n",
    "        ax[n, 0].set_xlim(0, len_data)\n",
    "        ax[n, 1].set_xlim(xmin, xmax)\n",
    "        con = mpl.patches.ConnectionPatch(xyA=[len_data, y_max/2], coordsA=ax[n, 0].transData,\n",
    "                                          xyB=[xmin, y_max/2], coordsB=ax[n, 1].transData,\n",
    "                                          arrowstyle='->')\n",
    "        fig.add_artist(con)\n",
    "        \n",
    "    days = np.arange(xmin, xmax) \n",
    "    ax[0, 1].plot(days, normalise_0_to_1(input_raw)[xmin: xmax], color=colors[0], alpha=0.8)\n",
    "    ax[1, 1].plot(days, normalise_0_to_1(input_pp)[xmin: xmax], color=colors[1], alpha=0.8)\n",
    "    ax[2, 1].plot(days, normalise_0_to_1(target_ts)[xmin: xmax], color=colors[2], alpha=0.8)\n",
    "    \n",
    "    for n in range(2):\n",
    "        ax[n, 0].axes.xaxis.set_ticklabels([])\n",
    "        ax[n, 1].axes.xaxis.set_ticklabels([])\n",
    "        ax[2, n].set_xlabel('Time [days]')\n",
    "        \n",
    "    if title is None:\n",
    "        title = input_name + ' raw and pre-processed data, compared to ' + target_name\n",
    "    fig.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def plot_correl_matrix(data, names, title=None):\n",
    "    \"\"\"\n",
    "    Plot cross-correlation matrix for input data, using Spearman's rank\n",
    "    data should have shape: (num variables, num samples)\n",
    "    names should be list of length (num variables)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Calculate cross-correlation\n",
    "    cross_corr, p_value = stats.spearmanr(data.T, nan_policy='omit')\n",
    "    \n",
    "    # Plot results\n",
    "    numdata = len(names)\n",
    "    if title is None:\n",
    "        title = 'Cross-correlation matrix'\n",
    "    fig = plt.figure(figsize=(9, 9))\n",
    "    ax = plt.subplot(111)\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.08)\n",
    "    norm = mpl.colors.Normalize(vmin=-1, vmax=1)\n",
    "    sc_map = mpl.cm.ScalarMappable(norm=norm, cmap='PiYG')\n",
    "    ms = ax.imshow(cross_corr, cmap='PiYG', interpolation='none', vmin=-1, vmax=1)\n",
    "    fig.suptitle(title)\n",
    "    ax.set_xticks(np.arange(numdata))\n",
    "    ax.set_yticks(np.arange(numdata))\n",
    "    ax.set_xticklabels(names)\n",
    "    ax.set_yticklabels(names)\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    plt.setp(ax.get_xticklabels(), rotation=60, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "    plt.colorbar(mappable=sc_map, cax=cax)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "      \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tau analysis functions \n",
    "\n",
    "def find_tau_correlation(target_ts, rain_ts, tau_array=None):\n",
    "    \"\"\"\n",
    "    Calculate convolution of rainfall data with an exponential window\n",
    "    with time constant tau, for a range of values of tau.\n",
    "    Then determine how correlated these convolved signals are to the\n",
    "    target data by calculating Spearman's rank correlation coefficient\n",
    "    for each value of tau\n",
    "    \n",
    "    \"\"\"\n",
    "    if tau_array is None:\n",
    "        tau_array = np.linspace(2, 120, 60)\n",
    "        \n",
    "    rain_ts = np.asarray(rain_ts)\n",
    "    target_ts = np.asarray(target_ts)\n",
    "    winlength = rain_ts.size\n",
    "    target_len = target_ts.size\n",
    "    \n",
    "    t = np.linspace(0, winlength-1, winlength)\n",
    "    src = np.empty(len(tau_array))\n",
    "    \n",
    "    for n in range(len(tau_array)):\n",
    "        exp_win = np.exp(-t/tau_array[n])\n",
    "        rain_conv = np.convolve(rain_ts, exp_win, 'full')[:target_len]\n",
    "        rain_conv = rain_conv / np.sum(exp_win)\n",
    "        # if n==40:\n",
    "        #     plt.figure()\n",
    "        #     plt.plot(rain_ts, label='rain_ts')\n",
    "        #     plt.plot(rain_conv, label='rain_conv')\n",
    "        #     plt.plot(target_ts, label='target_ts')\n",
    "        #     plt.legend()\n",
    "        #     plt.show()\n",
    "        src[n], _ = stats.spearmanr(target_ts, rain_conv)\n",
    "        \n",
    "    return src, tau_array\n",
    "\n",
    "\n",
    "def find_best_tau(target_ts, rain_ts, plot=True, rain_name=None, target_name=None, tau_array=None):\n",
    "    \"\"\"\n",
    "    Calculate correlation of convolved rainfall signal with the\n",
    "    target signal for different time constants of exponential window\n",
    "    and select the tau value that gives the best correlation.\n",
    "    Optional plot of correlation for different tau values.\n",
    "    \n",
    "    \"\"\"\n",
    "    tau_best = []\n",
    "    if plot: plt.figure()\n",
    "    for n in range(len(rain_ts)):\n",
    "        src, tau_array = find_tau_correlation(\n",
    "                                normalise_0_to_1(target_ts),\n",
    "                                normalise_0_to_1(rain_ts[n]),\n",
    "                                tau_array=tau_array)\n",
    "        tau_best.append(tau_array[np.argmax(src)])\n",
    "        if plot: plt.plot(tau_array, src, label=rain_name[n], lw=1.5, alpha=0.7)  \n",
    "            \n",
    "    if plot:\n",
    "        plt.xlabel(\"Time constant (tau) for convolution with exponential window\")\n",
    "        plt.ylabel(\"Spearman's Rank correlation coefficient\")\n",
    "        title_text = 'Correlation of convolved rainfall data with {} for different tau values'.format(target_name)\n",
    "        plt.title(title_text, wrap=True)\n",
    "        plt.ylim(0, 1)\n",
    "        plt.legend(fontsize=9)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    return tau_best\n",
    "\n",
    "\n",
    "def find_all_best_tau(input_ts, input_name, target_ts, target_name):\n",
    "    \"\"\"\n",
    "    Calculating optimum tau for each input/target combination that\n",
    "    maximises the Spearman's Rank correlation coefficient when the input is\n",
    "    convolved with an exponential window (time constant tau)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Find best tau values\n",
    "    all_tau_best = []\n",
    "    for n in range(len(target_ts)):\n",
    "        if n == 0:\n",
    "            tau_array = np.linspace(240, 2200, 50)\n",
    "        else:\n",
    "            tau_array = None\n",
    "        tau_best = find_best_tau(target_ts[n], input_ts, plot=False,\n",
    "                                 tau_array=tau_array)\n",
    "        all_tau_best.append(tau_best)\n",
    "    all_tau_best = np.asarray(all_tau_best)\n",
    "\n",
    "    # Plot results\n",
    "    fig = plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=0.3, pad=0.08)\n",
    "    cbar_min = np.min(all_tau_best)\n",
    "    cbar_max = np.max(all_tau_best[1:, :]) + 5\n",
    "    norm = mpl.colors.Normalize(vmin=cbar_min, vmax=cbar_max)\n",
    "    sc_map = mpl.cm.ScalarMappable(norm=norm, cmap='viridis')\n",
    "    ms = ax.imshow(all_tau_best, norm=norm, cmap='viridis')\n",
    "    for i in range(len(target_name)):\n",
    "        for j in range(len(input_name)):\n",
    "            if (cbar_max - all_tau_best[i, j]) / (cbar_max - cbar_min) < 0.2:\n",
    "                text_col = 'k'\n",
    "            else:\n",
    "                text_col = 'w'\n",
    "            text = ax.text(j, i, np.int(all_tau_best[i, j]),\n",
    "                           ha=\"center\", va=\"center\", color=text_col,\n",
    "                           fontsize=8)\n",
    "    fig.suptitle('Optimum Tau value for exponential window')\n",
    "    ax.set_xticks(np.arange(len(input_name)))\n",
    "    ax.set_yticks(np.arange(len(target_name)))\n",
    "    ax.set_xticklabels(input_name)\n",
    "    ax.set_yticklabels(target_name)\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    plt.setp(ax.get_xticklabels(), rotation=60, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "    plt.colorbar(mappable=sc_map, cax=cax, extend='max')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return all_tau_best\n",
    "\n",
    "\n",
    "def tau_and_lag_correl(target_ts, rain_ts, lag_array=None, tau_array=None,\n",
    "                       plot=True):\n",
    "    \"\"\"\n",
    "    Calculate correlation coefficients for different time lag and tau\n",
    "    values for rainfall. Optional plot.\n",
    "\n",
    "    \"\"\"\n",
    "    # Set default lag and tau values to analyse\n",
    "    if lag_array is None:\n",
    "        lag_array = np.arange(20)\n",
    "    if tau_array is None:\n",
    "        tau_array = np.linspace(22, 90, 35).astype(np.int)\n",
    "        \n",
    "    # Calculate Spearman's Rank coefficient for each tau and lag combination\n",
    "    sp_rank_cc = []\n",
    "    for n in range(len(lag_array)):\n",
    "        lag = lag_array[n]\n",
    "        data_length = rain_ts.size\n",
    "        if lag > 0:\n",
    "            data_lag = np.zeros(data_length)\n",
    "            data_lag[lag:] = rain_ts[:-lag]\n",
    "            data_lag[:lag] = rain_ts[0]\n",
    "        else:\n",
    "            data_lag = rain_ts\n",
    "        src, _ = find_tau_correlation(target_ts, data_lag,\n",
    "                                      tau_array=tau_array)\n",
    "        sp_rank_cc.append(src)\n",
    "    sp_rank_cc = np.asarray(sp_rank_cc)\n",
    "    \n",
    "    # Plot matrix showing correlation for each tau and lag combination\n",
    "    if plot:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        cax = ax.matshow(sp_rank_cc)\n",
    "        fig.colorbar(cax)\n",
    "        ax.set_xticks(np.arange(len(tau_array)))\n",
    "        ax.set_yticks(np.arange(len(lag_array)))\n",
    "        ax.set_xticklabels(tau_array)\n",
    "        ax.set_yticklabels(lag_array)\n",
    "        ax.xaxis.set_ticks_position('bottom')\n",
    "        plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n",
    "                 rotation_mode=\"anchor\")\n",
    "        ax.set_xlabel('Tau for exponential window')\n",
    "        ax.set_ylabel('Time lag [days]')\n",
    "        plt.title('Correlation of rainfall with target for different values of tau and time lag')\n",
    "        fig.show()\n",
    "        \n",
    "    return sp_rank_cc, lag_array, tau_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volume pre-processing\n",
    "\n",
    "def preprocess_vol(vol_data, smooth_amt_1=800, smooth_amt_2=30):\n",
    "    \"\"\"Pre-process volume data by smoothing, calculating deviation from the smoothed data,\n",
    "    and then smoothing again\"\"\"\n",
    "    deviation = vol_data - movingav(vol_data, smooth_amt_1, winfunc=np.hanning)\n",
    "    smoothed = movingav(deviation, smooth_amt_2, winfunc=np.hanning)\n",
    "    return normalise_0_to_1(smoothed)\n",
    "    \n",
    "\n",
    "def find_best_vol_params(vol_ts, target_ts, smoothing_1=None, smoothing_2=None, plot=True):\n",
    "    \"\"\"Calculate the correlation of the preprocessed volume data with the target data for\n",
    "    different lengths of smoothing window, in order to find the most appropropriate settings\"\"\"\n",
    "    if smoothing_1 is None:\n",
    "        smoothing_1 = np.linspace(50, 2000, 40).astype(np.int)\n",
    "    if smoothing_2 is None:\n",
    "        smoothing_2 = np.linspace(2, 40, 20).astype(np.int)\n",
    "    num_smth_1 = smoothing_1.size\n",
    "    num_smth_2 = smoothing_2.size\n",
    "    coeffs = np.empty((num_smth_1, num_smth_2))\n",
    "    for i in range(num_smth_1):\n",
    "        for j in range(num_smth_2):\n",
    "            vol_processed = preprocess_vol(vol_ts, smooth_amt_1=smoothing_1[i],\n",
    "                                           smooth_amt_2=smoothing_2[j])\n",
    "            corr_coeff, _ = stats.spearmanr(vol_processed, target_ts)\n",
    "            coeffs[i, j] = corr_coeff\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        ax = plt.subplot(111)\n",
    "        im = ax.imshow(coeffs.T)\n",
    "#         norm = mpl.colors.Normalize(vmin=0, vmax=1)\n",
    "#         sc_map = mpl.cm.ScalarMappable(norm=norm, cmap='viridis')\n",
    "#         plt.colorbar(mappable=sc_map)\n",
    "        ax.figure.colorbar(im, ax=ax)\n",
    "        ax.set_xticks(np.arange(num_smth_1))\n",
    "        ax.set_yticks(np.arange(num_smth_2))\n",
    "        ax.set_xticklabels(smoothing_1)\n",
    "        ax.set_yticklabels(smoothing_2)\n",
    "        ax.xaxis.set_ticks_position('bottom')\n",
    "        plt.setp(ax.get_xticklabels(), rotation=60, ha=\"right\",\n",
    "                 rotation_mode=\"anchor\")\n",
    "        plt.show()\n",
    "        \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM functions \n",
    "\n",
    "def compile_model_input_variables(model_rain_ind,\n",
    "                                  model_vol_ind,\n",
    "                                  model_temp_ind,\n",
    "                                  model_hydro_ind,\n",
    "                                  rain_pp,\n",
    "                                  vol_pp,\n",
    "                                  temp_pp,\n",
    "                                  hydro_pp):\n",
    "    \"\"\"\n",
    "    Extract the selected variables from the preprocessed data by their indices\n",
    "    and populate an input array to be used by the model.\n",
    "    The input array will have shape: (num features, num timesteps)\n",
    "    \n",
    "    \"\"\"    \n",
    "    num_inputs = len(model_rain_ind) + len(model_vol_ind) + \\\n",
    "                 len(model_temp_ind) + len(model_hydro_ind)\n",
    "    num_timesteps = rain_pp[0].size\n",
    "    model_input_data = np.empty((num_inputs, num_timesteps))\n",
    "    count = 0\n",
    "    for i in model_rain_ind:\n",
    "        model_input_data[count, :] = rain_pp[i]\n",
    "        count += 1\n",
    "    for i in model_vol_ind:\n",
    "        model_input_data[count, :] = vol_pp[i]\n",
    "        count += 1\n",
    "    for i in model_temp_ind:\n",
    "        model_input_data[count, :] = temp_pp[i]\n",
    "        count += 1\n",
    "    for i in model_hydro_ind:\n",
    "        model_input_data[count, :] = hydro_pp[i]\n",
    "        count += 1\n",
    "#     model_rain_data = np.asarray([rain_pp[i] for i in model_rain_ind])\n",
    "#     model_vol_data = np.asarray([vol_pp[i] for i in model_vol_ind])\n",
    "#     model_temp_data = np.asarray([temp_pp[i] for i in model_temp_ind])\n",
    "#     model_hydro_data = np.asarray([hydro_pp[i] for i in model_hydro_ind])\n",
    "    \n",
    "    return model_input_data\n",
    "\n",
    "\n",
    "def create_dataset(dataset, look_back=1, chunk_step=1, predict_steps=1):\n",
    "    \"\"\"\n",
    "    Convert data set into a dataset matrix by splitting the data into a number of 'chunks'\n",
    "    that are randomly ordered. Each chunk has an input (x) of length 'look_back', and an\n",
    "    output (y) of length 'predict_steps'. The indices of each chunk within the original array\n",
    "    are stored in 'y_ind'.\n",
    "    The shape of the input and output arrays is as follows:\n",
    "    dataset: (num features, num timesteps)\n",
    "    x:       (num chunks, look_back, num features)\n",
    "    y:       (num chunks, predict_steps)\n",
    "    y_ind:   (num chunks)\n",
    "    \n",
    "    \"\"\"\n",
    "    numchunk = int(np.floor((dataset.shape[1] - look_back - 1) / chunk_step))\n",
    "    dataX = np.empty((numchunk, look_back, dataset.shape[0]))\n",
    "    dataY = np.empty((numchunk, predict_steps))\n",
    "    y_ind = []\n",
    "    \n",
    "    # Create chunks of data with the specified look back\n",
    "    for i in range(numchunk):\n",
    "        start_ind = chunk_step*i\n",
    "        dataX[i, :, :] = dataset[:, start_ind:(start_ind + look_back)].T\n",
    "        dataY[i, :] = dataset[0,start_ind+look_back:start_ind+look_back+predict_steps] #MG\n",
    "        #dataY.append(dataset[0, start_ind + look_back])\n",
    "        y_ind.append(start_ind + look_back)\n",
    "        \n",
    "    # Randomise order of chunks\n",
    "    rand_indices = np.random.permutation(numchunk)\n",
    "    x = np.array(dataX)\n",
    "    y = np.array(dataY)\n",
    "    y_ind = np.array(y_ind)\n",
    "    x = x[rand_indices, :]\n",
    "    y = y[rand_indices,:]\n",
    "    #y = np.reshape(y, (y.size, 1)) # MG\n",
    "    y_ind = y_ind[rand_indices]\n",
    "    \n",
    "    return x, y, y_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aquifier Auser\n",
    "## Data pre-processing\n",
    "### Handling missing data\n",
    "First, the data was read in and an analysis of missing data was carried out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Pre-processing data\n",
    "\n",
    "# Importing all data into pandas dataframe\n",
    "foldpath = r\"acea-water-prediction\"\n",
    "files = list(Path(foldpath).rglob('*.csv'))\n",
    "df = pd.read_csv(files[0])\n",
    "\n",
    "# Find and plot gaps in the data\n",
    "all_data, col_names, is_nan, is_zero = find_data_gaps(df, plot=True, title=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows that there is a significant amount of data missing over the total time period, either in the form of NaNs, or zeros. In the case of the rainfall data, the zeros are considered to be 'real data' since there are many days with zero rainfall, however for the other data it is assumed that zeros imply missing data.\n",
    "\n",
    "When deciding how to handle these gaps in the data, the aim was to find a compromise between maintaining as much of the original data as possible, and not introducing too much inaccuracy from filling in the gaps. For each variable, we consider a combination of three different options for handling these gaps:\n",
    "1. Crop the entire dataset over a certain time range to avoid the gaps. If many of the variables contain missing data for the same time period, then this option is sensible as this time period is unlikely to be useful for the model.\n",
    "2. Remove the variable completely. This may be necessary if there is missing data for large time periods, particularly if many other variables contain data in those time periods. For example, 'Volume_CSA' and 'Volume CSAL' both contain large gaps in the data where there is data for most other variables (from days ~3000-6000, so it makes sense to remove these from the analysis.\n",
    "3. Fill in the missing data for that variable by prediction (e.g. interpolation, backpropagation). Where the gaps in the data are relatively short and make up a small proportion of the total data, this option is likely to be preferable.\n",
    "\n",
    "Since there are NaNs in most of the data for the first ~3000 days, including all of the target variables, it was decided that this time period would not be used for the model at all.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find column indices for different datatypes\n",
    "datatypes, col_inds = find_datatypes(df)\n",
    "\n",
    "# Get time series data for all variable types\n",
    "start_ind = 3955\n",
    "rain_ts, rain_name = get_time_series(col_inds[0], start_ind=start_ind, fill_zeros=False)\n",
    "target_ts, target_name = get_time_series(col_inds[1], start_ind=start_ind)\n",
    "temp_ts, temp_name = get_time_series(col_inds[2], start_ind=start_ind)\n",
    "vol_ts, vol_name = get_time_series(col_inds[3], start_ind=start_ind)\n",
    "hydro_ts, hydro_name = get_time_series(col_inds[4], start_ind=start_ind)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove zeros from target\n",
    "# N.B. SHOULD REPLACE THIS WITH A DIFFERENT METHOD\n",
    "for m in range(len(target_ts)):\n",
    "    for n in range(len(target_ts[m])):\n",
    "        if n > 0 and target_ts[m][n] == 0:\n",
    "            target_ts[m][n] = target_ts[m][n-1]\n",
    "\n",
    "# Extend target with 2 values missing from the end\n",
    "# to_append = target_ts[1][-1]*np.ones(2)\n",
    "# target_ts[1] = np.append(target_ts[1], target_ts[1][-1]*np.ones(2), 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rainfall\n",
    "Rainfall was expected to be one of the most useful variables in predicting groundwater levels, since it is one the key physical mechanisms that drives the inflow of water into aquifiers. From a visual inspection of the data, it was clear that many of the peaks in rainfall correspond approximately to peaks in the groundwater level. This was seen to apply both to the longer term trend, as well as on a smaller day-by-day scale. However, while the peaks in rainfall are often immediately followed by days of zero rainfall, the peaks in the groundwater data seem to decay more slowly over time.\n",
    "\n",
    "This led to the hypothesis that the rainfall on a particular day adds to the groundwater level relatively quickly, and then this level decays more slowly over several weeks. The physical explanation for this seems to be logical - if the rate of the rainfall is significantly larger than the rate that the water drains out of the groundwater subsystem, then the level will rise quickly and fall slowly (similar to the level of a slowly leaking cup when water is poured into it). For shallower subsystems, we may expect to see more short term reactions to the rainfall, as it would reach the subsystem more quickly and drain out more quickly; and conversely for deeper subsystems, we would expect the levels to react more slowly, with smaller short term effects.\n",
    "\n",
    "To test this hypothesis, the rainfall data was convolved with an exponential window function, for a range of different time constants. The correlation of these convolved signals with the 'depth to groundwater' variables was calculated to find the time constant that gave the best correlation for each combination of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Rainfall data analysis\n",
    "\n",
    "r_i = 0\n",
    "t_i = 1\n",
    "rain_pp = exp_convolve(rain_ts[r_i], tau=54)\n",
    "plot_preprocessing(rain_ts[r_i], rain_pp, rain_name[r_i], target_ts[t_i], target_name[t_i],\n",
    "                   crop_min=1600, crop_max=2000, title=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot example of correlation for different tau values\n",
    "\n",
    "tau_best = find_best_tau(target_ts=target_ts[1], rain_ts=rain_ts,\n",
    "                         rain_name=rain_name, plot=True,\n",
    "                         target_name=target_name[1],\n",
    "                         tau_array=np.linspace(2, 160, 80))\n",
    "\n",
    "tau_best = find_best_tau(target_ts=target_ts[0], rain_ts=rain_ts, plot=True,\n",
    "                         target_name=target_name[0],\n",
    "                         rain_name=rain_name,\n",
    "                         tau_array=np.linspace(20, 1600, 80))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating optimum tau for each rainfall/target combination that\n",
    "# maximises the Spearman's Rank correlation coefficient\n",
    "\n",
    "rain_tau = find_all_best_tau(rain_ts, rain_name, target_ts, target_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'Rainfall_Piaggione' from pre-processed data list due to missing values\n",
    "rain_filt = rain_ts.copy()\n",
    "rain_name_pp = rain_name.copy()\n",
    "del rain_filt[5]\n",
    "del rain_name_pp[5]\n",
    "\n",
    "# Create 3 different sets of pre-processed data\n",
    "#   - rain_pp_shallow is a list of size (num selected rain), where each element is a time series array. This\n",
    "#     array is just for the cross-correlation anaysis, so tau value is an average value of the best tau values for\n",
    "#     the shallow groundwater variables (all but LT2).\n",
    "#   - rain_pp_deep is a list of size (num selected rain), where each element is a time series array. This\n",
    "#     array is just for the cross-correlation anaysis, so tau value is the best tau values for the deep\n",
    "#     groundwater variable (LT2).\n",
    "#   - rain_pp_all is a list of size (num target variables, num selected hydro), where each element is a time\n",
    "#     series array. Each array has the preprocessed data with the optimum tau value for that particular\n",
    "#     combination of hydro/target. This will be used for the model.\n",
    "\n",
    "# Rainfall data preprocessing\n",
    "rain_pp_shallow = []\n",
    "rain_pp_deep = []\n",
    "rain_pp_all = []\n",
    "for n in range(len(rain_filt)):\n",
    "    tau_shallow = np.mean(rain_tau[1:, n])\n",
    "    tau_deep = np.mean(rain_tau[0, n])\n",
    "    rain_pp_shallow.append(exp_convolve(rain_filt[n], tau_shallow))\n",
    "    rain_pp_deep.append(exp_convolve(rain_filt[n], tau_deep))\n",
    "    rain_pp_ = []\n",
    "    for m in range(len(target_ts)):\n",
    "        tau_ = rain_tau[m, n]\n",
    "        rain_pp_.append(exp_convolve(rain_filt[n], tau_))\n",
    "    rain_pp_all.append(rain_pp_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volume\n",
    "Initial inspection of the 'volume' data showed the following:\n",
    "- There is a relatively strong weekly pattern, and typically step changes every month. These are presumably linked to human activity/demand, and similar patterns are not clearly visible in the 'depth to groundwater' target variables. This suggests that it may be helpful to apply some smoothing to the volume data prior to modelling, in order to reduce the effect of the weekly/monthly variation.\n",
    "- There are some very long timescale trends in the ampltidue, for example both Volume_CC1 and Volume_CC2 both increase gradually over the whole time period of the data. This timescale of change (perhaps linked to human activity/demand changes) is not present in the depth to groundwater variables, so there may be some benefit in removing it from the volume data prior to modelling.\n",
    "\n",
    "In order to try to make the volume data more useful for modelling depth to groundwater, the following pre-processing was proposed based on the findings above:\n",
    "- Caclulate the deviation of the volume data from a smoothed version of the data. The smoothed version of the data should be created with a relatively long smoothing window so that it represents the long term trend of the volume data.\n",
    "- Apply some smoothing to the deviation calculated above, with a relatively short smoothing window, in order to reduce the effect of the weekly/monthly variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all volume data - for reference, not for final notebook\n",
    "\n",
    "plt.figure()\n",
    "for n in range(len(vol_ts)):\n",
    "    plt.plot(vol_ts[n], label=vol_name[n])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Volume pre-processing\n",
    "\n",
    "# find_best_vol_params(vol_ts[0], np.sum(target_ts, 0), smoothing_1=None, smoothing_2=None, plot=True)\n",
    "# find_best_vol_params(vol_ts[0], np.sum(target_ts, 0), smoothing_1=None, smoothing_2=None, plot=True)\n",
    "# find_best_vol_params(vol_ts[1], np.sum(target_ts, 0), smoothing_1=None, smoothing_2=None, plot=True)\n",
    "# find_best_vol_params(vol_ts[2], np.sum(target_ts, 0), smoothing_1=None, smoothing_2=None, plot=True)\n",
    "\n",
    "vol_pp = []\n",
    "for n in range(len(vol_ts)):\n",
    "    vol_pp.append(preprocess_vol(vol_ts[n]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparing original and pre-processed volume data\n",
    "\n",
    "v_i = 0\n",
    "t_i = 2\n",
    "plot_preprocessing(vol_ts[v_i], vol_pp[v_i], vol_name[v_i], target_ts[t_i], target_name[t_i],\n",
    "                   crop_min=2300, crop_max=2800, title=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature\n",
    "All of the temperature variables appear to be reasonably well correlated to each other, showing similar seasonal variation.\n",
    "However, 'Temperature_Lucca_Orto_Botanico' has long periods of missing data, including the most recent ~1000 days, so it will not be used for the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot all temp data - for reference, not for final notebook\n",
    "\n",
    "plt.figure(figsize=(9, 4))\n",
    "for n in range(len(temp_ts)):\n",
    "    plt.plot(temp_ts[n], label=temp_name[n])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 4))\n",
    "plt.plot(normalise_0_to_1(-temp_ts[0]), label=temp_name[0])\n",
    "plt.plot(normalise_0_to_1(target_ts[1]), label=target_name[1])\n",
    "# plt.xlim(2000, 2500)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure(figsize=(9, 4))\n",
    "plt.plot(normalise_0_to_1(-temp_ts[0]), label=temp_name[0])\n",
    "plt.plot(normalise_0_to_1(target_ts[1]), label=target_name[1])\n",
    "plt.xlim(2000, 2500)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove 'Temperature_Lucca_Orto_Botanico' from pre-processed data list\n",
    "temp_pp = temp_ts.copy()\n",
    "temp_name_pp = temp_name.copy()\n",
    "del temp_pp[2]\n",
    "del temp_name_pp[2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hydrometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot all hydrometry data - for reference, not for final notebook\n",
    "\n",
    "plt.figure(figsize=(9, 4))\n",
    "for n in range(len(hydro_ts)):\n",
    "    plt.plot(hydro_ts[n], label=hydro_name[n])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 4))\n",
    "for n in range(len(target_ts)):\n",
    "    plt.plot(target_ts[n], label=target_name[n])\n",
    "plt.xlim(4100, 4200)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot showing comparison of raw hydrometry data with pre-processed (smoothed) data\n",
    "\n",
    "h_i = 0\n",
    "t_i = 1\n",
    "hydro_pp = exp_convolve(hydro_ts[h_i], tau=7, winlength=None)\n",
    "plot_preprocessing(hydro_ts[h_i], hydro_pp, hydro_name[h_i], target_ts[t_i], target_name[t_i],\n",
    "                   crop_min=1700, crop_max=2200, title=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calculating optimum tau for each hydrometry/target combination that\n",
    "# maximises the Spearman's Rank correlation coefficient\n",
    "\n",
    "hydro_tau = find_all_best_tau(hydro_ts, hydro_name, target_ts, target_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hydrometry data preprocessing\n",
    "\n",
    "# Remove Piaggione from data list due to missing values\n",
    "hydro_filt = hydro_ts.copy()\n",
    "hydro_name_pp = hydro_name.copy()\n",
    "del hydro_filt[1]\n",
    "del hydro_name_pp[1]\n",
    "\n",
    "# Create 3 different sets of pre-processed data\n",
    "#   - hydro_pp_shallow is a list of size (num selected hydro = 1), where each element is a time series array. This\n",
    "#     array is just for the cross-correlation anaysis, so tau value is an average value of the best tau values for\n",
    "#     the shallow groundwater variables (all but LT2).\n",
    "#   - hydro_pp_deep is a list of size (num selected hydro = 1), where each element is a time series array. This\n",
    "#     array is just for the cross-correlation anaysis, so tau value is the best tau values for the deep\n",
    "#     groundwater variable (LT2).\n",
    "#   - hydro_pp_all is a list of size (num target variables, num selected hydro), where each element is a time\n",
    "#     series array. Each array has the preprocessed data with the optimum tau value for that particular\n",
    "#     combination of hydro/target. This will be used for the model.\n",
    "\n",
    "hydro_pp_shallow = []\n",
    "hydro_pp_deep = []\n",
    "hydro_pp_all = []\n",
    "for n in range(len(hydro_filt)):\n",
    "    tau_shallow = np.mean(hydro_tau[1:, n])\n",
    "    tau_deep = np.mean(hydro_tau[0, n])\n",
    "    hydro_pp_shallow.append(exp_convolve(hydro_filt[n], tau_shallow))\n",
    "    hydro_pp_deep.append(exp_convolve(hydro_filt[n], tau_deep))\n",
    "    hydro_pp_ = []\n",
    "    for m in range(len(target_ts)):\n",
    "        tau_ = hydro_tau[m, n]\n",
    "        hydro_pp_.append(exp_convolve(hydro_filt[n], tau_))\n",
    "    hydro_pp_all.append(hydro_pp_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of how lag applied to rainfall affects correlation with target\n",
    "\n",
    "lag_array = np.arange(200)\n",
    "plt.figure()\n",
    "for n in range(len(rain_ts)):\n",
    "    crosscorr_lag, lag_array = cross_corr_lag(\n",
    "                                    normalise_0_to_1(target_ts[0]),\n",
    "                                    normalise_0_to_1(rain_ts[n]),\n",
    "                                    lag_array=lag_array)\n",
    "    plt.plot(lag_array, crosscorr_lag, label=rain_name[n])\n",
    "plt.legend()\n",
    "plt.xlabel('Lag applied to rainfall data [days]')\n",
    "plt.ylabel('Spearman''s rank correlation coefficient')\n",
    "plt.title('Correlation of rainfall with target for different lag amounts')\n",
    "plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "target_test_ind = 0\n",
    "rain_test_ind = 0\n",
    "# lag_array = np.arange(30)\n",
    "# tau_array = np.linspace(12, 90, 40).astype(np.int)\n",
    "lag_array = np.linspace(0, 60, 31).astype(np.int)\n",
    "tau_array = np.linspace(40, 2000, 50).astype(np.int)\n",
    "data1 = target_ts[target_test_ind]\n",
    "data2 = rain_ts[rain_test_ind]\n",
    "sp_rank_cc, lag_array, tau_array = tau_and_lag_correl(data1,\n",
    "                                                      data2,\n",
    "                                                      lag_array=lag_array,\n",
    "                                                      tau_array=tau_array,\n",
    "                                                      plot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data down-selection\n",
    "Since many of the variables seem to show a high correlation with each other, using all of them to train the model is likely to add unnecessary computation time, without improvement over a model trained with a smaller selection of variables. Therefore, some down-selection of variables was carried out prior to modelling, in order to make the machine learning model as computationally efficient as possible. This was done by calculating the cross-correlation of the pre-processed data, and evaluating the expected usefulness of each variable based on the following principles:\n",
    "1. If input variables have a strong correlation to the target variable, they are likely to be useful in training the model.\n",
    "2. If input variables have a strong correlation with other input variables, there may be limited benefit in including all of them in the model.\n",
    "\n",
    "During the pre-processing of the rainfall and hydrometry data, it was found that for 'Depth_to_Groundwater_LT2', the optimum time constant (tau) for the convolution was much higher than the optimum value for the other groundwater variables. This was thought to be because the LT2 sub-system is significantly deeper underground than the other variables, and therefore it takes longer for the other variables to impact the level. In order to best evaluate the correlation of the variables, two different correlation matrices were calculated: one with variable pre-processing optimised for the shallower groundwater variables; and one with variable pre-processing optimised for the deepest groundwater variable LT2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot correlation matrices\n",
    "\n",
    "# Combine all pre-processed data for the shallow groundwater variables\n",
    "data_pp_shallow = np.concatenate((np.asarray(target_ts),\n",
    "                                  np.asarray(rain_pp_shallow),\n",
    "                                  np.asarray(vol_pp),\n",
    "                                  np.asarray(temp_pp),\n",
    "                                  np.asarray(hydro_pp_shallow)))\n",
    "name_pp_shallow = target_name + rain_name_pp + vol_name + temp_name_pp + hydro_name_pp\n",
    "\n",
    "# Combine all pre-processed data for the deep groundwater variables\n",
    "data_pp_deep = np.concatenate((np.asarray(target_ts),\n",
    "                               np.asarray(rain_pp_deep),\n",
    "                               np.asarray(vol_pp),\n",
    "                               np.asarray(temp_pp),\n",
    "                               np.asarray(hydro_pp_deep)))\n",
    "name_pp_deep = target_name + rain_name_pp + vol_name + temp_name_pp + hydro_name_pp\n",
    "\n",
    "# Plot correlation matrices\n",
    "plot_correl_matrix(data_pp_deep, name_pp_deep,\n",
    "                   title='Correlation matrix, data pre-processed for deep LT2 groundwater variable')\n",
    "plot_correl_matrix(data_pp_shallow, name_pp_shallow,\n",
    "                   title='Correlation matrix, data pre-processed for shallow groundwater variables')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# LSTM analysis\n",
    "\n",
    "def run_LSTM(target_data, input_data, look_back=30, chunk_step=50, train_ratio=0.67,\n",
    "             num_epochs=30, batch_size=5, predict_steps=15, plot=True):\n",
    "    \"\"\"\n",
    "    Wrapper function for LSTM\n",
    "    \n",
    "    Add in option to not use target variable?\n",
    "    I haven't really thought about what would be useful outputs yet\n",
    "    \n",
    "    \"\"\"\n",
    "    # # Calculate differential of target & rainfall\n",
    "    # target_diff = np.diff(target_ts[target_ind])\n",
    "    # rain_conv_diff = []\n",
    "    # for n in range(len(rain_pp_all[target_ind])):\n",
    "    #     rain_conv_diff.append(np.diff(rain_pp_all[target_ind][n]))\n",
    "\n",
    "    # Normalize the target dataset\n",
    "    # target_ts[target_ind] = target_ts[target_ind][1:] #for diff\n",
    "    target_data = np.reshape(target_data, (target_data.size, 1))\n",
    "    # scaler = MinMaxScaler(feature_range=(-1, 1)) #for diff\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    target_scaled = scaler.fit_transform(target_data)\n",
    "    target_scaled = np.squeeze(target_scaled)\n",
    "\n",
    "    # Normalise the input variables\n",
    "    num_input = model_input_data.shape[0]\n",
    "    input_data_scaled = np.empty(model_input_data.shape)\n",
    "    for n in range(num_input):\n",
    "        input_data_scaled[n, :] = normalise_0_to_1(model_input_data[n, :])\n",
    "\n",
    "    # Combine target and inputs into single array\n",
    "    num_timesteps = target_data.size\n",
    "    model_data = np.empty((num_input + 1, num_timesteps))\n",
    "    model_data[0, :] = target_scaled\n",
    "    model_data[1:, :] = input_data_scaled\n",
    "    # model_data = np.vstack((target_scaled, input_scaled))\n",
    "    print('Model_data shape: {}'.format(model_data.shape))\n",
    "\n",
    "    # Plot preprocessed data\n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        plt.plot(model_data[0, :], label=target_name[target_ind])\n",
    "        plt.plot(model_data[1, :], label='input')\n",
    "        plt.legend()\n",
    "        plt.title('Preprocessed data')\n",
    "        plt.show()\n",
    "\n",
    "    # Split data into chunks with random order\n",
    "    x, y, y_ind = create_dataset(model_data, look_back=look_back,\n",
    "                                 chunk_step=chunk_step,predict_steps=predict_steps)\n",
    "    numchunk = y.shape[0]\n",
    "    print('x shape: {}'.format(x.shape))\n",
    "    print('y shape: {}'.format(y.shape))\n",
    "\n",
    "    # split into train and test sets\n",
    "    train_size = int(numchunk * train_ratio)\n",
    "    test_size = numchunk - train_size\n",
    "    trainX, testX = x[0:train_size, :, :], x[train_size:numchunk, :, :]\n",
    "    trainY, testY = y[0:train_size, :], y[train_size:numchunk, :]\n",
    "    trainYind, testYind = y_ind[0:train_size], y_ind[train_size:numchunk]\n",
    "\n",
    "    # Plot one example of chunk\n",
    "    if plot:\n",
    "        sample_num = 10\n",
    "        plt.figure()\n",
    "        time_ = np.arange(look_back)\n",
    "        plt.plot(time_, trainX[sample_num, :, 0], label='Input: target')\n",
    "        plt.plot(time_, trainX[sample_num, :, 1], label='Input: rain')\n",
    "        plt.plot([look_back], trainY[sample_num, 0], 'xr', label='Output: target')\n",
    "        plt.legend()\n",
    "        plt.title(\"Example of one chunk from dataset\")\n",
    "        plt.show()\n",
    "\n",
    "    # create and fit the LSTM network\n",
    "    num_features = x.shape[2]\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(4, input_shape=(look_back, num_features)))\n",
    "    model.add(Dense(predict_steps))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.fit(trainX, trainY, epochs=num_epochs, batch_size=batch_size, verbose=1)\n",
    "\n",
    "    # make predictions\n",
    "    trainPredict = model.predict(trainX)\n",
    "    testPredict = model.predict(testX)\n",
    "\n",
    "    # invert predictions\n",
    "    trainPredict = scaler.inverse_transform(trainPredict)\n",
    "    trainY = scaler.inverse_transform(trainY)\n",
    "    testPredict = scaler.inverse_transform(testPredict)\n",
    "    testY = scaler.inverse_transform(testY)\n",
    "\n",
    "    # # Convert from diff to actual prediction\n",
    "    # for n in range(train_size):\n",
    "    #     prev_day = target_ts[target_ind][trainYind[n] - 1]\n",
    "    #     trainY[n] = prev_day + trainY[n]\n",
    "    #     trainPredict[n] = prev_day + trainPredict[n]\n",
    "    # for n in range(test_size):\n",
    "    #     prev_day = target_ts[target_ind][testYind[n] - 1]\n",
    "    #     testY[n] = prev_day + testY[n]\n",
    "    #     testPredict[n] = prev_day + testPredict[n]\n",
    "\n",
    "    # calculate root mean squared error\n",
    "    trainScore = math.sqrt(mean_squared_error(trainY[:, 0], trainPredict[:,0]))\n",
    "    print('Train Score: %.2f RMSE' % (trainScore))\n",
    "    testScore = math.sqrt(mean_squared_error(testY[:, 0], testPredict[:,0]))\n",
    "    print('Test Score: %.2f RMSE' % (testScore))\n",
    "    \n",
    "    return trainScore, testScore, testYind, testPredict, trainYind, trainPredict\n",
    "\n",
    "\n",
    "# Choose variables to include in model by their indices\n",
    "target_ind = 1\n",
    "model_rain_ind = [1]\n",
    "model_vol_ind = []\n",
    "model_temp_ind = []\n",
    "model_hydro_ind = []\n",
    "\n",
    "\n",
    "# Compile target data and inputs to model\n",
    "target_data = target_ts[target_ind]\n",
    "model_input_data = compile_model_input_variables(model_rain_ind,\n",
    "                                                 model_vol_ind,\n",
    "                                                 model_temp_ind,\n",
    "                                                 model_hydro_ind,\n",
    "                                                 rain_pp_all[:][target_ind],\n",
    "                                                 vol_pp,\n",
    "                                                 temp_pp,\n",
    "                                                 hydro_pp_all[0][target_ind])\n",
    "\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "\n",
    "\n",
    "trainScore, testScore, testYind, testPredict, trainYind, trainPredict = run_LSTM(target_data, model_input_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot one prediction example from test sample\n",
    "# pred_sample = 4\n",
    "# pred_ind = testYind[pred_sample]\n",
    "# plt.figure()\n",
    "# sample_t = np.linspace(pred_ind - look_back - 1, pred_ind - 1, look_back)\n",
    "# sample_t = sample_t.astype(np.int)\n",
    "# plt.plot(sample_t, target_ts[target_ind][sample_t[0]:sample_t[-1]])\n",
    "# plt.plot(pred_ind, testPredict[pred_sample, 0], 'xr')\n",
    "# plt.title(\"Example of one prediction from test data\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all predictions from test samples against original data\n",
    "plt.figure(figsize=(9, 4))\n",
    "plt.plot(np.linspace(0, target_ts[target_ind].size-1,\n",
    "                     target_ts[target_ind].size),\n",
    "         target_ts[target_ind],\n",
    "         label='Original data')\n",
    "plt.plot(testYind, testPredict[:, 0], 'xr', label='Test predictions')\n",
    "plt.plot(trainYind, trainPredict[:, 0], 'xg', label='Train predictions')\n",
    "plt.legend()\n",
    "plt.title(\"Predictions compared to original dataset\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictability horizon\n",
    "\n",
    "Here we evaluate the performance of the model as a function of the number of days in advance it predicts. As expected, the performance tails off with the number of days, but remains above 90% for over 15 days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_predict_horizon(target_data,testPredict,testYind,predict_steps):\n",
    "    testTarget = np.empty(np.shape(testPredict))\n",
    "    for ii in range(len(testYind)):\n",
    "        testTarget[ii,:] = target_data[testYind[ii]:testYind[ii] + predict_steps]\n",
    "    #predict_horizon_R2 = [rsquared(testTarget[:,ii], testPredict[:,ii]) for ii in range(predict_steps)]\n",
    "    predict_horizon = [spearman_lag(testTarget[:,ii], testPredict[:,ii], 0) for ii in range(predict_steps)]\n",
    "    return predict_horizon\n",
    "\n",
    "predict_horizon = get_predict_horizon(target_ts[target_ind], testPredict, testYind, predict_steps)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0,predict_steps), predict_horizon)\n",
    "plt.title(\"Predictability horizon\")\n",
    "plt.xlabel(\"Spearman rank coefficient\")\n",
    "plt.ylabel(\"Steps in advance [days]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
